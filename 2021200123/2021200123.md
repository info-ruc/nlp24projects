# 项目报告：基于GPT2中文大模型的古诗词生成模型
孙志钢 2021200123
## 1. 研究背景和研究目标
### 1.1 研究背景
古诗词作为中国传统文化的瑰宝，具有深厚的历史背景和广泛的文化影响力。它们不仅是文学艺术的表现形式，也承载了丰富的哲学思想、历史故事、情感表达等内容。

随着人工智能和自然语言处理（NLP）技术的飞速发展，深度学习模型，特别是基于神经网络的生成模型，已经在多个领域取得了突破。近年来，GPT系列模型、BERT、Transformer等技术的出现使得语言生成任务，尤其是文学创作，进入了新的阶段。

尽管现代技术在诗词创作方面已取得一定进展，但由于古诗词具有独特的韵律、对仗、意境等要求，古诗词的自动生成依然面临诸多挑战。许多早期的模型未能很好地解决语言流畅性、文化底蕴与创意之间的平衡问题。因此，如何利用深度学习技术生成具有较高文学价值的古诗词，成为了一个重要的研究方向。

### 1.2 研究目的
目前的自动古诗词生成模型面临的问题包括语法结构不准确、韵律不和谐、情感表达不到位等。研究的目的之一是通过改进模型架构，优化训练数据，增强模型对古诗词特征的学习能力，提高生成诗词的质量与准确性，使其更具文学性与艺术性。

## 2. 数据准备
本项目采用中华诗词数据库，数据库链接：[诗词数据库](https://github.com/chinese-poetry/chinese-poetry)

此数据库是最全的中华古典文集数据库，包含 5.5 万首唐诗、26 万首宋诗、2.1 万首宋词和其他古典文集。诗人包括唐宋两朝近 1.4 万古诗人，和两宋时期 1.5 千古词人。数据来源于互联网。

本项目提取其中的5.5 万首唐诗、26 万首宋诗、2.1 万首宋词作为训练集进行训练，利用数据库进行基本操作后转化为csv。详细路径见[数据](./src/data_path.md)

## 3. LSTM模型尝试
本次首先采用LSTM模型进行初步的生成模型训练，主要代码见src下old version文件夹

### 3.1 数据处理
由于数据库主要为繁体字，首先调用hanziconv库进行简单的简繁转换。

首先，利用函数读取并清洗数据，去除缺失值，过滤掉包含禁用词或过长的诗句，并拼接成完整的诗。然后，函数为诗句中的所有字符创建字符到索引的映射字典和索引到字符的反向映射字典。接着，将每个诗句转化为由字符索引构成的向量，并根据设定的最大长度进行填充或截断。

采用独特字符标记标题和内容的分隔和标记诗句的结束，进而加深模型对这些关键位置的理解。

### 3.2 模型设置
利用简单的lstm模型进行训练，模型如下：
```python
class RNN_model(nn.Module):
    def __init__(self, vocab_len ,word_embedding, embedding_dim, lstm_hidden_dim):
        super(RNN_model,self).__init__()
        self.word_embedding_lookup = word_embedding
        self.vocab_length = vocab_len  #可选择的单词数目 或者说 word embedding层的word数目
        self.word_embedding_dim = embedding_dim
        self.lstm_dim = lstm_hidden_dim
        self.rnn_lstm = nn.LSTM(input_size=embedding_dim, 
                                 hidden_size=lstm_hidden_dim, 
                                 num_layers=2,
                                 batch_first=True)

        self.fc = nn.Linear(self.lstm_dim, self.vocab_length)
        nn.init.xavier_uniform_(self.fc.weight)
    def forward(self,sentence,batch_size,is_test = False):
        batch_input = self.word_embedding_lookup(sentence).view(batch_size,-1,self.word_embedding_dim)
        hidden = torch.zeros(2, batch_size, self.lstm_dim).to(next(self.parameters()).device)
        cell = torch.zeros(2, batch_size, self.lstm_dim).to(next(self.parameters()).device)
        output, _ = self.rnn_lstm(batch_input, (hidden, cell))

        out = output.contiguous().view(-1,self.lstm_dim)
        out = self.fc(out)   #out.size: (batch_size * sequence_length ,vocab_length)
        if is_test:
            #测试阶段(或者说生成诗句阶段)使用
            prediction = out[ -1, : ].view(1,-1)
            output = prediction
        else:
            #训练阶段使用
           output = out
        return output
```
### 3.3 模型训练
由于模型的主要目标是根据上下文生成古诗文，在训练中，我利用向量化后的数据的前n-1个字符作为x，利用后n-1个字符作为目标数据y。

我按照朝代（唐、宋）划分数据集进行分别训练，历经30个epoch训练后获得两个最终的模型。

模型使用本机cpu进行简单训练。

### 3.4 训练结果
训练了多个模型 其中宋诗模型表现还可以，可能是因为数据相对更多一些

测试代码：
```python
print(gen_poem("春日访王丞相X"))
print(gen_poem("江岸别杜甫X"))
print(gen_poem("送白居易X"))
print(gen_poem("月见X"))
print(gen_poem("与李白会诗X"))
print(gen_poem("北山雪X"))
```
宋诗模型结果：

#### 春日访王丞相

春风吹雨过，春色满春风。花柳花如锦，春风吹柳花。

#### 江岸别杜甫

江南江北路，江北见山川。一舸归船去，孤帆过岭云。

#### 送白居易

一别三年别，今年一再归。一年归故国，一别一年秋。

#### 月见

一片飞来月，朣朣照夜光。月明蟾影碎，风入玉楼斜。

#### 与李白会诗

一片青山一点尘，一年春色满人家。春风吹尽春风急，不见春风一片春。

#### 北山雪

山中有佳处，山色自相依。山色无人到，山深不见梅。

### 3.5 模型反思
可以看到，经过大量的数据训练，基础的lstm模型已经能根据标题写出似是而非的诗句，但是可能对于给出的标题的理解程度不高，同时出现一首诗中单字多次出现的情况。

LSTM生成的序列通常是基于当前状态的线性递推，而缺乏足够的上下文理解，导致在生成诗句时，容易出现词汇重复或不合适的词汇使用。

LSTM在理解上下文关联、捕捉复杂语义及生成更连贯、自然的诗句方面相较于更强大的模型（如GPT-2）较为有限

## 4. 基于中文GPT2微调的古诗生成模型

### 4.1 模型选择

考虑到lstm模型的局限性，我最终决定采用GPT-2模型来进一步进行模型的训练。我最终采用一个已经经过预训练的中文GPT-2进行训练和微调，模型来自transformer库。

### 4.2 模型调用
由于网络访问存在问题，我将原模型下载下来，利用transformer库进行调用。
```python
from transformers import BertTokenizer, GPT2LMHeadModel,TextGenerationPipeline
local_model_path = './gpt2-chn'
tokenizer = BertTokenizer.from_pretrained(local_model_path)
model = GPT2LMHeadModel.from_pretrained(local_model_path)
```
GPT2LMHeadModel是用于语言建模任务的GPT-2模型，它根据输入的token序列生成预测的下一个token，正好符合我的希望。

### 4.3 数据处理
由于模型自带比较全面的tokenizer，我只需要对诗句进行简单处理即可，利用冒号分割标题和内容，并整合在一起。

随后，我使用了datasets库来处理和标记化诗歌数据，并将其准备好以供模型训练。首先使用预先加载的tokenizer将每个诗句进行标记化，形成输入序列input_ids。在数据的标签问题上，我直接将将input_ids复制给labels，使得标签和输入序列相同，因为GPT-2模型是基于自回归生成的，因此当前输入的下一个词（字符）是目标。随后我将数据转换为pytorch形式。
```python
from datasets import Dataset
max_length=128
def tokenize_function(examples):
    encodings = tokenizer(examples['fullpoem'], padding="max_length", truncation=True, max_length=max_length, return_tensors="pt")
    input_ids = encodings['input_ids']
    labels = input_ids.clone()
    encodings['labels'] = labels
    return encodings
dataset = Dataset.from_pandas(df_poems)
tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format(type='torch', columns=['labels','input_ids', 'attention_mask'])
```
在数据的分离上，我选择用五言和七言进行区分，而不是按照朝代，希望能训练出良好的五言诗生成器和七言诗生成器。
### 4.4 模型训练
采用 nvidia RTX 4090进行模型的微调训练。

通过TrainingArguments设置了模型微调的关键参数，包括训练轮次、批次大小、梯度累积、模型保存和日志记录等。训练将在20轮内进行，每200步保存一次模型并保留最新的2个模型，批次大小设置为128，并通过梯度累积模拟更大的批次。每100步记录一次日志，并启用混合精度训练（FP16）以加速训练并减少显存占用。这些设置确保了训练过程高效、稳定，并便于后期模型监控与管理。

训练代码如下：
```python
from transformers import Trainer, TrainingArguments,EarlyStoppingCallback
training_args = TrainingArguments(
    output_dir='./results',             # 训练结果保存目录
    num_train_epochs=20,                 # 训练轮次
    per_device_train_batch_size=128,  # 相对较小的批次大小
    gradient_accumulation_steps=4,   # 累积梯度以模拟更大的批次  
    save_steps=200,                     # 每200步保存一次模型
    save_total_limit=2,                 # 保留最新的2个模型
    logging_dir='./logs',               # 日志目录
    logging_steps=100,                   # 每100步记录一次日志
    fp16=True, 
)
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,

)
trainer.train()
```
### 4.5 古诗生成模型
最终生成一个五言诗模型和一个七言诗模型，我们再次使用测试标题进行测试，测试代码如下：
```python
text_generator("[CLS]春 日 访 王 丞 相", max_length=100, do_sample=True,temperature=0.8)
text_generator("[CLS]江 岸 别 杜 甫", max_length=80, do_sample=True,temperature=0.8)
text_generator("[CLS]送 白 居 易", max_length=80, do_sample=True,temperature=0.8)
text_generator("[CLS]月 见 ", max_length=100, do_sample=True,temperature=0.8)
text_generator("[CLS]与 李 白 会 诗", max_length=100, do_sample=True,temperature=0.8)
print(gen_poem("北山雪X"))
```
五言诗模型（较好的输出结果）：
```python
from transformers import BertTokenizer, GPT2LMHeadModel,TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained('./fine_tuned_gpt2_chn_5-new')
model = GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2_chn_5-new')
text_generator = TextGenerationPipeline(model, tokenizer)   
```
#### 春 日 访 王 丞 相 
公 昔 少 年 日 ， 人 能 十 岁 余 。 

何 须 叹 牢 落 ， 聊 且 看 诗 书 。

知 我 独 携 酒 ， 逢 君 亦 起 予 。 

只 今 俱 老 矣 ， 不 恨 岁 时 除 。
#### 江 岸 别 杜 甫 ： 
东 郭 久 不 窥 ， 如 今 还 复 然 。

端 来 就 渔 客 ， 想 见 下 江 船 。 

宿 疾 故 相 恼 ， 闲 心 私 自 怜 。 

犹 欣 有 余 论 ， 小 子 亦 能 传 。
#### 送 白 居 易 ： 
闻 说 中 朝 邸 ， 三 年 一 拜 亲 。 

文 华 真 鸑 鷟 （yuè zhuó）， 才 气 更 麒 麟 。

政 事 从 今 绝 ， 功 名 未 画 麟 。 

伤 心 濑 东 路 ， 松 柏 自 成 薪 。
#### 月 见  ： 
朝 饮 月 已 出 ， 昏 归 月 未 生 。

荒 林 不 相 与 ， 魑 魅 与 同 行 。 

不 寐 闻 空 庑 ， 孤 魂 疑 复 惊 。 

离 乡 知 几 许 ， 四 顾 但 茫 茫 。
#### 与 李 白 会 诗 ： 
君 有 西 邑 志 ， 苦 县 吾 所 慕 。 

它 日 从 吾 兄 ， 骑 马 南 山 去 。
#### 北 山 雪 ： 
不 向 寒 江 去 ， 那 知 绝 世 机 。 

野 泉 依 砌 冷 ， 宿 雪 隔 窗 飞 。 

竹 伴 离 骚 影 ， 松 因 退 鹤 归 。 

仍 余 一 轮 月 ， 相 向 到 禅 扉 。

七言诗模型：
```python
from transformers import BertTokenizer, GPT2LMHeadModel,TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained('./fine_tuned_gpt2_chn_7-new')
model = GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2_chn_7-new')
text_generator = TextGenerationPipeline(model, tokenizer)   
```
#### 春 日 访 王 丞 相 示 五 首 其 二 ： 
少 年 尝 学 赋 高 峰 ， 岂 欲 逃 名 学 塞 翁 。 

万 目 共 知 惟 一 骥 ， 四 难 皆 足 慰 三 农 。 

不 须 直 上 麒 麟 阁 ， 肯 着 来 依 虎 豹 丛 。 

莫 负 太 平 平 日 约 ， 明 明 有 道 佐 时 雍 。
#### 送 白 居 易 ： 
一 番 离 思 又 分 明 ， 水 国 风 光 未 入 城 。 

莫 向 尊 前 重 索 句 ， 故 乡 何 处 最 关 情 。
#### 江 岸 别 杜 甫 ： 
江 头 千 树 欲 啼 莺 ， 却 向 东 风 弄 晚 晴 。 

唤 得 愁 魂 骑 不 去 ， 自 摇 明 月 舞 春 鶑 。
#### 月 见  ： 
云 际 悠 然 起 一 沤 ， 物 情 何 自 使 人 愁 。 

扫 开 云 雾 天 疑 晓 ， 放 出 蟾 光 夜 不 收 。 

金 醴 坐 倾 怀 素 友 ， 冰 轮 行 上 辗 青 丘 。 

但 能 照 我 心 神 爽 ， 不 患 无 人 桂 影 秋 。
#### 与 李 白 会 诗 乐 府 十 三 ： 
平 池 春 后 碧 波 寒 ， 风 月 尤 宜 共 倚 栏 。 

醉 面 凭 陵 归 酒 圣 ， 时 情 浅 淡 与 花 残 。 

千 金 不 买 清 狂 客 ， 一 笑 来 投 寂 寞 官 。 

惟 有 梅 花 如 索 笑 ， 此 身 终 作 老 儒 酸 。
#### 北 山 雪 ： 
风 吹 大 雪 暗 江 天 ， 行 李 萧 条 道 路 寒 。 

官 冷 无 人 乡 信 远 ， 自 惭 黄 菊 对 谁 看 。


### 4.6 宋词生成模型

我也尝试用宋词微调训练了一个宋词生成模型，但是由于本身宋词数据量很少，所以效果不是非常好。

测试代码：
```python
text_generator("[CLS]明 月 几 时 有", max_length=100, do_sample=True,temperature=0.8)
```
输出：
```python
from transformers import BertTokenizer, GPT2LMHeadModel,TextGenerationPipeline
tokenizer = BertTokenizer.from_pretrained('./fine_tuned_gpt2_chn_ci')
model = GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2_chn_ci')
text_generator = TextGenerationPipeline(model, tokenizer)   
```
明 月 几 时 有 。 何 待 酒 ， 无 人 共 语 更 论 。 且 泛 一 轮 秋 ， 莫 遣 十 分 满 。 谁 把 平 康 作 梦 ， 更 教 老 子 分 狂 。 欲 唤 刘 郎 和 靖 ， 笑 共 嵇 康 。

可以看出可能因为宋词里面词牌的多变性，很难训练出一个比较模范的宋词生成模型，可能需要进一步对输出结构做出规定才行。

## 5.总结和改进
### 5.1 总结
本项目基于深度学习技术，旨在通过微调GPT-2模型生成具有文学价值的古诗词。通过对大量古诗词数据的预处理、标记化，并利用GPT-2进行微调，我们成功地构建了一个可以生成五言和七言诗的模型。实验结果显示，GPT-2在古诗词生成任务中表现出了较高的创作能力，生成的诗句既符合语法要求，又具有一定的艺术性和情感表达，尤其是在生成五言诗时效果较为突出。模型能够根据给定的标题生成多样化的诗句，展示了其强大的自回归生成能力。

在宋词生成方面，由于数据量相对较少，训练效果较为有限。特别是宋词的词牌和形式多变，导致模型难以捕捉到宋词的独特结构和韵律。因此，宋词生成的效果较为差强人意，未来可能需要进一步的结构化训练策略来提升其表现。

### 5.2 未来改进方向
1. 优化数据集：宋词数据量较少，且其结构多样性较大，导致模型训练效果不佳。未来可以通过数据增强、增加更多的宋词数据或对现有数据进行标注和增强来改善模型的表现。

2. 更精细的模型架构：虽然GPT-2已经能生成较为流畅的诗句，但它的生成仍然受限于模型的自回归特性。在生成过程中，缺乏对全局上下文的深入理解，导致某些生成结果仍然显得不够自然或富有创意。未来可以尝试结合更多类型的模型，比如BERT与GPT-2结合，或者通过强化学习等方法来进一步优化模型的生成能力。

