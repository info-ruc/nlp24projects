{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功连接到数据库\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\szg\\AppData\\Local\\Temp\\ipykernel_12860\\3677323888.py:24: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  poetry_df = pd.read_sql(sql_query1, connection)\n",
      "C:\\Users\\szg\\AppData\\Local\\Temp\\ipykernel_12860\\3677323888.py:28: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  author_df = pd.read_sql(sql_query1, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据读取成功\n",
      "数据库连接已关闭\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "from pymysql import Error\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    # 连接到 MySQL 数据库\n",
    "    connection = pymysql.connect(\n",
    "        host='127.0.0.1',\n",
    "        database='poems',\n",
    "        user='root',\n",
    "        port=3306,\n",
    "        password='112358szg'\n",
    "    )\n",
    "\n",
    "    if connection.open:\n",
    "        print(\"成功连接到数据库\")\n",
    "        cursor = connection.cursor()\n",
    "    # 查询 SQL 语句\n",
    "    sql_query1 = \"SELECT * FROM poetry\"  # 替换为你的表名\n",
    "\n",
    "    # 将结果读取到 DataFrame\n",
    "    poetry_df = pd.read_sql(sql_query1, connection)\n",
    "    sql_query2 = \"SELECT * FROM poetry_author\"  # 替换为你的表名\n",
    "\n",
    "    # 将结果读取到 DataFrame\n",
    "    author_df = pd.read_sql(sql_query1, connection)\n",
    "    print('数据读取成功')\n",
    "\n",
    "\n",
    "except Error as e:\n",
    "    print(f\"连接错误: {e}\")\n",
    "\n",
    "finally:\n",
    "    if connection.open:\n",
    "        cursor.close()\n",
    "        connection.close()\n",
    "        print(\"数据库连接已关闭\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>yunlv_rule</th>\n",
       "      <th>author_id</th>\n",
       "      <th>content</th>\n",
       "      <th>dynasty</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19364</th>\n",
       "      <td>19365</td>\n",
       "      <td>再謁紫極宮</td>\n",
       "      <td>仄仄平平仄仄平，平平平仄仄平平。|平平仄仄平平仄，平仄平平仄仄平。</td>\n",
       "      <td>10628</td>\n",
       "      <td>一去重來已十年，頭顱堪笑尚依然。|何當了却塵紛事，來結梅花紙帳緣。</td>\n",
       "      <td>S</td>\n",
       "      <td>楊公遠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194355</th>\n",
       "      <td>194356</td>\n",
       "      <td>感寓二十首  其八</td>\n",
       "      <td>平平仄平平，仄仄仄平仄。|仄平平仄仄，仄仄仄平仄。|平平仄仄平，仄仄仄○仄。|仄平平平平，仄...</td>\n",
       "      <td>5841</td>\n",
       "      <td>西山有靈丸，五色發光耀。|餌之生羽翰，可以後天老。|莊楊擲斗筲，宇宙决籠罩。|往逢洪崖生，揖...</td>\n",
       "      <td>S</td>\n",
       "      <td>劉弇</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56680</th>\n",
       "      <td>56681</td>\n",
       "      <td>伊誰</td>\n",
       "      <td>平平仄仄仄平平，平仄平平仄仄平。|仄仄仄平平仄仄，平平仄仄仄平平。</td>\n",
       "      <td>9737</td>\n",
       "      <td>伊誰闖我小窗關，偷却西樓一面山。|誶語白雲猜是汝，秋風出意急追還。</td>\n",
       "      <td>S</td>\n",
       "      <td>史彌寧</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163983</th>\n",
       "      <td>163984</td>\n",
       "      <td>龍眠居士畫十六大阿羅漢贊  其二</td>\n",
       "      <td>仄仄平平，仄仄仄仄。|仄平仄仄，○仄仄仄。|仄平仄仄，平平仄平。|仄仄仄○，○平平平。</td>\n",
       "      <td>6901</td>\n",
       "      <td>倚杖于肩，屈指近喙。|廢經不讀，思第一義。|大獅子吼，真獅子兒。|轉秘密藏，其誰能知。</td>\n",
       "      <td>S</td>\n",
       "      <td>李綱</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50102</th>\n",
       "      <td>50103</td>\n",
       "      <td>偶題</td>\n",
       "      <td>仄平仄仄平平仄，仄仄平平仄仄平。|仄仄仄平平仄仄，仄平平仄仄平平。</td>\n",
       "      <td>9837</td>\n",
       "      <td>夜深水冷無香餌，若箇黄鱗肯上鈎。|我欲釣鼇安爾用，一竿風月好迴舟。</td>\n",
       "      <td>S</td>\n",
       "      <td>張侃</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261738</th>\n",
       "      <td>261739</td>\n",
       "      <td>小園</td>\n",
       "      <td>仄平平仄仄，平仄仄○平。|仄仄平○仄，平平仄仄○。|○平平○仄，仄仄仄○平。|仄仄平○仄，平...</td>\n",
       "      <td>2825</td>\n",
       "      <td>小園春色麗，花發兩三株。|露笋抽泥立，風蘭狎石鋪。|教兒棋正歇，得客酒重沽。|意緒渾如此，詩...</td>\n",
       "      <td>T</td>\n",
       "      <td>呂從慶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214670</th>\n",
       "      <td>214671</td>\n",
       "      <td>於忽操  其三</td>\n",
       "      <td>平仄平，仄仄仄平，○仄平平。|仄平平平，平仄平平。|仄仄平仄，平仄平仄。|仄平仄仄平仄平，平...</td>\n",
       "      <td>5300</td>\n",
       "      <td>於忽乎，不可以爲，其又奚爲。|謂雞斯飛，誰得而覊。|謂豕斯突，何取於縛。|是皆以食而得之，吾...</td>\n",
       "      <td>S</td>\n",
       "      <td>王令</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94097</th>\n",
       "      <td>94098</td>\n",
       "      <td>池邊梅花一株盛開  其一</td>\n",
       "      <td>平仄平平仄仄平，平平仄仄仄平平。|平平仄仄平平仄，仄仄平平仄仄平。</td>\n",
       "      <td>8572</td>\n",
       "      <td>天外青鸞不復歸，蕭蕭短髮日成絲。|寒梅不管人憔悴，故著新花滿舊枝。</td>\n",
       "      <td>S</td>\n",
       "      <td>王炎</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181428</th>\n",
       "      <td>181429</td>\n",
       "      <td>早發新吳</td>\n",
       "      <td>仄仄仄平仄，平平仄仄平。|仄平平仄仄，平仄仄平平。|仄仄平平仄，平平仄仄平。|平平平仄仄，平...</td>\n",
       "      <td>6311</td>\n",
       "      <td>行路柳枝弱，池塘草色齊。|石蘿人共遠，洲蘂意兼迷。|宿霧籠城郭，春颸入鼓鼙。|黄鸝花葉底，何...</td>\n",
       "      <td>S</td>\n",
       "      <td>洪朋</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138701</th>\n",
       "      <td>138702</td>\n",
       "      <td>次韻馮圓中酴醾  其二</td>\n",
       "      <td>仄平平仄平○仄，平仄仄○平仄仄。|平平仄仄仄平平，平仄仄平平仄仄。|平平平仄仄平平，仄仄仄平...</td>\n",
       "      <td>7744</td>\n",
       "      <td>碧雲堆裏顔如玉，紅紫便應高閣束。|千尋結架上青冥，山外水邊隨詰曲。|飄然來及牡丹時，潔白自知...</td>\n",
       "      <td>S</td>\n",
       "      <td>史浩</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id             title  \\\n",
       "19364    19365             再謁紫極宮   \n",
       "194355  194356         感寓二十首  其八   \n",
       "56680    56681                伊誰   \n",
       "163983  163984  龍眠居士畫十六大阿羅漢贊  其二   \n",
       "50102    50103                偶題   \n",
       "261738  261739                小園   \n",
       "214670  214671           於忽操  其三   \n",
       "94097    94098      池邊梅花一株盛開  其一   \n",
       "181428  181429              早發新吳   \n",
       "138701  138702       次韻馮圓中酴醾  其二   \n",
       "\n",
       "                                               yunlv_rule  author_id  \\\n",
       "19364                   仄仄平平仄仄平，平平平仄仄平平。|平平仄仄平平仄，平仄平平仄仄平。      10628   \n",
       "194355  平平仄平平，仄仄仄平仄。|仄平平仄仄，仄仄仄平仄。|平平仄仄平，仄仄仄○仄。|仄平平平平，仄...       5841   \n",
       "56680                   平平仄仄仄平平，平仄平平仄仄平。|仄仄仄平平仄仄，平平仄仄仄平平。       9737   \n",
       "163983        仄仄平平，仄仄仄仄。|仄平仄仄，○仄仄仄。|仄平仄仄，平平仄平。|仄仄仄○，○平平平。       6901   \n",
       "50102                   仄平仄仄平平仄，仄仄平平仄仄平。|仄仄仄平平仄仄，仄平平仄仄平平。       9837   \n",
       "261738  仄平平仄仄，平仄仄○平。|仄仄平○仄，平平仄仄○。|○平平○仄，仄仄仄○平。|仄仄平○仄，平...       2825   \n",
       "214670  平仄平，仄仄仄平，○仄平平。|仄平平平，平仄平平。|仄仄平仄，平仄平仄。|仄平仄仄平仄平，平...       5300   \n",
       "94097                   平仄平平仄仄平，平平仄仄仄平平。|平平仄仄平平仄，仄仄平平仄仄平。       8572   \n",
       "181428  仄仄仄平仄，平平仄仄平。|仄平平仄仄，平仄仄平平。|仄仄平平仄，平平仄仄平。|平平平仄仄，平...       6311   \n",
       "138701  仄平平仄平○仄，平仄仄○平仄仄。|平平仄仄仄平平，平仄仄平平仄仄。|平平平仄仄平平，仄仄仄平...       7744   \n",
       "\n",
       "                                                  content dynasty author  \n",
       "19364                   一去重來已十年，頭顱堪笑尚依然。|何當了却塵紛事，來結梅花紙帳緣。       S    楊公遠  \n",
       "194355  西山有靈丸，五色發光耀。|餌之生羽翰，可以後天老。|莊楊擲斗筲，宇宙决籠罩。|往逢洪崖生，揖...       S     劉弇  \n",
       "56680                   伊誰闖我小窗關，偷却西樓一面山。|誶語白雲猜是汝，秋風出意急追還。       S    史彌寧  \n",
       "163983        倚杖于肩，屈指近喙。|廢經不讀，思第一義。|大獅子吼，真獅子兒。|轉秘密藏，其誰能知。       S     李綱  \n",
       "50102                   夜深水冷無香餌，若箇黄鱗肯上鈎。|我欲釣鼇安爾用，一竿風月好迴舟。       S     張侃  \n",
       "261738  小園春色麗，花發兩三株。|露笋抽泥立，風蘭狎石鋪。|教兒棋正歇，得客酒重沽。|意緒渾如此，詩...       T    呂從慶  \n",
       "214670  於忽乎，不可以爲，其又奚爲。|謂雞斯飛，誰得而覊。|謂豕斯突，何取於縛。|是皆以食而得之，吾...       S     王令  \n",
       "94097                   天外青鸞不復歸，蕭蕭短髮日成絲。|寒梅不管人憔悴，故著新花滿舊枝。       S     王炎  \n",
       "181428  行路柳枝弱，池塘草色齊。|石蘿人共遠，洲蘂意兼迷。|宿霧籠城郭，春颸入鼓鼙。|黄鸝花葉底，何...       S     洪朋  \n",
       "138701  碧雲堆裏顔如玉，紅紫便應高閣束。|千尋結架上青冥，山外水邊隨詰曲。|飄然來及牡丹時，潔白自知...       S     史浩  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poetry_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 311828/311828 [00:01<00:00, 293151.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from hanziconv import HanziConv as HC\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 假设 HC.toSimplified 是转换函数\n",
    "def process_row(row):\n",
    "    row['content'] = HC.toSimplified(row['content'])\n",
    "    row['title'] = HC.toSimplified(row['title'])\n",
    "    row['author'] = HC.toSimplified(row['author'])\n",
    "    return row\n",
    "\n",
    "# 并行处理并显示进度\n",
    "def parallel_process_with_progress(df):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(14) as executor:\n",
    "        # 创建任务\n",
    "        tasks = {executor.submit(process_row, row): i for i, row in df.iterrows()}\n",
    "        # 使用 tqdm 显示进度\n",
    "        for future in tqdm(as_completed(tasks), total=len(tasks)):\n",
    "            results.append(future.result())\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 转换\n",
    "poetry_df = parallel_process_with_progress(poetry_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>yunlv_rule</th>\n",
       "      <th>author_id</th>\n",
       "      <th>content</th>\n",
       "      <th>dynasty</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>275205</th>\n",
       "      <td>275206</td>\n",
       "      <td>题天柱观鱼尊师旧院</td>\n",
       "      <td>仄仄平平平仄仄，平平仄仄仄平平。|平平仄仄平平仄，平仄仄平仄仄平。</td>\n",
       "      <td>1360</td>\n",
       "      <td>早识吾师频到此，芝童药犬亦相迎。|今师一去无来日，花洞石坛空月明。</td>\n",
       "      <td>T</td>\n",
       "      <td>方干</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143896</th>\n",
       "      <td>143897</td>\n",
       "      <td>新岭庵  其六</td>\n",
       "      <td>平平仄仄仄平平，平仄平平仄仄平。|仄仄仄平平仄仄，平平仄仄仄平平。</td>\n",
       "      <td>7516</td>\n",
       "      <td>中原回首涕沾裳，谁是当时柱石强。|会唤谪仙天上去，扶将日毂出扶桑。</td>\n",
       "      <td>S</td>\n",
       "      <td>邓柞</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75222</th>\n",
       "      <td>75223</td>\n",
       "      <td>蔡京葬处女冠居之</td>\n",
       "      <td>仄仄平平仄仄平，平平平仄仄平平。|平平平仄平平仄，平仄平平仄仄平。</td>\n",
       "      <td>9241</td>\n",
       "      <td>观里黄蜂晚退衙，观前桃着小春花。|更无寻访麻姑处，零落风烟似蔡家。</td>\n",
       "      <td>S</td>\n",
       "      <td>释居简</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158840</th>\n",
       "      <td>158841</td>\n",
       "      <td>五月十三日种竹偶成二首  其二</td>\n",
       "      <td>平平平仄仄，平仄仄平平。|仄仄平平仄，平平仄仄平。|仄平平仄仄，仄仄仄平平。|平仄平平仄，平...</td>\n",
       "      <td>7054</td>\n",
       "      <td>园夫扶出径，元是醉爲媒。|密叶疏疏理，纤根细细栽。|卷帘斜日避，倚杖好风来。|佳客如相顾，还...</td>\n",
       "      <td>S</td>\n",
       "      <td>郭印</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304932</th>\n",
       "      <td>304933</td>\n",
       "      <td>苑舍人能书梵字兼达梵音皆曲尽其妙戏爲之赠</td>\n",
       "      <td>平平仄仄仄平平，平仄平平仄仄平。|平平仄○平平仄，仄仄平平仄仄平。|仄平仄仄○平仄，仄仄平平...</td>\n",
       "      <td>125</td>\n",
       "      <td>名儒待诏满公车，才子爲郎典石渠。|莲花法藏心悬悟，贝叶经文手自书。|楚词共许胜杨马，梵字何人...</td>\n",
       "      <td>T</td>\n",
       "      <td>王维</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217218</th>\n",
       "      <td>217219</td>\n",
       "      <td>和对雪</td>\n",
       "      <td>平平平仄仄，仄仄仄○平。|平仄平平仄，平平仄○平。|仄平平仄仄，平○○平平。|仄仄平平仄，○...</td>\n",
       "      <td>5192</td>\n",
       "      <td>严冬飞朔雪，此景望中嘉。|云暗垂天幕，江平压浪花。|翅寒低独鴈，枝重折丛葭。|冷甲穷边戍，凝...</td>\n",
       "      <td>S</td>\n",
       "      <td>吕陶</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136394</th>\n",
       "      <td>136395</td>\n",
       "      <td>师廷珍以诗见惠因用其韵  其三</td>\n",
       "      <td>平仄平平仄仄仄，仄平仄仄仄平平。|仄平仄仄平平仄，仄仄平平仄仄平。</td>\n",
       "      <td>7803</td>\n",
       "      <td>诗礼初闻卫尉旁，起家期擅蜀文章。|快吟芍药当阶句，剩看芙蓉别殿香。</td>\n",
       "      <td>S</td>\n",
       "      <td>晁公遡</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250552</th>\n",
       "      <td>250553</td>\n",
       "      <td>出商州有感</td>\n",
       "      <td>平平平仄仄平平，仄仄平平仄仄平。|平仄平平仄平仄，平平平仄仄平平。</td>\n",
       "      <td>3991</td>\n",
       "      <td>圜丘恩例得量移，笑领全家出翠微。|惟有来时的颅马，商山埋骨不同归。</td>\n",
       "      <td>S</td>\n",
       "      <td>王禹偁</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226155</th>\n",
       "      <td>226156</td>\n",
       "      <td>端午内中帖子词 皇帝阁 其五</td>\n",
       "      <td>仄仄平平仄，平平仄仄平。|平平平仄仄，平仄仄平平。</td>\n",
       "      <td>4936</td>\n",
       "      <td>万里来珍贡，风澜静四溟。|鱼龙惊咒水，神雾忽生庭。</td>\n",
       "      <td>S</td>\n",
       "      <td>王珪</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311760</th>\n",
       "      <td>311761</td>\n",
       "      <td>横吹曲辞 出塞</td>\n",
       "      <td>平平平仄仄平平，仄仄平平仄仄○。|仄平○仄平○仄，平仄平平仄仄平。|仄平仄仄平○仄，仄仄○平...</td>\n",
       "      <td>125</td>\n",
       "      <td>居延城外猎天骄，白草连天野火烧。|暮云空碛时驱马，秋日平原好射鵰。|护羌校尉朝乘障，破虏将军...</td>\n",
       "      <td>T</td>\n",
       "      <td>王维</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                 title  \\\n",
       "275205  275206             题天柱观鱼尊师旧院   \n",
       "143896  143897               新岭庵  其六   \n",
       "75222    75223              蔡京葬处女冠居之   \n",
       "158840  158841       五月十三日种竹偶成二首  其二   \n",
       "304932  304933  苑舍人能书梵字兼达梵音皆曲尽其妙戏爲之赠   \n",
       "217218  217219                   和对雪   \n",
       "136394  136395       师廷珍以诗见惠因用其韵  其三   \n",
       "250552  250553                 出商州有感   \n",
       "226155  226156        端午内中帖子词 皇帝阁 其五   \n",
       "311760  311761               横吹曲辞 出塞   \n",
       "\n",
       "                                               yunlv_rule  author_id  \\\n",
       "275205                  仄仄平平平仄仄，平平仄仄仄平平。|平平仄仄平平仄，平仄仄平仄仄平。       1360   \n",
       "143896                  平平仄仄仄平平，平仄平平仄仄平。|仄仄仄平平仄仄，平平仄仄仄平平。       7516   \n",
       "75222                   仄仄平平仄仄平，平平平仄仄平平。|平平平仄平平仄，平仄平平仄仄平。       9241   \n",
       "158840  平平平仄仄，平仄仄平平。|仄仄平平仄，平平仄仄平。|仄平平仄仄，仄仄仄平平。|平仄平平仄，平...       7054   \n",
       "304932  平平仄仄仄平平，平仄平平仄仄平。|平平仄○平平仄，仄仄平平仄仄平。|仄平仄仄○平仄，仄仄平平...        125   \n",
       "217218  平平平仄仄，仄仄仄○平。|平仄平平仄，平平仄○平。|仄平平仄仄，平○○平平。|仄仄平平仄，○...       5192   \n",
       "136394                  平仄平平仄仄仄，仄平仄仄仄平平。|仄平仄仄平平仄，仄仄平平仄仄平。       7803   \n",
       "250552                  平平平仄仄平平，仄仄平平仄仄平。|平仄平平仄平仄，平平平仄仄平平。       3991   \n",
       "226155                          仄仄平平仄，平平仄仄平。|平平平仄仄，平仄仄平平。       4936   \n",
       "311760  平平平仄仄平平，仄仄平平仄仄○。|仄平○仄平○仄，平仄平平仄仄平。|仄平仄仄平○仄，仄仄○平...        125   \n",
       "\n",
       "                                                  content dynasty author  \n",
       "275205                  早识吾师频到此，芝童药犬亦相迎。|今师一去无来日，花洞石坛空月明。       T     方干  \n",
       "143896                  中原回首涕沾裳，谁是当时柱石强。|会唤谪仙天上去，扶将日毂出扶桑。       S     邓柞  \n",
       "75222                   观里黄蜂晚退衙，观前桃着小春花。|更无寻访麻姑处，零落风烟似蔡家。       S    释居简  \n",
       "158840  园夫扶出径，元是醉爲媒。|密叶疏疏理，纤根细细栽。|卷帘斜日避，倚杖好风来。|佳客如相顾，还...       S     郭印  \n",
       "304932  名儒待诏满公车，才子爲郎典石渠。|莲花法藏心悬悟，贝叶经文手自书。|楚词共许胜杨马，梵字何人...       T     王维  \n",
       "217218  严冬飞朔雪，此景望中嘉。|云暗垂天幕，江平压浪花。|翅寒低独鴈，枝重折丛葭。|冷甲穷边戍，凝...       S     吕陶  \n",
       "136394                  诗礼初闻卫尉旁，起家期擅蜀文章。|快吟芍药当阶句，剩看芙蓉别殿香。       S    晁公遡  \n",
       "250552                  圜丘恩例得量移，笑领全家出翠微。|惟有来时的颅马，商山埋骨不同归。       S    王禹偁  \n",
       "226155                          万里来珍贡，风澜静四溟。|鱼龙惊咒水，神雾忽生庭。       S     王珪  \n",
       "311760  居延城外猎天骄，白草连天野火烧。|暮云空碛时驱马，秋日平原好射鵰。|护羌校尉朝乘障，破虏将军...       T     王维  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poetry_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254237\n"
     ]
    }
   ],
   "source": [
    "df_TS_all=poetry_df[poetry_df['dynasty']=='S']\n",
    "print(len(df_TS_all))\n",
    "df_TS=df_TS_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class RNN_model(nn.Module):\n",
    "    def __init__(self, vocab_len ,word_embedding, embedding_dim, lstm_hidden_dim):\n",
    "        super(RNN_model,self).__init__()\n",
    "        self.word_embedding_lookup = word_embedding\n",
    "        self.vocab_length = vocab_len  #可选择的单词数目 或者说 word embedding层的word数目\n",
    "        self.word_embedding_dim = embedding_dim\n",
    "        self.lstm_dim = lstm_hidden_dim\n",
    "        self.rnn_lstm = nn.LSTM(input_size=embedding_dim, \n",
    "                                 hidden_size=lstm_hidden_dim, \n",
    "                                 num_layers=2,\n",
    "                                 batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(self.lstm_dim, self.vocab_length)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self,sentence,batch_size,is_test = False):\n",
    "        batch_input = self.word_embedding_lookup(sentence).view(batch_size,-1,self.word_embedding_dim)\n",
    "        hidden = torch.zeros(2, batch_size, self.lstm_dim).to(next(self.parameters()).device)\n",
    "        cell = torch.zeros(2, batch_size, self.lstm_dim).to(next(self.parameters()).device)\n",
    "        output, _ = self.rnn_lstm(batch_input, (hidden, cell))\n",
    "\n",
    "        out = output.contiguous().view(-1,self.lstm_dim)\n",
    "        out = self.fc(out)   #out.size: (batch_size * sequence_length ,vocab_length)\n",
    "        if is_test:\n",
    "            #测试阶段(或者说生成诗句阶段)使用\n",
    "            prediction = out[ -1, : ].view(1,-1)\n",
    "            output = prediction\n",
    "        else:\n",
    "            #训练阶段使用\n",
    "           output = out\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_embedding(nn.Module):\n",
    "    def __init__(self,vocab_length , embedding_dim):\n",
    "        super(word_embedding, self).__init__()\n",
    "        w_embeding_random_intial = np.random.uniform(-1,1,size=(vocab_length ,embedding_dim))#初始化\n",
    "        self.word_embedding = nn.Embedding(vocab_length,embedding_dim)\n",
    "        self.word_embedding.weight.data.copy_(torch.from_numpy(w_embeding_random_intial))\n",
    "    def forward(self,input_sentence):\n",
    "        \"\"\"\n",
    "        :param input_sentence:  a tensor ,contain several word index.\n",
    "        :return: a tensor ,contain word embedding tensor\n",
    "        \"\"\"\n",
    "        sen_embed = self.word_embedding(input_sentence)\n",
    "        return sen_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "poem_begin = 'X' #诗句开始的标志\n",
    "poem_end = 'Y'   #诗句结束的标志\n",
    "BATCH_SIZE = 512\n",
    "WORD_EMBEDDING_DIM = 128\n",
    "LSTM_HIDDEN_DIM =256\n",
    "\n",
    "\n",
    "\n",
    "disallow = ['（', '）', '(', ')', '__', '《', '》', '【', '】', '[', ']','+','-','/']\n",
    "\n",
    "def process_poems():\n",
    "    #函数用于数据预处理\n",
    "    #:return: poems_vector, word_int_map, int_word_map\n",
    "    #         poems_vector  是个2维的list ,第一维度对应每句诗, 第二个维度对应着每句诗的word index\n",
    "    #         word_int_map  是个字典(dict)，key为所有诗句出现的字符，value则是字符对应的word index\n",
    "    #         int_word_amp  也是个字典(dict),key为所有诗句出现的字符对应的word index,value为word index对应的字符\n",
    "    #读取文件\n",
    "    count=0\n",
    "    poems = pd.DataFrame()\n",
    "    poems_list = [] \n",
    "    max_length=60\n",
    "    df_TS.dropna()\n",
    "    for i in range(0,len(df_TS)):  # 遍历指定列，跳过缺失值\n",
    "        ignore_flag =False\n",
    "        title=df_TS['title'].iloc[i]\n",
    "        poem=poem_begin \n",
    "        content=df_TS['content'].iloc[i]\n",
    "        for dis_word in disallow:\n",
    "            if dis_word in content or dis_word in poem:\n",
    "                ignore_flag = True\n",
    "                break\n",
    "        if ignore_flag:\n",
    "            continue\n",
    "        if len(content)> max_length:\n",
    "            continue\n",
    "        \n",
    "        for line in content.split('|'):  # 按照 | 分割诗句\n",
    "            line = line.strip()  # 去掉首尾空格\n",
    "            poem= poem+line\n",
    "        poem=poem+poem_end\n",
    "        poems_list.append({'title': title, 'poem': poem})\n",
    "    poems = pd.DataFrame(poems_list)\n",
    "\n",
    "    # 提取所有诗句内容并拼接成一个字符串\n",
    "\n",
    "    all_word=''.join(poems['title'].tolist()) +''.join(poems['poem'].tolist())  \n",
    "    words = sorted(set(all_word))\n",
    "    word_int_map = {c: i for i, c in enumerate(words)}\n",
    "    int_word_map = {i: c for i, c in enumerate(words)}\n",
    "    \n",
    "    # 转换为索引，并填充/截断\n",
    "    poems_vector = []\n",
    "    for i in range(len(poems)):\n",
    "        poem = poems['poem'].iloc[i]\n",
    "        title= poems['title'].iloc[i]\n",
    "        poetry=title+poem\n",
    "        vector= [word_int_map[word] for word in poetry]\n",
    "        # 填充或截断序列\n",
    "        if len(vector) > max_length:  # 截断\n",
    "            vector = vector[:max_length]\n",
    "        elif len(vector) < max_length:  # 填充\n",
    "            vector= vector + [0] * (max_length - len(vector))\n",
    "        poems_vector.append(vector)\n",
    "    \n",
    "    return poems_vector, word_int_map, int_word_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch_size, poems_vec):\n",
    "    #用于生成batch\n",
    "    n_chunk = len(poems_vec) // batch_size\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    for i in range(n_chunk):\n",
    "        start_index = i * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        x_data = [poem_vec[:-1] for poem_vec in poems_vec[start_index:end_index]]\n",
    "        y_data = [poem_vec[1: ] for poem_vec in poems_vec[start_index:end_index]]\n",
    "        x_batches.append(x_data)\n",
    "        y_batches.append(y_data)\n",
    "    return x_batches, y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "class PoemDataset(Dataset):\n",
    "    def __init__(self, poems_vec):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        :param poems_vec: 整个诗向量化的列表，每首诗是一个向量\n",
    "        \"\"\"\n",
    "        self.data = poems_vec\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        返回数据集中样本数量\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        获取数据中的一个样本，返回输入和目标\n",
    "        :param idx: 样本索引\n",
    "        \"\"\"\n",
    "        poem_vec = self.data[idx]\n",
    "        x_data = torch.tensor(poem_vec[:-1], dtype=torch.long)  # 输入为前 n-1 个单词\n",
    "        y_data = torch.tensor(poem_vec[1:], dtype=torch.long)  # 输出为后 n-1 个单词\n",
    "        return x_data, y_data\n",
    "\n",
    "\n",
    "\n",
    "def run_training():\n",
    "    # 处理数据集\n",
    "    poems_vector, word_to_int, int_to_word = process_poems()\n",
    "    # 建立word embedding层(用于根据word index得到对应word的embedding表示，或者说向量表示)\n",
    "    word_embedding1 = word_embedding( vocab_length= len(word_to_int) + 1 , embedding_dim= WORD_EMBEDDING_DIM)\n",
    "    # 建立RNN模型,之前建立的word embedding层作为它的一部分\n",
    "    rnn_model =RNN_model(vocab_len = len(word_to_int) + 1 ,\n",
    "                              word_embedding = word_embedding1 ,\n",
    "                              embedding_dim= WORD_EMBEDDING_DIM,\n",
    "                              lstm_hidden_dim=LSTM_HIDDEN_DIM)\n",
    "    optimizer = optim.Adam(rnn_model.parameters(), lr=3e-3)\n",
    "    Criterion = nn.CrossEntropyLoss()\n",
    "    # Learning rate scheduler\n",
    "    BATCH_SIZE=128\n",
    "        # 创建数据集\n",
    "    dataset = PoemDataset(poems_vector)\n",
    "    # 创建 DataLoader\n",
    "    data_loader = DataLoader(\n",
    "        dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,  # 打乱数据顺序\n",
    "\n",
    "        drop_last=True  # 保证每个 batch 的大小一致\n",
    "    )\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    for epoch in range(30):\n",
    "        start_time = time.time()\n",
    "        rnn_model.train()\n",
    "        total_loss = 0\n",
    "        # 生成batch\n",
    "        # 混合精度训练\n",
    "        scaler = GradScaler()\n",
    "        for batch_num, (batch_x, batch_y) in enumerate(data_loader):\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            batch_size = batch_x.size(0)\n",
    "\n",
    "            # 混合精度训练\n",
    "            with autocast():\n",
    "                predictions = rnn_model(batch_x, batch_size=batch_size)\n",
    "                loss = Criterion(predictions, batch_y.view(-1))  # 交叉熵计算\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            nn.utils.clip_grad_norm_(rnn_model.parameters(), 1)  # 梯度裁剪\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 打印日志\n",
    "            if (batch_num + 1) % 10 == 0:  # 每10个batch打印一次\n",
    "                print(f\"Epoch {epoch + 1}/30, Batch {batch_num + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "        torch.save(rnn_model.state_dict(), './poem_generator_rnn_song')\n",
    "        print(\"finish  save model of epoch : {}!\".format(epoch))\n",
    "        print(\"epoch using time {:.3f}\".format(time.time()-start_time))\n",
    "\n",
    "# 示例用法\n",
    "# run_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_word(predict, int_to_word, poem):\n",
    "    # 预测的结果转化成对应字符\n",
    "    sample = np.argmax(predict)\n",
    "    return int_to_word[sample]\n",
    "\n",
    "def gen_poem(begin_word):\n",
    "    #用于测试，或者说古诗生成\n",
    "    poems_vector, word_int_map, int_word_map = process_poems()\n",
    "    word_embedding1 = word_embedding(vocab_length=len(word_int_map) + 1, embedding_dim=WORD_EMBEDDING_DIM)\n",
    "    rnn_model = RNN_model(vocab_len=len(word_int_map) + 1,\n",
    "                              word_embedding=word_embedding1,\n",
    "                              embedding_dim=WORD_EMBEDDING_DIM,\n",
    "                              lstm_hidden_dim=LSTM_HIDDEN_DIM)\n",
    "    rnn_model.load_state_dict(torch.load('./poem_generator_rnn_song'), strict=False) #读取训练得到的模型\n",
    "\n",
    "    # 指定开始的字\n",
    "    poem=begin_word\n",
    "    word = begin_word\n",
    "    while word != poem_end :\n",
    "        input = np.array([word_int_map[w] for w in poem],dtype= np.int64)\n",
    "        input = torch.from_numpy(input)\n",
    "        output = rnn_model(input, batch_size = 1, is_test=True)\n",
    "        word = to_word(output.data.numpy()[-1], int_word_map, poem)\n",
    "        poem += word\n",
    "        if len(poem) > 60:\n",
    "            break\n",
    "    return poem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共处理有效诗句数量: 151798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\python\\Lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "m:\\python\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Batch 10, Loss: 5.6307\n",
      "Epoch 1/30, Batch 20, Loss: 5.6851\n",
      "Epoch 1/30, Batch 30, Loss: 5.6724\n",
      "Epoch 1/30, Batch 40, Loss: 5.8639\n",
      "Epoch 1/30, Batch 50, Loss: 5.4756\n",
      "Epoch 1/30, Batch 60, Loss: 5.2656\n",
      "Epoch 1/30, Batch 70, Loss: 5.1948\n",
      "Epoch 1/30, Batch 80, Loss: 5.0765\n",
      "Epoch 1/30, Batch 90, Loss: 5.0110\n",
      "Epoch 1/30, Batch 100, Loss: 4.9260\n",
      "Epoch 1/30, Batch 110, Loss: 4.8278\n",
      "Epoch 1/30, Batch 120, Loss: 5.0335\n",
      "Epoch 1/30, Batch 130, Loss: 4.8644\n",
      "Epoch 1/30, Batch 140, Loss: 4.7018\n",
      "Epoch 1/30, Batch 150, Loss: 4.7779\n",
      "Epoch 1/30, Batch 160, Loss: 4.5574\n",
      "Epoch 1/30, Batch 170, Loss: 4.7297\n",
      "Epoch 1/30, Batch 180, Loss: 4.8042\n",
      "Epoch 1/30, Batch 190, Loss: 4.7841\n",
      "Epoch 1/30, Batch 200, Loss: 4.8443\n",
      "Epoch 1/30, Batch 210, Loss: 4.6046\n",
      "Epoch 1/30, Batch 220, Loss: 4.7415\n",
      "Epoch 1/30, Batch 230, Loss: 4.5541\n",
      "Epoch 1/30, Batch 240, Loss: 4.5737\n",
      "Epoch 1/30, Batch 250, Loss: 4.4851\n",
      "Epoch 1/30, Batch 260, Loss: 4.4945\n",
      "Epoch 1/30, Batch 270, Loss: 4.4851\n",
      "Epoch 1/30, Batch 280, Loss: 4.5386\n",
      "Epoch 1/30, Batch 290, Loss: 4.4376\n",
      "Epoch 1/30, Batch 300, Loss: 4.4981\n",
      "Epoch 1/30, Batch 310, Loss: 4.2875\n",
      "Epoch 1/30, Batch 320, Loss: 4.4934\n",
      "Epoch 1/30, Batch 330, Loss: 4.4016\n",
      "Epoch 1/30, Batch 340, Loss: 4.3783\n",
      "Epoch 1/30, Batch 350, Loss: 4.5348\n",
      "Epoch 1/30, Batch 360, Loss: 4.3275\n",
      "Epoch 1/30, Batch 370, Loss: 4.3743\n",
      "Epoch 1/30, Batch 380, Loss: 4.5952\n",
      "Epoch 1/30, Batch 390, Loss: 4.3442\n",
      "Epoch 1/30, Batch 400, Loss: 4.2396\n",
      "Epoch 1/30, Batch 410, Loss: 4.1983\n",
      "Epoch 1/30, Batch 420, Loss: 4.2279\n",
      "Epoch 1/30, Batch 430, Loss: 4.2684\n",
      "Epoch 1/30, Batch 440, Loss: 4.5209\n",
      "Epoch 1/30, Batch 450, Loss: 4.2670\n",
      "Epoch 1/30, Batch 460, Loss: 4.2658\n",
      "Epoch 1/30, Batch 470, Loss: 4.4454\n",
      "Epoch 1/30, Batch 480, Loss: 4.2100\n",
      "Epoch 1/30, Batch 490, Loss: 4.2370\n",
      "Epoch 1/30, Batch 500, Loss: 4.2975\n",
      "Epoch 1/30, Batch 510, Loss: 4.2234\n",
      "Epoch 1/30, Batch 520, Loss: 4.1834\n",
      "Epoch 1/30, Batch 530, Loss: 4.3786\n",
      "Epoch 1/30, Batch 540, Loss: 4.2216\n",
      "Epoch 1/30, Batch 550, Loss: 4.2851\n",
      "Epoch 1/30, Batch 560, Loss: 4.1409\n",
      "Epoch 1/30, Batch 570, Loss: 4.2192\n",
      "Epoch 1/30, Batch 580, Loss: 4.0433\n",
      "Epoch 1/30, Batch 590, Loss: 4.0571\n",
      "Epoch 1/30, Batch 600, Loss: 4.2128\n",
      "Epoch 1/30, Batch 610, Loss: 4.2787\n",
      "Epoch 1/30, Batch 620, Loss: 4.2416\n",
      "Epoch 1/30, Batch 630, Loss: 4.1815\n",
      "Epoch 1/30, Batch 640, Loss: 4.1983\n",
      "Epoch 1/30, Batch 650, Loss: 4.2305\n",
      "Epoch 1/30, Batch 660, Loss: 4.0671\n",
      "Epoch 1/30, Batch 670, Loss: 4.1714\n",
      "Epoch 1/30, Batch 680, Loss: 4.1994\n",
      "Epoch 1/30, Batch 690, Loss: 4.2984\n",
      "Epoch 1/30, Batch 700, Loss: 4.1409\n",
      "Epoch 1/30, Batch 710, Loss: 4.1120\n",
      "Epoch 1/30, Batch 720, Loss: 4.1583\n",
      "Epoch 1/30, Batch 730, Loss: 4.3000\n",
      "Epoch 1/30, Batch 740, Loss: 4.1541\n",
      "Epoch 1/30, Batch 750, Loss: 4.1643\n",
      "Epoch 1/30, Batch 760, Loss: 4.2067\n",
      "Epoch 1/30, Batch 770, Loss: 4.0000\n",
      "Epoch 1/30, Batch 780, Loss: 4.1080\n",
      "Epoch 1/30, Batch 790, Loss: 3.9659\n",
      "Epoch 1/30, Batch 800, Loss: 4.0407\n",
      "Epoch 1/30, Batch 810, Loss: 4.0147\n",
      "Epoch 1/30, Batch 820, Loss: 3.9047\n",
      "Epoch 1/30, Batch 830, Loss: 3.9891\n",
      "Epoch 1/30, Batch 840, Loss: 4.0077\n",
      "Epoch 1/30, Batch 850, Loss: 4.0167\n",
      "Epoch 1/30, Batch 860, Loss: 4.0720\n",
      "Epoch 1/30, Batch 870, Loss: 3.9948\n",
      "Epoch 1/30, Batch 880, Loss: 4.0195\n",
      "Epoch 1/30, Batch 890, Loss: 3.9408\n",
      "Epoch 1/30, Batch 900, Loss: 4.0335\n",
      "Epoch 1/30, Batch 910, Loss: 4.0232\n",
      "Epoch 1/30, Batch 920, Loss: 3.8428\n",
      "Epoch 1/30, Batch 930, Loss: 3.8818\n",
      "Epoch 1/30, Batch 940, Loss: 3.8078\n",
      "Epoch 1/30, Batch 950, Loss: 4.0574\n",
      "Epoch 1/30, Batch 960, Loss: 3.9178\n",
      "Epoch 1/30, Batch 970, Loss: 4.2145\n",
      "Epoch 1/30, Batch 980, Loss: 4.0686\n",
      "Epoch 1/30, Batch 990, Loss: 3.9005\n",
      "Epoch 1/30, Batch 1000, Loss: 3.8572\n",
      "Epoch 1/30, Batch 1010, Loss: 3.8253\n",
      "Epoch 1/30, Batch 1020, Loss: 3.8903\n",
      "Epoch 1/30, Batch 1030, Loss: 3.8917\n",
      "Epoch 1/30, Batch 1040, Loss: 3.9600\n",
      "Epoch 1/30, Batch 1050, Loss: 3.8673\n",
      "Epoch 1/30, Batch 1060, Loss: 3.9824\n",
      "Epoch 1/30, Batch 1070, Loss: 3.9019\n",
      "Epoch 1/30, Batch 1080, Loss: 3.8581\n",
      "Epoch 1/30, Batch 1090, Loss: 3.6040\n",
      "Epoch 1/30, Batch 1100, Loss: 3.8448\n",
      "Epoch 1/30, Batch 1110, Loss: 3.9391\n",
      "Epoch 1/30, Batch 1120, Loss: 3.9806\n",
      "Epoch 1/30, Batch 1130, Loss: 3.7008\n",
      "Epoch 1/30, Batch 1140, Loss: 3.8638\n",
      "Epoch 1/30, Batch 1150, Loss: 3.7761\n",
      "Epoch 1/30, Batch 1160, Loss: 3.7453\n",
      "Epoch 1/30, Batch 1170, Loss: 3.8799\n",
      "Epoch 1/30, Batch 1180, Loss: 3.7809\n",
      "finish  save model of epoch : 0!\n",
      "epoch using time 1076.132\n",
      "Epoch 2/30, Batch 10, Loss: 3.7977\n",
      "Epoch 2/30, Batch 20, Loss: 3.8088\n",
      "Epoch 2/30, Batch 30, Loss: 3.9106\n",
      "Epoch 2/30, Batch 40, Loss: 3.6058\n",
      "Epoch 2/30, Batch 50, Loss: 3.7384\n",
      "Epoch 2/30, Batch 60, Loss: 3.8925\n",
      "Epoch 2/30, Batch 70, Loss: 3.7069\n",
      "Epoch 2/30, Batch 80, Loss: 3.5730\n",
      "Epoch 2/30, Batch 90, Loss: 3.7194\n",
      "Epoch 2/30, Batch 100, Loss: 3.8008\n",
      "Epoch 2/30, Batch 110, Loss: 3.8782\n",
      "Epoch 2/30, Batch 120, Loss: 3.7381\n",
      "Epoch 2/30, Batch 130, Loss: 3.8590\n",
      "Epoch 2/30, Batch 140, Loss: 3.8038\n",
      "Epoch 2/30, Batch 150, Loss: 3.6029\n",
      "Epoch 2/30, Batch 160, Loss: 3.8191\n",
      "Epoch 2/30, Batch 170, Loss: 3.7101\n",
      "Epoch 2/30, Batch 180, Loss: 3.7138\n",
      "Epoch 2/30, Batch 190, Loss: 3.7219\n",
      "Epoch 2/30, Batch 200, Loss: 3.7044\n",
      "Epoch 2/30, Batch 210, Loss: 3.8303\n",
      "Epoch 2/30, Batch 220, Loss: 3.7694\n",
      "Epoch 2/30, Batch 230, Loss: 3.7053\n",
      "Epoch 2/30, Batch 240, Loss: 3.7116\n",
      "Epoch 2/30, Batch 250, Loss: 3.6282\n",
      "Epoch 2/30, Batch 260, Loss: 3.6013\n",
      "Epoch 2/30, Batch 270, Loss: 3.5790\n",
      "Epoch 2/30, Batch 280, Loss: 3.7019\n",
      "Epoch 2/30, Batch 290, Loss: 3.4737\n",
      "Epoch 2/30, Batch 300, Loss: 3.6597\n",
      "Epoch 2/30, Batch 310, Loss: 3.6528\n",
      "Epoch 2/30, Batch 320, Loss: 3.6811\n",
      "Epoch 2/30, Batch 330, Loss: 3.7095\n",
      "Epoch 2/30, Batch 340, Loss: 3.7908\n",
      "Epoch 2/30, Batch 350, Loss: 3.6305\n",
      "Epoch 2/30, Batch 360, Loss: 3.5112\n",
      "Epoch 2/30, Batch 370, Loss: 3.5977\n",
      "Epoch 2/30, Batch 380, Loss: 3.6346\n",
      "Epoch 2/30, Batch 390, Loss: 3.6061\n",
      "Epoch 2/30, Batch 400, Loss: 3.5837\n",
      "Epoch 2/30, Batch 410, Loss: 3.5429\n",
      "Epoch 2/30, Batch 420, Loss: 3.4567\n",
      "Epoch 2/30, Batch 430, Loss: 3.6854\n",
      "Epoch 2/30, Batch 440, Loss: 3.6858\n",
      "Epoch 2/30, Batch 450, Loss: 3.6815\n",
      "Epoch 2/30, Batch 460, Loss: 3.6166\n",
      "Epoch 2/30, Batch 470, Loss: 3.5442\n",
      "Epoch 2/30, Batch 480, Loss: 3.5906\n",
      "Epoch 2/30, Batch 490, Loss: 3.4990\n",
      "Epoch 2/30, Batch 500, Loss: 3.7732\n",
      "Epoch 2/30, Batch 510, Loss: 3.7848\n",
      "Epoch 2/30, Batch 520, Loss: 3.4816\n",
      "Epoch 2/30, Batch 530, Loss: 3.5661\n",
      "Epoch 2/30, Batch 540, Loss: 3.6211\n",
      "Epoch 2/30, Batch 550, Loss: 3.6567\n",
      "Epoch 2/30, Batch 560, Loss: 3.5344\n",
      "Epoch 2/30, Batch 570, Loss: 3.4818\n",
      "Epoch 2/30, Batch 580, Loss: 3.6374\n",
      "Epoch 2/30, Batch 590, Loss: 3.6505\n",
      "Epoch 2/30, Batch 600, Loss: 3.5212\n",
      "Epoch 2/30, Batch 610, Loss: 3.5484\n",
      "Epoch 2/30, Batch 620, Loss: 3.6600\n",
      "Epoch 2/30, Batch 630, Loss: 3.5440\n",
      "Epoch 2/30, Batch 640, Loss: 3.5576\n",
      "Epoch 2/30, Batch 650, Loss: 3.5319\n",
      "Epoch 2/30, Batch 660, Loss: 3.6645\n",
      "Epoch 2/30, Batch 670, Loss: 3.5388\n",
      "Epoch 2/30, Batch 680, Loss: 3.5655\n",
      "Epoch 2/30, Batch 690, Loss: 3.5991\n",
      "Epoch 2/30, Batch 700, Loss: 3.5697\n",
      "Epoch 2/30, Batch 710, Loss: 3.5447\n",
      "Epoch 2/30, Batch 720, Loss: 3.4416\n",
      "Epoch 2/30, Batch 730, Loss: 3.5257\n",
      "Epoch 2/30, Batch 740, Loss: 3.5960\n",
      "Epoch 2/30, Batch 750, Loss: 3.7371\n",
      "Epoch 2/30, Batch 760, Loss: 3.5816\n",
      "Epoch 2/30, Batch 770, Loss: 3.5695\n",
      "Epoch 2/30, Batch 780, Loss: 3.5254\n",
      "Epoch 2/30, Batch 790, Loss: 3.6807\n",
      "Epoch 2/30, Batch 800, Loss: 3.7875\n",
      "Epoch 2/30, Batch 810, Loss: 3.3939\n",
      "Epoch 2/30, Batch 820, Loss: 3.4761\n",
      "Epoch 2/30, Batch 830, Loss: 3.5266\n",
      "Epoch 2/30, Batch 840, Loss: 3.5070\n",
      "Epoch 2/30, Batch 850, Loss: 3.4994\n",
      "Epoch 2/30, Batch 860, Loss: 3.5448\n",
      "Epoch 2/30, Batch 870, Loss: 3.5419\n",
      "Epoch 2/30, Batch 880, Loss: 3.4634\n",
      "Epoch 2/30, Batch 890, Loss: 3.4576\n",
      "Epoch 2/30, Batch 900, Loss: 3.5601\n",
      "Epoch 2/30, Batch 910, Loss: 3.4994\n",
      "Epoch 2/30, Batch 920, Loss: 3.6641\n",
      "Epoch 2/30, Batch 930, Loss: 3.4869\n",
      "Epoch 2/30, Batch 940, Loss: 3.5684\n",
      "Epoch 2/30, Batch 950, Loss: 3.5738\n",
      "Epoch 2/30, Batch 960, Loss: 3.7396\n",
      "Epoch 2/30, Batch 970, Loss: 3.5106\n",
      "Epoch 2/30, Batch 980, Loss: 3.4869\n",
      "Epoch 2/30, Batch 990, Loss: 3.5855\n",
      "Epoch 2/30, Batch 1000, Loss: 3.5538\n",
      "Epoch 2/30, Batch 1010, Loss: 3.4014\n",
      "Epoch 2/30, Batch 1020, Loss: 3.6753\n",
      "Epoch 2/30, Batch 1030, Loss: 3.3701\n",
      "Epoch 2/30, Batch 1040, Loss: 3.4714\n",
      "Epoch 2/30, Batch 1050, Loss: 3.5987\n",
      "Epoch 2/30, Batch 1060, Loss: 3.4799\n",
      "Epoch 2/30, Batch 1070, Loss: 3.5500\n",
      "Epoch 2/30, Batch 1080, Loss: 3.4388\n",
      "Epoch 2/30, Batch 1090, Loss: 3.5571\n",
      "Epoch 2/30, Batch 1100, Loss: 3.5808\n",
      "Epoch 2/30, Batch 1110, Loss: 3.4820\n",
      "Epoch 2/30, Batch 1120, Loss: 3.5253\n",
      "Epoch 2/30, Batch 1130, Loss: 3.3510\n",
      "Epoch 2/30, Batch 1140, Loss: 3.5515\n",
      "Epoch 2/30, Batch 1150, Loss: 3.4830\n",
      "Epoch 2/30, Batch 1160, Loss: 3.4440\n",
      "Epoch 2/30, Batch 1170, Loss: 3.5594\n",
      "Epoch 2/30, Batch 1180, Loss: 3.5344\n",
      "finish  save model of epoch : 1!\n",
      "epoch using time 1067.330\n",
      "Epoch 3/30, Batch 10, Loss: 3.3556\n",
      "Epoch 3/30, Batch 20, Loss: 3.4837\n",
      "Epoch 3/30, Batch 30, Loss: 3.4691\n",
      "Epoch 3/30, Batch 40, Loss: 3.3545\n",
      "Epoch 3/30, Batch 50, Loss: 3.3806\n",
      "Epoch 3/30, Batch 60, Loss: 3.5686\n",
      "Epoch 3/30, Batch 70, Loss: 3.3430\n",
      "Epoch 3/30, Batch 80, Loss: 3.4277\n",
      "Epoch 3/30, Batch 90, Loss: 3.5302\n",
      "Epoch 3/30, Batch 100, Loss: 3.3608\n",
      "Epoch 3/30, Batch 110, Loss: 3.3558\n",
      "Epoch 3/30, Batch 120, Loss: 3.3642\n",
      "Epoch 3/30, Batch 130, Loss: 3.2829\n",
      "Epoch 3/30, Batch 140, Loss: 3.3055\n",
      "Epoch 3/30, Batch 150, Loss: 3.5734\n",
      "Epoch 3/30, Batch 160, Loss: 3.5898\n",
      "Epoch 3/30, Batch 170, Loss: 3.5394\n",
      "Epoch 3/30, Batch 180, Loss: 3.5720\n",
      "Epoch 3/30, Batch 190, Loss: 3.5273\n",
      "Epoch 3/30, Batch 200, Loss: 3.4302\n",
      "Epoch 3/30, Batch 210, Loss: 3.4799\n",
      "Epoch 3/30, Batch 220, Loss: 3.4028\n",
      "Epoch 3/30, Batch 230, Loss: 3.5554\n",
      "Epoch 3/30, Batch 240, Loss: 3.6184\n",
      "Epoch 3/30, Batch 250, Loss: 3.4895\n",
      "Epoch 3/30, Batch 260, Loss: 3.5629\n",
      "Epoch 3/30, Batch 270, Loss: 3.4035\n",
      "Epoch 3/30, Batch 280, Loss: 3.3871\n",
      "Epoch 3/30, Batch 290, Loss: 3.4737\n",
      "Epoch 3/30, Batch 300, Loss: 3.3992\n",
      "Epoch 3/30, Batch 310, Loss: 3.6901\n",
      "Epoch 3/30, Batch 320, Loss: 3.1992\n",
      "Epoch 3/30, Batch 330, Loss: 3.4796\n",
      "Epoch 3/30, Batch 340, Loss: 3.5842\n",
      "Epoch 3/30, Batch 350, Loss: 3.3316\n",
      "Epoch 3/30, Batch 360, Loss: 3.4242\n",
      "Epoch 3/30, Batch 370, Loss: 3.5026\n",
      "Epoch 3/30, Batch 380, Loss: 3.3579\n",
      "Epoch 3/30, Batch 390, Loss: 3.5170\n",
      "Epoch 3/30, Batch 400, Loss: 3.4853\n",
      "Epoch 3/30, Batch 410, Loss: 3.3909\n",
      "Epoch 3/30, Batch 420, Loss: 3.4682\n",
      "Epoch 3/30, Batch 430, Loss: 3.4898\n",
      "Epoch 3/30, Batch 440, Loss: 3.3873\n",
      "Epoch 3/30, Batch 450, Loss: 3.2787\n",
      "Epoch 3/30, Batch 460, Loss: 3.4868\n",
      "Epoch 3/30, Batch 470, Loss: 3.4096\n",
      "Epoch 3/30, Batch 480, Loss: 3.3618\n",
      "Epoch 3/30, Batch 490, Loss: 3.3536\n",
      "Epoch 3/30, Batch 500, Loss: 3.3542\n",
      "Epoch 3/30, Batch 510, Loss: 3.4517\n",
      "Epoch 3/30, Batch 520, Loss: 3.5974\n",
      "Epoch 3/30, Batch 530, Loss: 3.4145\n",
      "Epoch 3/30, Batch 540, Loss: 3.4395\n",
      "Epoch 3/30, Batch 550, Loss: 3.4966\n",
      "Epoch 3/30, Batch 560, Loss: 3.2986\n",
      "Epoch 3/30, Batch 570, Loss: 3.3178\n",
      "Epoch 3/30, Batch 580, Loss: 3.3714\n",
      "Epoch 3/30, Batch 590, Loss: 3.2059\n",
      "Epoch 3/30, Batch 600, Loss: 3.4121\n",
      "Epoch 3/30, Batch 610, Loss: 3.4806\n",
      "Epoch 3/30, Batch 620, Loss: 3.3736\n",
      "Epoch 3/30, Batch 630, Loss: 3.4356\n",
      "Epoch 3/30, Batch 640, Loss: 3.4299\n",
      "Epoch 3/30, Batch 650, Loss: 3.2961\n",
      "Epoch 3/30, Batch 660, Loss: 3.2993\n",
      "Epoch 3/30, Batch 670, Loss: 3.4198\n",
      "Epoch 3/30, Batch 680, Loss: 3.4738\n",
      "Epoch 3/30, Batch 690, Loss: 3.2880\n",
      "Epoch 3/30, Batch 700, Loss: 3.4322\n",
      "Epoch 3/30, Batch 710, Loss: 3.3178\n",
      "Epoch 3/30, Batch 720, Loss: 3.3404\n",
      "Epoch 3/30, Batch 730, Loss: 3.3593\n",
      "Epoch 3/30, Batch 740, Loss: 3.3261\n",
      "Epoch 3/30, Batch 750, Loss: 3.5061\n",
      "Epoch 3/30, Batch 760, Loss: 3.2810\n",
      "Epoch 3/30, Batch 770, Loss: 3.4073\n",
      "Epoch 3/30, Batch 780, Loss: 3.1971\n",
      "Epoch 3/30, Batch 790, Loss: 3.3189\n",
      "Epoch 3/30, Batch 800, Loss: 3.2572\n",
      "Epoch 3/30, Batch 810, Loss: 3.4849\n",
      "Epoch 3/30, Batch 820, Loss: 3.3844\n",
      "Epoch 3/30, Batch 830, Loss: 3.3751\n",
      "Epoch 3/30, Batch 840, Loss: 3.3517\n",
      "Epoch 3/30, Batch 850, Loss: 3.3753\n",
      "Epoch 3/30, Batch 860, Loss: 3.4417\n",
      "Epoch 3/30, Batch 870, Loss: 3.3267\n",
      "Epoch 3/30, Batch 880, Loss: 3.3861\n",
      "Epoch 3/30, Batch 890, Loss: 3.3668\n",
      "Epoch 3/30, Batch 900, Loss: 3.3656\n",
      "Epoch 3/30, Batch 910, Loss: 3.2383\n",
      "Epoch 3/30, Batch 920, Loss: 3.3529\n",
      "Epoch 3/30, Batch 930, Loss: 3.3252\n",
      "Epoch 3/30, Batch 940, Loss: 3.3191\n",
      "Epoch 3/30, Batch 950, Loss: 3.3648\n",
      "Epoch 3/30, Batch 960, Loss: 3.4093\n",
      "Epoch 3/30, Batch 970, Loss: 3.3164\n",
      "Epoch 3/30, Batch 980, Loss: 3.4447\n",
      "Epoch 3/30, Batch 990, Loss: 3.3178\n",
      "Epoch 3/30, Batch 1000, Loss: 3.3373\n",
      "Epoch 3/30, Batch 1010, Loss: 3.4389\n",
      "Epoch 3/30, Batch 1020, Loss: 3.4980\n",
      "Epoch 3/30, Batch 1030, Loss: 3.3855\n",
      "Epoch 3/30, Batch 1040, Loss: 3.2959\n",
      "Epoch 3/30, Batch 1050, Loss: 3.3886\n",
      "Epoch 3/30, Batch 1060, Loss: 3.2347\n",
      "Epoch 3/30, Batch 1070, Loss: 3.3278\n",
      "Epoch 3/30, Batch 1080, Loss: 3.4609\n",
      "Epoch 3/30, Batch 1090, Loss: 3.4872\n",
      "Epoch 3/30, Batch 1100, Loss: 3.3723\n",
      "Epoch 3/30, Batch 1110, Loss: 3.4340\n",
      "Epoch 3/30, Batch 1120, Loss: 3.4117\n",
      "Epoch 3/30, Batch 1130, Loss: 3.4192\n",
      "Epoch 3/30, Batch 1140, Loss: 3.4052\n",
      "Epoch 3/30, Batch 1150, Loss: 3.2741\n",
      "Epoch 3/30, Batch 1160, Loss: 3.3263\n",
      "Epoch 3/30, Batch 1170, Loss: 3.2880\n",
      "Epoch 3/30, Batch 1180, Loss: 3.2351\n",
      "finish  save model of epoch : 2!\n",
      "epoch using time 1156.628\n",
      "Epoch 4/30, Batch 10, Loss: 3.2730\n",
      "Epoch 4/30, Batch 20, Loss: 3.3381\n",
      "Epoch 4/30, Batch 30, Loss: 3.3197\n",
      "Epoch 4/30, Batch 40, Loss: 3.2538\n",
      "Epoch 4/30, Batch 50, Loss: 3.2558\n",
      "Epoch 4/30, Batch 60, Loss: 3.1747\n",
      "Epoch 4/30, Batch 70, Loss: 3.3148\n",
      "Epoch 4/30, Batch 80, Loss: 3.3647\n",
      "Epoch 4/30, Batch 90, Loss: 3.1936\n",
      "Epoch 4/30, Batch 100, Loss: 3.3101\n",
      "Epoch 4/30, Batch 110, Loss: 3.2211\n",
      "Epoch 4/30, Batch 120, Loss: 3.1298\n",
      "Epoch 4/30, Batch 130, Loss: 3.4206\n",
      "Epoch 4/30, Batch 140, Loss: 3.1794\n",
      "Epoch 4/30, Batch 150, Loss: 3.2519\n",
      "Epoch 4/30, Batch 160, Loss: 3.3675\n",
      "Epoch 4/30, Batch 170, Loss: 3.2528\n",
      "Epoch 4/30, Batch 180, Loss: 3.2220\n",
      "Epoch 4/30, Batch 190, Loss: 3.4215\n",
      "Epoch 4/30, Batch 200, Loss: 3.4204\n",
      "Epoch 4/30, Batch 210, Loss: 3.1913\n",
      "Epoch 4/30, Batch 220, Loss: 3.2796\n",
      "Epoch 4/30, Batch 230, Loss: 3.2680\n",
      "Epoch 4/30, Batch 240, Loss: 3.2975\n",
      "Epoch 4/30, Batch 250, Loss: 3.3465\n",
      "Epoch 4/30, Batch 260, Loss: 3.3321\n",
      "Epoch 4/30, Batch 270, Loss: 3.3619\n",
      "Epoch 4/30, Batch 280, Loss: 3.3409\n",
      "Epoch 4/30, Batch 290, Loss: 3.2106\n",
      "Epoch 4/30, Batch 300, Loss: 3.2039\n",
      "Epoch 4/30, Batch 310, Loss: 3.4745\n",
      "Epoch 4/30, Batch 320, Loss: 3.2647\n",
      "Epoch 4/30, Batch 330, Loss: 3.3076\n",
      "Epoch 4/30, Batch 340, Loss: 3.2911\n",
      "Epoch 4/30, Batch 350, Loss: 3.1621\n",
      "Epoch 4/30, Batch 360, Loss: 3.3796\n",
      "Epoch 4/30, Batch 370, Loss: 3.2342\n",
      "Epoch 4/30, Batch 380, Loss: 3.2917\n",
      "Epoch 4/30, Batch 390, Loss: 3.1121\n",
      "Epoch 4/30, Batch 400, Loss: 3.2819\n",
      "Epoch 4/30, Batch 410, Loss: 3.2163\n",
      "Epoch 4/30, Batch 420, Loss: 3.3795\n",
      "Epoch 4/30, Batch 430, Loss: 3.3744\n",
      "Epoch 4/30, Batch 440, Loss: 3.3702\n",
      "Epoch 4/30, Batch 450, Loss: 3.2602\n",
      "Epoch 4/30, Batch 460, Loss: 3.3213\n",
      "Epoch 4/30, Batch 470, Loss: 3.2025\n",
      "Epoch 4/30, Batch 480, Loss: 3.1501\n",
      "Epoch 4/30, Batch 490, Loss: 3.5164\n",
      "Epoch 4/30, Batch 500, Loss: 3.2675\n",
      "Epoch 4/30, Batch 510, Loss: 3.1570\n",
      "Epoch 4/30, Batch 520, Loss: 3.3630\n",
      "Epoch 4/30, Batch 530, Loss: 3.2265\n",
      "Epoch 4/30, Batch 540, Loss: 3.2619\n",
      "Epoch 4/30, Batch 550, Loss: 3.3354\n",
      "Epoch 4/30, Batch 560, Loss: 3.1787\n",
      "Epoch 4/30, Batch 570, Loss: 3.2654\n",
      "Epoch 4/30, Batch 580, Loss: 3.2538\n",
      "Epoch 4/30, Batch 590, Loss: 3.2813\n",
      "Epoch 4/30, Batch 600, Loss: 3.3464\n",
      "Epoch 4/30, Batch 610, Loss: 3.2687\n",
      "Epoch 4/30, Batch 620, Loss: 3.2683\n",
      "Epoch 4/30, Batch 630, Loss: 3.1892\n",
      "Epoch 4/30, Batch 640, Loss: 3.3374\n",
      "Epoch 4/30, Batch 650, Loss: 3.1637\n",
      "Epoch 4/30, Batch 660, Loss: 3.2881\n",
      "Epoch 4/30, Batch 670, Loss: 3.1326\n",
      "Epoch 4/30, Batch 680, Loss: 3.2933\n",
      "Epoch 4/30, Batch 690, Loss: 3.3145\n",
      "Epoch 4/30, Batch 700, Loss: 3.1715\n",
      "Epoch 4/30, Batch 710, Loss: 3.3288\n",
      "Epoch 4/30, Batch 720, Loss: 3.2013\n",
      "Epoch 4/30, Batch 730, Loss: 3.2662\n",
      "Epoch 4/30, Batch 740, Loss: 3.4383\n",
      "Epoch 4/30, Batch 750, Loss: 3.3403\n",
      "Epoch 4/30, Batch 760, Loss: 3.4335\n",
      "Epoch 4/30, Batch 770, Loss: 3.1653\n",
      "Epoch 4/30, Batch 780, Loss: 3.1987\n",
      "Epoch 4/30, Batch 790, Loss: 3.2389\n",
      "Epoch 4/30, Batch 800, Loss: 3.3397\n",
      "Epoch 4/30, Batch 810, Loss: 3.2898\n",
      "Epoch 4/30, Batch 820, Loss: 3.2466\n",
      "Epoch 4/30, Batch 830, Loss: 3.2609\n",
      "Epoch 4/30, Batch 840, Loss: 3.2471\n",
      "Epoch 4/30, Batch 850, Loss: 3.1372\n",
      "Epoch 4/30, Batch 860, Loss: 3.2609\n",
      "Epoch 4/30, Batch 870, Loss: 3.3820\n",
      "Epoch 4/30, Batch 880, Loss: 3.2607\n",
      "Epoch 4/30, Batch 890, Loss: 3.2510\n",
      "Epoch 4/30, Batch 900, Loss: 3.2066\n",
      "Epoch 4/30, Batch 910, Loss: 3.1950\n",
      "Epoch 4/30, Batch 920, Loss: 3.2824\n",
      "Epoch 4/30, Batch 930, Loss: 3.3956\n",
      "Epoch 4/30, Batch 940, Loss: 3.2796\n",
      "Epoch 4/30, Batch 950, Loss: 3.2177\n",
      "Epoch 4/30, Batch 960, Loss: 3.3024\n",
      "Epoch 4/30, Batch 970, Loss: 3.2598\n",
      "Epoch 4/30, Batch 980, Loss: 3.2352\n",
      "Epoch 4/30, Batch 990, Loss: 3.1616\n",
      "Epoch 4/30, Batch 1000, Loss: 3.3054\n",
      "Epoch 4/30, Batch 1010, Loss: 3.2299\n",
      "Epoch 4/30, Batch 1020, Loss: 3.1523\n",
      "Epoch 4/30, Batch 1030, Loss: 3.4467\n",
      "Epoch 4/30, Batch 1040, Loss: 3.1682\n",
      "Epoch 4/30, Batch 1050, Loss: 3.1562\n",
      "Epoch 4/30, Batch 1060, Loss: 3.3082\n",
      "Epoch 4/30, Batch 1070, Loss: 3.1328\n",
      "Epoch 4/30, Batch 1080, Loss: 3.2924\n",
      "Epoch 4/30, Batch 1090, Loss: 3.2860\n",
      "Epoch 4/30, Batch 1100, Loss: 3.1517\n",
      "Epoch 4/30, Batch 1110, Loss: 3.2742\n",
      "Epoch 4/30, Batch 1120, Loss: 3.3329\n",
      "Epoch 4/30, Batch 1130, Loss: 3.3606\n",
      "Epoch 4/30, Batch 1140, Loss: 3.3237\n",
      "Epoch 4/30, Batch 1150, Loss: 3.1642\n",
      "Epoch 4/30, Batch 1160, Loss: 3.1816\n",
      "Epoch 4/30, Batch 1170, Loss: 3.1310\n",
      "Epoch 4/30, Batch 1180, Loss: 3.2305\n",
      "finish  save model of epoch : 3!\n",
      "epoch using time 1162.624\n",
      "Epoch 5/30, Batch 10, Loss: 3.1902\n",
      "Epoch 5/30, Batch 20, Loss: 3.2034\n",
      "Epoch 5/30, Batch 30, Loss: 3.2653\n",
      "Epoch 5/30, Batch 40, Loss: 3.1979\n",
      "Epoch 5/30, Batch 50, Loss: 3.2243\n",
      "Epoch 5/30, Batch 60, Loss: 3.1435\n",
      "Epoch 5/30, Batch 70, Loss: 3.1683\n",
      "Epoch 5/30, Batch 80, Loss: 3.2698\n",
      "Epoch 5/30, Batch 90, Loss: 3.1878\n",
      "Epoch 5/30, Batch 100, Loss: 3.1225\n",
      "Epoch 5/30, Batch 110, Loss: 3.1107\n",
      "Epoch 5/30, Batch 120, Loss: 3.2786\n",
      "Epoch 5/30, Batch 130, Loss: 3.2331\n",
      "Epoch 5/30, Batch 140, Loss: 3.1626\n",
      "Epoch 5/30, Batch 150, Loss: 3.2812\n",
      "Epoch 5/30, Batch 160, Loss: 3.0645\n",
      "Epoch 5/30, Batch 170, Loss: 3.2507\n",
      "Epoch 5/30, Batch 180, Loss: 3.3179\n",
      "Epoch 5/30, Batch 190, Loss: 3.2396\n",
      "Epoch 5/30, Batch 200, Loss: 3.1180\n",
      "Epoch 5/30, Batch 210, Loss: 3.1892\n",
      "Epoch 5/30, Batch 220, Loss: 3.1949\n",
      "Epoch 5/30, Batch 230, Loss: 3.0755\n",
      "Epoch 5/30, Batch 240, Loss: 3.1755\n",
      "Epoch 5/30, Batch 250, Loss: 3.0719\n",
      "Epoch 5/30, Batch 260, Loss: 3.1890\n",
      "Epoch 5/30, Batch 270, Loss: 3.0852\n",
      "Epoch 5/30, Batch 280, Loss: 3.2230\n",
      "Epoch 5/30, Batch 290, Loss: 3.2014\n",
      "Epoch 5/30, Batch 300, Loss: 3.3600\n",
      "Epoch 5/30, Batch 310, Loss: 3.1526\n",
      "Epoch 5/30, Batch 320, Loss: 3.1696\n",
      "Epoch 5/30, Batch 330, Loss: 3.1032\n",
      "Epoch 5/30, Batch 340, Loss: 3.1723\n",
      "Epoch 5/30, Batch 350, Loss: 3.2299\n",
      "Epoch 5/30, Batch 360, Loss: 3.0738\n",
      "Epoch 5/30, Batch 370, Loss: 3.1425\n",
      "Epoch 5/30, Batch 380, Loss: 3.0909\n",
      "Epoch 5/30, Batch 390, Loss: 3.1292\n",
      "Epoch 5/30, Batch 400, Loss: 3.2620\n",
      "Epoch 5/30, Batch 410, Loss: 3.2500\n",
      "Epoch 5/30, Batch 420, Loss: 3.2640\n",
      "Epoch 5/30, Batch 430, Loss: 3.0724\n",
      "Epoch 5/30, Batch 440, Loss: 3.1424\n",
      "Epoch 5/30, Batch 450, Loss: 3.1187\n",
      "Epoch 5/30, Batch 460, Loss: 3.1847\n",
      "Epoch 5/30, Batch 470, Loss: 3.1841\n",
      "Epoch 5/30, Batch 480, Loss: 3.0901\n",
      "Epoch 5/30, Batch 490, Loss: 3.1596\n",
      "Epoch 5/30, Batch 500, Loss: 3.3058\n",
      "Epoch 5/30, Batch 510, Loss: 3.1371\n",
      "Epoch 5/30, Batch 520, Loss: 3.2115\n",
      "Epoch 5/30, Batch 530, Loss: 3.0754\n",
      "Epoch 5/30, Batch 540, Loss: 3.2309\n",
      "Epoch 5/30, Batch 550, Loss: 3.2862\n",
      "Epoch 5/30, Batch 560, Loss: 3.1927\n",
      "Epoch 5/30, Batch 570, Loss: 3.1175\n",
      "Epoch 5/30, Batch 580, Loss: 3.2505\n",
      "Epoch 5/30, Batch 590, Loss: 3.2438\n",
      "Epoch 5/30, Batch 600, Loss: 3.2095\n",
      "Epoch 5/30, Batch 610, Loss: 3.2013\n",
      "Epoch 5/30, Batch 620, Loss: 3.1377\n",
      "Epoch 5/30, Batch 630, Loss: 3.3106\n",
      "Epoch 5/30, Batch 640, Loss: 3.2404\n",
      "Epoch 5/30, Batch 650, Loss: 3.3146\n",
      "Epoch 5/30, Batch 660, Loss: 3.2180\n",
      "Epoch 5/30, Batch 670, Loss: 3.3189\n",
      "Epoch 5/30, Batch 680, Loss: 3.4451\n",
      "Epoch 5/30, Batch 690, Loss: 3.0759\n",
      "Epoch 5/30, Batch 700, Loss: 3.1667\n",
      "Epoch 5/30, Batch 710, Loss: 3.2495\n",
      "Epoch 5/30, Batch 720, Loss: 3.1697\n",
      "Epoch 5/30, Batch 730, Loss: 3.2387\n",
      "Epoch 5/30, Batch 740, Loss: 3.2575\n",
      "Epoch 5/30, Batch 750, Loss: 2.9781\n",
      "Epoch 5/30, Batch 760, Loss: 3.2307\n",
      "Epoch 5/30, Batch 770, Loss: 3.2632\n",
      "Epoch 5/30, Batch 780, Loss: 3.1095\n",
      "Epoch 5/30, Batch 790, Loss: 3.2711\n",
      "Epoch 5/30, Batch 800, Loss: 3.2940\n",
      "Epoch 5/30, Batch 810, Loss: 3.1053\n",
      "Epoch 5/30, Batch 820, Loss: 3.2311\n",
      "Epoch 5/30, Batch 830, Loss: 3.1606\n",
      "Epoch 5/30, Batch 840, Loss: 3.2891\n",
      "Epoch 5/30, Batch 850, Loss: 3.1731\n",
      "Epoch 5/30, Batch 860, Loss: 3.1395\n",
      "Epoch 5/30, Batch 870, Loss: 3.1677\n",
      "Epoch 5/30, Batch 880, Loss: 3.0535\n",
      "Epoch 5/30, Batch 890, Loss: 3.0473\n",
      "Epoch 5/30, Batch 900, Loss: 3.1755\n",
      "Epoch 5/30, Batch 910, Loss: 3.2673\n",
      "Epoch 5/30, Batch 920, Loss: 3.2157\n",
      "Epoch 5/30, Batch 930, Loss: 3.1685\n",
      "Epoch 5/30, Batch 940, Loss: 3.2592\n",
      "Epoch 5/30, Batch 950, Loss: 3.2594\n",
      "Epoch 5/30, Batch 960, Loss: 3.1089\n",
      "Epoch 5/30, Batch 970, Loss: 3.2040\n",
      "Epoch 5/30, Batch 980, Loss: 3.3128\n",
      "Epoch 5/30, Batch 990, Loss: 3.0543\n",
      "Epoch 5/30, Batch 1000, Loss: 3.1374\n",
      "Epoch 5/30, Batch 1010, Loss: 3.1316\n",
      "Epoch 5/30, Batch 1020, Loss: 3.1898\n",
      "Epoch 5/30, Batch 1030, Loss: 3.3036\n",
      "Epoch 5/30, Batch 1040, Loss: 3.2979\n",
      "Epoch 5/30, Batch 1050, Loss: 3.2341\n",
      "Epoch 5/30, Batch 1060, Loss: 3.1993\n",
      "Epoch 5/30, Batch 1070, Loss: 3.1337\n",
      "Epoch 5/30, Batch 1080, Loss: 3.1213\n",
      "Epoch 5/30, Batch 1090, Loss: 3.1313\n",
      "Epoch 5/30, Batch 1100, Loss: 3.2786\n",
      "Epoch 5/30, Batch 1110, Loss: 3.0284\n",
      "Epoch 5/30, Batch 1120, Loss: 3.0924\n",
      "Epoch 5/30, Batch 1130, Loss: 2.9626\n",
      "Epoch 5/30, Batch 1140, Loss: 3.2409\n",
      "Epoch 5/30, Batch 1150, Loss: 3.1439\n",
      "Epoch 5/30, Batch 1160, Loss: 3.1221\n",
      "Epoch 5/30, Batch 1170, Loss: 3.0408\n",
      "Epoch 5/30, Batch 1180, Loss: 3.2848\n",
      "finish  save model of epoch : 4!\n",
      "epoch using time 1185.883\n",
      "Epoch 6/30, Batch 10, Loss: 3.0536\n",
      "Epoch 6/30, Batch 20, Loss: 3.0338\n",
      "Epoch 6/30, Batch 30, Loss: 3.2006\n",
      "Epoch 6/30, Batch 40, Loss: 3.0716\n",
      "Epoch 6/30, Batch 50, Loss: 3.1077\n",
      "Epoch 6/30, Batch 60, Loss: 3.1488\n",
      "Epoch 6/30, Batch 70, Loss: 3.1719\n",
      "Epoch 6/30, Batch 80, Loss: 3.0442\n",
      "Epoch 6/30, Batch 90, Loss: 3.0995\n",
      "Epoch 6/30, Batch 100, Loss: 3.1531\n",
      "Epoch 6/30, Batch 110, Loss: 3.2338\n",
      "Epoch 6/30, Batch 120, Loss: 3.1463\n",
      "Epoch 6/30, Batch 130, Loss: 3.2756\n",
      "Epoch 6/30, Batch 140, Loss: 3.1310\n",
      "Epoch 6/30, Batch 150, Loss: 3.1585\n",
      "Epoch 6/30, Batch 160, Loss: 3.1050\n",
      "Epoch 6/30, Batch 170, Loss: 3.0010\n",
      "Epoch 6/30, Batch 180, Loss: 3.1840\n",
      "Epoch 6/30, Batch 190, Loss: 3.2650\n",
      "Epoch 6/30, Batch 200, Loss: 3.0852\n",
      "Epoch 6/30, Batch 210, Loss: 3.2277\n",
      "Epoch 6/30, Batch 220, Loss: 3.2481\n",
      "Epoch 6/30, Batch 230, Loss: 3.2544\n",
      "Epoch 6/30, Batch 240, Loss: 3.1668\n",
      "Epoch 6/30, Batch 250, Loss: 3.1827\n",
      "Epoch 6/30, Batch 260, Loss: 3.1193\n",
      "Epoch 6/30, Batch 270, Loss: 2.9880\n",
      "Epoch 6/30, Batch 280, Loss: 3.1295\n",
      "Epoch 6/30, Batch 290, Loss: 3.2193\n",
      "Epoch 6/30, Batch 300, Loss: 3.1353\n",
      "Epoch 6/30, Batch 310, Loss: 3.3351\n",
      "Epoch 6/30, Batch 320, Loss: 3.3070\n",
      "Epoch 6/30, Batch 330, Loss: 3.2788\n",
      "Epoch 6/30, Batch 340, Loss: 3.1870\n",
      "Epoch 6/30, Batch 350, Loss: 3.1846\n",
      "Epoch 6/30, Batch 360, Loss: 3.1316\n",
      "Epoch 6/30, Batch 370, Loss: 3.3366\n",
      "Epoch 6/30, Batch 380, Loss: 3.2387\n",
      "Epoch 6/30, Batch 390, Loss: 3.0386\n",
      "Epoch 6/30, Batch 400, Loss: 3.1463\n",
      "Epoch 6/30, Batch 410, Loss: 3.0417\n",
      "Epoch 6/30, Batch 420, Loss: 3.2335\n",
      "Epoch 6/30, Batch 430, Loss: 3.1611\n",
      "Epoch 6/30, Batch 440, Loss: 3.0816\n",
      "Epoch 6/30, Batch 450, Loss: 3.0335\n",
      "Epoch 6/30, Batch 460, Loss: 3.0950\n",
      "Epoch 6/30, Batch 470, Loss: 3.1620\n",
      "Epoch 6/30, Batch 480, Loss: 3.1111\n",
      "Epoch 6/30, Batch 490, Loss: 3.1111\n",
      "Epoch 6/30, Batch 500, Loss: 3.1214\n",
      "Epoch 6/30, Batch 510, Loss: 3.1392\n",
      "Epoch 6/30, Batch 520, Loss: 3.1291\n",
      "Epoch 6/30, Batch 530, Loss: 3.1566\n",
      "Epoch 6/30, Batch 540, Loss: 3.2368\n",
      "Epoch 6/30, Batch 550, Loss: 3.0204\n",
      "Epoch 6/30, Batch 560, Loss: 3.1948\n",
      "Epoch 6/30, Batch 570, Loss: 3.1644\n",
      "Epoch 6/30, Batch 580, Loss: 3.0206\n",
      "Epoch 6/30, Batch 590, Loss: 3.1796\n",
      "Epoch 6/30, Batch 600, Loss: 3.2147\n",
      "Epoch 6/30, Batch 610, Loss: 3.1221\n",
      "Epoch 6/30, Batch 620, Loss: 3.1280\n",
      "Epoch 6/30, Batch 630, Loss: 3.1199\n",
      "Epoch 6/30, Batch 640, Loss: 3.3436\n",
      "Epoch 6/30, Batch 650, Loss: 3.0453\n",
      "Epoch 6/30, Batch 660, Loss: 3.2185\n",
      "Epoch 6/30, Batch 670, Loss: 3.0830\n",
      "Epoch 6/30, Batch 680, Loss: 3.0995\n",
      "Epoch 6/30, Batch 690, Loss: 3.1201\n",
      "Epoch 6/30, Batch 700, Loss: 3.0876\n",
      "Epoch 6/30, Batch 710, Loss: 3.1056\n",
      "Epoch 6/30, Batch 720, Loss: 3.1266\n",
      "Epoch 6/30, Batch 730, Loss: 2.9241\n",
      "Epoch 6/30, Batch 740, Loss: 3.1253\n",
      "Epoch 6/30, Batch 750, Loss: 3.2916\n",
      "Epoch 6/30, Batch 760, Loss: 3.1461\n",
      "Epoch 6/30, Batch 770, Loss: 3.0972\n",
      "Epoch 6/30, Batch 780, Loss: 3.1388\n",
      "Epoch 6/30, Batch 790, Loss: 3.1687\n",
      "Epoch 6/30, Batch 800, Loss: 3.1185\n",
      "Epoch 6/30, Batch 810, Loss: 3.0489\n",
      "Epoch 6/30, Batch 820, Loss: 3.1679\n",
      "Epoch 6/30, Batch 830, Loss: 2.9321\n",
      "Epoch 6/30, Batch 840, Loss: 3.1842\n",
      "Epoch 6/30, Batch 850, Loss: 3.0799\n",
      "Epoch 6/30, Batch 860, Loss: 3.0241\n",
      "Epoch 6/30, Batch 870, Loss: 3.1400\n",
      "Epoch 6/30, Batch 880, Loss: 3.1006\n",
      "Epoch 6/30, Batch 890, Loss: 3.1857\n",
      "Epoch 6/30, Batch 900, Loss: 3.0992\n",
      "Epoch 6/30, Batch 910, Loss: 3.1203\n",
      "Epoch 6/30, Batch 920, Loss: 3.0362\n",
      "Epoch 6/30, Batch 930, Loss: 3.0965\n",
      "Epoch 6/30, Batch 940, Loss: 3.1844\n",
      "Epoch 6/30, Batch 950, Loss: 3.0852\n",
      "Epoch 6/30, Batch 960, Loss: 3.0647\n",
      "Epoch 6/30, Batch 970, Loss: 3.2395\n",
      "Epoch 6/30, Batch 980, Loss: 3.1562\n",
      "Epoch 6/30, Batch 990, Loss: 3.0617\n",
      "Epoch 6/30, Batch 1000, Loss: 3.1545\n",
      "Epoch 6/30, Batch 1010, Loss: 3.1203\n",
      "Epoch 6/30, Batch 1020, Loss: 3.2189\n",
      "Epoch 6/30, Batch 1030, Loss: 3.0526\n",
      "Epoch 6/30, Batch 1040, Loss: 3.0839\n",
      "Epoch 6/30, Batch 1050, Loss: 3.2515\n",
      "Epoch 6/30, Batch 1060, Loss: 3.1432\n",
      "Epoch 6/30, Batch 1070, Loss: 3.2439\n",
      "Epoch 6/30, Batch 1080, Loss: 3.1714\n",
      "Epoch 6/30, Batch 1090, Loss: 3.0730\n",
      "Epoch 6/30, Batch 1100, Loss: 3.0832\n",
      "Epoch 6/30, Batch 1110, Loss: 3.2415\n",
      "Epoch 6/30, Batch 1120, Loss: 3.2166\n",
      "Epoch 6/30, Batch 1130, Loss: 3.1064\n",
      "Epoch 6/30, Batch 1140, Loss: 3.1975\n",
      "Epoch 6/30, Batch 1150, Loss: 3.0863\n",
      "Epoch 6/30, Batch 1160, Loss: 3.0684\n",
      "Epoch 6/30, Batch 1170, Loss: 3.0033\n",
      "Epoch 6/30, Batch 1180, Loss: 3.0180\n",
      "finish  save model of epoch : 5!\n",
      "epoch using time 1084.983\n",
      "Epoch 7/30, Batch 10, Loss: 3.0349\n",
      "Epoch 7/30, Batch 20, Loss: 3.0859\n",
      "Epoch 7/30, Batch 30, Loss: 3.1343\n",
      "Epoch 7/30, Batch 40, Loss: 2.8801\n",
      "Epoch 7/30, Batch 50, Loss: 3.0551\n",
      "Epoch 7/30, Batch 60, Loss: 3.1515\n",
      "Epoch 7/30, Batch 70, Loss: 3.1892\n",
      "Epoch 7/30, Batch 80, Loss: 3.0356\n",
      "Epoch 7/30, Batch 90, Loss: 3.1044\n",
      "Epoch 7/30, Batch 100, Loss: 2.9835\n",
      "Epoch 7/30, Batch 110, Loss: 3.0052\n",
      "Epoch 7/30, Batch 120, Loss: 3.0533\n",
      "Epoch 7/30, Batch 130, Loss: 3.1012\n",
      "Epoch 7/30, Batch 140, Loss: 3.1028\n",
      "Epoch 7/30, Batch 150, Loss: 3.0836\n",
      "Epoch 7/30, Batch 160, Loss: 3.0215\n",
      "Epoch 7/30, Batch 170, Loss: 3.1085\n",
      "Epoch 7/30, Batch 180, Loss: 3.1786\n",
      "Epoch 7/30, Batch 190, Loss: 3.0284\n",
      "Epoch 7/30, Batch 200, Loss: 3.0099\n",
      "Epoch 7/30, Batch 210, Loss: 3.0514\n",
      "Epoch 7/30, Batch 220, Loss: 3.0352\n",
      "Epoch 7/30, Batch 230, Loss: 3.0094\n",
      "Epoch 7/30, Batch 240, Loss: 3.1535\n",
      "Epoch 7/30, Batch 250, Loss: 3.0941\n",
      "Epoch 7/30, Batch 260, Loss: 3.2158\n",
      "Epoch 7/30, Batch 270, Loss: 3.0066\n",
      "Epoch 7/30, Batch 280, Loss: 3.0312\n",
      "Epoch 7/30, Batch 290, Loss: 3.0681\n",
      "Epoch 7/30, Batch 300, Loss: 3.1458\n",
      "Epoch 7/30, Batch 310, Loss: 3.0757\n",
      "Epoch 7/30, Batch 320, Loss: 2.9159\n",
      "Epoch 7/30, Batch 330, Loss: 3.2030\n",
      "Epoch 7/30, Batch 340, Loss: 3.1632\n",
      "Epoch 7/30, Batch 350, Loss: 3.2155\n",
      "Epoch 7/30, Batch 360, Loss: 3.1290\n",
      "Epoch 7/30, Batch 370, Loss: 3.0846\n",
      "Epoch 7/30, Batch 380, Loss: 3.1747\n",
      "Epoch 7/30, Batch 390, Loss: 3.2273\n",
      "Epoch 7/30, Batch 400, Loss: 3.0110\n",
      "Epoch 7/30, Batch 410, Loss: 3.0723\n",
      "Epoch 7/30, Batch 420, Loss: 3.1158\n",
      "Epoch 7/30, Batch 430, Loss: 3.2651\n",
      "Epoch 7/30, Batch 440, Loss: 3.1366\n",
      "Epoch 7/30, Batch 450, Loss: 3.1410\n",
      "Epoch 7/30, Batch 460, Loss: 3.1153\n",
      "Epoch 7/30, Batch 470, Loss: 3.0395\n",
      "Epoch 7/30, Batch 480, Loss: 3.0326\n",
      "Epoch 7/30, Batch 490, Loss: 2.9915\n",
      "Epoch 7/30, Batch 500, Loss: 3.0119\n",
      "Epoch 7/30, Batch 510, Loss: 3.1965\n",
      "Epoch 7/30, Batch 520, Loss: 3.0844\n",
      "Epoch 7/30, Batch 530, Loss: 3.0461\n",
      "Epoch 7/30, Batch 540, Loss: 3.0262\n",
      "Epoch 7/30, Batch 550, Loss: 3.1120\n",
      "Epoch 7/30, Batch 560, Loss: 3.0976\n",
      "Epoch 7/30, Batch 570, Loss: 2.9932\n",
      "Epoch 7/30, Batch 580, Loss: 3.0514\n",
      "Epoch 7/30, Batch 590, Loss: 3.1240\n",
      "Epoch 7/30, Batch 600, Loss: 3.1577\n",
      "Epoch 7/30, Batch 610, Loss: 3.1700\n",
      "Epoch 7/30, Batch 620, Loss: 3.1901\n",
      "Epoch 7/30, Batch 630, Loss: 3.0080\n",
      "Epoch 7/30, Batch 640, Loss: 3.2928\n",
      "Epoch 7/30, Batch 650, Loss: 3.0770\n",
      "Epoch 7/30, Batch 660, Loss: 3.1440\n",
      "Epoch 7/30, Batch 670, Loss: 3.0264\n",
      "Epoch 7/30, Batch 680, Loss: 3.0396\n",
      "Epoch 7/30, Batch 690, Loss: 3.0226\n",
      "Epoch 7/30, Batch 700, Loss: 2.9890\n",
      "Epoch 7/30, Batch 710, Loss: 3.1213\n",
      "Epoch 7/30, Batch 720, Loss: 3.2002\n",
      "Epoch 7/30, Batch 730, Loss: 3.1289\n",
      "Epoch 7/30, Batch 740, Loss: 3.0976\n",
      "Epoch 7/30, Batch 750, Loss: 3.0817\n",
      "Epoch 7/30, Batch 760, Loss: 3.0943\n",
      "Epoch 7/30, Batch 770, Loss: 3.1034\n",
      "Epoch 7/30, Batch 780, Loss: 3.1268\n",
      "Epoch 7/30, Batch 790, Loss: 3.1152\n",
      "Epoch 7/30, Batch 800, Loss: 3.0355\n",
      "Epoch 7/30, Batch 810, Loss: 3.0575\n",
      "Epoch 7/30, Batch 820, Loss: 3.1637\n",
      "Epoch 7/30, Batch 830, Loss: 3.1477\n",
      "Epoch 7/30, Batch 840, Loss: 3.1443\n",
      "Epoch 7/30, Batch 850, Loss: 3.0970\n",
      "Epoch 7/30, Batch 860, Loss: 3.1764\n",
      "Epoch 7/30, Batch 870, Loss: 3.0804\n",
      "Epoch 7/30, Batch 880, Loss: 3.1895\n",
      "Epoch 7/30, Batch 890, Loss: 3.1258\n",
      "Epoch 7/30, Batch 900, Loss: 3.0079\n",
      "Epoch 7/30, Batch 910, Loss: 3.0608\n",
      "Epoch 7/30, Batch 920, Loss: 3.1364\n",
      "Epoch 7/30, Batch 930, Loss: 2.9285\n",
      "Epoch 7/30, Batch 940, Loss: 3.0453\n",
      "Epoch 7/30, Batch 950, Loss: 2.9855\n",
      "Epoch 7/30, Batch 960, Loss: 3.1721\n",
      "Epoch 7/30, Batch 970, Loss: 2.9573\n",
      "Epoch 7/30, Batch 980, Loss: 3.1462\n",
      "Epoch 7/30, Batch 990, Loss: 3.0641\n",
      "Epoch 7/30, Batch 1000, Loss: 3.0820\n",
      "Epoch 7/30, Batch 1010, Loss: 3.0860\n",
      "Epoch 7/30, Batch 1020, Loss: 3.1323\n",
      "Epoch 7/30, Batch 1030, Loss: 3.0690\n",
      "Epoch 7/30, Batch 1040, Loss: 2.9657\n",
      "Epoch 7/30, Batch 1050, Loss: 3.1717\n",
      "Epoch 7/30, Batch 1060, Loss: 3.1050\n",
      "Epoch 7/30, Batch 1070, Loss: 2.9946\n",
      "Epoch 7/30, Batch 1080, Loss: 3.0683\n",
      "Epoch 7/30, Batch 1090, Loss: 3.0948\n",
      "Epoch 7/30, Batch 1100, Loss: 3.0781\n",
      "Epoch 7/30, Batch 1110, Loss: 3.1568\n",
      "Epoch 7/30, Batch 1120, Loss: 3.1676\n",
      "Epoch 7/30, Batch 1130, Loss: 3.0732\n",
      "Epoch 7/30, Batch 1140, Loss: 2.9764\n",
      "Epoch 7/30, Batch 1150, Loss: 3.2566\n",
      "Epoch 7/30, Batch 1160, Loss: 2.9671\n",
      "Epoch 7/30, Batch 1170, Loss: 3.0724\n",
      "Epoch 7/30, Batch 1180, Loss: 2.9980\n",
      "finish  save model of epoch : 6!\n",
      "epoch using time 1017.057\n",
      "Epoch 8/30, Batch 10, Loss: 3.0259\n",
      "Epoch 8/30, Batch 20, Loss: 3.0515\n",
      "Epoch 8/30, Batch 30, Loss: 3.0808\n",
      "Epoch 8/30, Batch 40, Loss: 3.0756\n",
      "Epoch 8/30, Batch 50, Loss: 2.9191\n",
      "Epoch 8/30, Batch 60, Loss: 3.0893\n",
      "Epoch 8/30, Batch 70, Loss: 3.0453\n",
      "Epoch 8/30, Batch 80, Loss: 3.0680\n",
      "Epoch 8/30, Batch 90, Loss: 2.9476\n",
      "Epoch 8/30, Batch 100, Loss: 3.0015\n",
      "Epoch 8/30, Batch 110, Loss: 2.9676\n",
      "Epoch 8/30, Batch 120, Loss: 3.0258\n",
      "Epoch 8/30, Batch 130, Loss: 2.9822\n",
      "Epoch 8/30, Batch 140, Loss: 3.1184\n",
      "Epoch 8/30, Batch 150, Loss: 3.1359\n",
      "Epoch 8/30, Batch 160, Loss: 3.0346\n",
      "Epoch 8/30, Batch 170, Loss: 2.9793\n",
      "Epoch 8/30, Batch 180, Loss: 2.9381\n",
      "Epoch 8/30, Batch 190, Loss: 3.0722\n",
      "Epoch 8/30, Batch 200, Loss: 2.9966\n",
      "Epoch 8/30, Batch 210, Loss: 3.1418\n",
      "Epoch 8/30, Batch 220, Loss: 2.9135\n",
      "Epoch 8/30, Batch 230, Loss: 3.0246\n",
      "Epoch 8/30, Batch 240, Loss: 3.1099\n",
      "Epoch 8/30, Batch 250, Loss: 3.1130\n",
      "Epoch 8/30, Batch 260, Loss: 2.9514\n",
      "Epoch 8/30, Batch 270, Loss: 3.0823\n",
      "Epoch 8/30, Batch 280, Loss: 2.9821\n",
      "Epoch 8/30, Batch 290, Loss: 2.8996\n",
      "Epoch 8/30, Batch 300, Loss: 2.8423\n",
      "Epoch 8/30, Batch 310, Loss: 2.9983\n",
      "Epoch 8/30, Batch 320, Loss: 3.1255\n",
      "Epoch 8/30, Batch 330, Loss: 2.9438\n",
      "Epoch 8/30, Batch 340, Loss: 3.0698\n",
      "Epoch 8/30, Batch 350, Loss: 3.1026\n",
      "Epoch 8/30, Batch 360, Loss: 3.0777\n",
      "Epoch 8/30, Batch 370, Loss: 3.1592\n",
      "Epoch 8/30, Batch 380, Loss: 3.0447\n",
      "Epoch 8/30, Batch 390, Loss: 2.9249\n",
      "Epoch 8/30, Batch 400, Loss: 2.9018\n",
      "Epoch 8/30, Batch 410, Loss: 3.1341\n",
      "Epoch 8/30, Batch 420, Loss: 2.9658\n",
      "Epoch 8/30, Batch 430, Loss: 3.0425\n",
      "Epoch 8/30, Batch 440, Loss: 3.1112\n",
      "Epoch 8/30, Batch 450, Loss: 3.0036\n",
      "Epoch 8/30, Batch 460, Loss: 3.0969\n",
      "Epoch 8/30, Batch 470, Loss: 2.9397\n",
      "Epoch 8/30, Batch 480, Loss: 3.0628\n",
      "Epoch 8/30, Batch 490, Loss: 2.8705\n",
      "Epoch 8/30, Batch 500, Loss: 3.0545\n",
      "Epoch 8/30, Batch 510, Loss: 3.1531\n",
      "Epoch 8/30, Batch 520, Loss: 3.0044\n",
      "Epoch 8/30, Batch 530, Loss: 3.1664\n",
      "Epoch 8/30, Batch 540, Loss: 3.0619\n",
      "Epoch 8/30, Batch 550, Loss: 3.1430\n",
      "Epoch 8/30, Batch 560, Loss: 3.1030\n",
      "Epoch 8/30, Batch 570, Loss: 2.9610\n",
      "Epoch 8/30, Batch 580, Loss: 3.0827\n",
      "Epoch 8/30, Batch 590, Loss: 3.1359\n",
      "Epoch 8/30, Batch 600, Loss: 3.0222\n",
      "Epoch 8/30, Batch 610, Loss: 3.1234\n",
      "Epoch 8/30, Batch 620, Loss: 2.9047\n",
      "Epoch 8/30, Batch 630, Loss: 3.0444\n",
      "Epoch 8/30, Batch 640, Loss: 3.2274\n",
      "Epoch 8/30, Batch 650, Loss: 2.9999\n",
      "Epoch 8/30, Batch 660, Loss: 2.9275\n",
      "Epoch 8/30, Batch 670, Loss: 2.9994\n",
      "Epoch 8/30, Batch 680, Loss: 3.1795\n",
      "Epoch 8/30, Batch 690, Loss: 2.9739\n",
      "Epoch 8/30, Batch 700, Loss: 3.0141\n",
      "Epoch 8/30, Batch 710, Loss: 3.0909\n",
      "Epoch 8/30, Batch 720, Loss: 2.9989\n",
      "Epoch 8/30, Batch 730, Loss: 3.0918\n",
      "Epoch 8/30, Batch 740, Loss: 3.0055\n",
      "Epoch 8/30, Batch 750, Loss: 3.1143\n",
      "Epoch 8/30, Batch 760, Loss: 3.0524\n",
      "Epoch 8/30, Batch 770, Loss: 3.1211\n",
      "Epoch 8/30, Batch 780, Loss: 2.9801\n",
      "Epoch 8/30, Batch 790, Loss: 3.1140\n",
      "Epoch 8/30, Batch 800, Loss: 2.9727\n",
      "Epoch 8/30, Batch 810, Loss: 2.9896\n",
      "Epoch 8/30, Batch 820, Loss: 3.0991\n",
      "Epoch 8/30, Batch 830, Loss: 3.0682\n",
      "Epoch 8/30, Batch 840, Loss: 3.1420\n",
      "Epoch 8/30, Batch 850, Loss: 3.1042\n",
      "Epoch 8/30, Batch 860, Loss: 3.1961\n",
      "Epoch 8/30, Batch 870, Loss: 3.0528\n",
      "Epoch 8/30, Batch 880, Loss: 3.0213\n",
      "Epoch 8/30, Batch 890, Loss: 2.9875\n",
      "Epoch 8/30, Batch 900, Loss: 3.0793\n",
      "Epoch 8/30, Batch 910, Loss: 3.0113\n",
      "Epoch 8/30, Batch 920, Loss: 3.1694\n",
      "Epoch 8/30, Batch 930, Loss: 3.0021\n",
      "Epoch 8/30, Batch 940, Loss: 3.0309\n",
      "Epoch 8/30, Batch 950, Loss: 3.1092\n",
      "Epoch 8/30, Batch 960, Loss: 3.0811\n",
      "Epoch 8/30, Batch 970, Loss: 3.0654\n",
      "Epoch 8/30, Batch 980, Loss: 3.1799\n",
      "Epoch 8/30, Batch 990, Loss: 3.0364\n",
      "Epoch 8/30, Batch 1000, Loss: 3.0394\n",
      "Epoch 8/30, Batch 1010, Loss: 3.1340\n",
      "Epoch 8/30, Batch 1020, Loss: 3.0533\n",
      "Epoch 8/30, Batch 1030, Loss: 3.0803\n",
      "Epoch 8/30, Batch 1040, Loss: 3.0355\n",
      "Epoch 8/30, Batch 1050, Loss: 3.1399\n",
      "Epoch 8/30, Batch 1060, Loss: 3.0709\n",
      "Epoch 8/30, Batch 1070, Loss: 3.1220\n",
      "Epoch 8/30, Batch 1080, Loss: 3.0568\n",
      "Epoch 8/30, Batch 1090, Loss: 2.9946\n",
      "Epoch 8/30, Batch 1100, Loss: 3.1600\n",
      "Epoch 8/30, Batch 1110, Loss: 3.1036\n",
      "Epoch 8/30, Batch 1120, Loss: 3.1208\n",
      "Epoch 8/30, Batch 1130, Loss: 3.0188\n",
      "Epoch 8/30, Batch 1140, Loss: 3.0184\n",
      "Epoch 8/30, Batch 1150, Loss: 3.1889\n",
      "Epoch 8/30, Batch 1160, Loss: 3.0025\n",
      "Epoch 8/30, Batch 1170, Loss: 3.0793\n",
      "Epoch 8/30, Batch 1180, Loss: 3.0950\n",
      "finish  save model of epoch : 7!\n",
      "epoch using time 1021.288\n",
      "Epoch 9/30, Batch 10, Loss: 2.9925\n",
      "Epoch 9/30, Batch 20, Loss: 2.9752\n",
      "Epoch 9/30, Batch 30, Loss: 3.0048\n",
      "Epoch 9/30, Batch 40, Loss: 2.9448\n",
      "Epoch 9/30, Batch 50, Loss: 2.9654\n",
      "Epoch 9/30, Batch 60, Loss: 3.0419\n",
      "Epoch 9/30, Batch 70, Loss: 2.9533\n",
      "Epoch 9/30, Batch 80, Loss: 3.0231\n",
      "Epoch 9/30, Batch 90, Loss: 3.0320\n",
      "Epoch 9/30, Batch 100, Loss: 2.8607\n",
      "Epoch 9/30, Batch 110, Loss: 2.9674\n",
      "Epoch 9/30, Batch 120, Loss: 3.0340\n",
      "Epoch 9/30, Batch 130, Loss: 3.1133\n",
      "Epoch 9/30, Batch 140, Loss: 3.0414\n",
      "Epoch 9/30, Batch 150, Loss: 3.0984\n",
      "Epoch 9/30, Batch 160, Loss: 3.0206\n",
      "Epoch 9/30, Batch 170, Loss: 3.0939\n",
      "Epoch 9/30, Batch 180, Loss: 3.0386\n",
      "Epoch 9/30, Batch 190, Loss: 3.1598\n",
      "Epoch 9/30, Batch 200, Loss: 2.9669\n",
      "Epoch 9/30, Batch 210, Loss: 3.1104\n",
      "Epoch 9/30, Batch 220, Loss: 3.1036\n",
      "Epoch 9/30, Batch 230, Loss: 3.1582\n",
      "Epoch 9/30, Batch 240, Loss: 2.9930\n",
      "Epoch 9/30, Batch 250, Loss: 2.9963\n",
      "Epoch 9/30, Batch 260, Loss: 2.9991\n",
      "Epoch 9/30, Batch 270, Loss: 2.9812\n",
      "Epoch 9/30, Batch 280, Loss: 2.9576\n",
      "Epoch 9/30, Batch 290, Loss: 2.9581\n",
      "Epoch 9/30, Batch 300, Loss: 2.9556\n",
      "Epoch 9/30, Batch 310, Loss: 2.9611\n",
      "Epoch 9/30, Batch 320, Loss: 3.0886\n",
      "Epoch 9/30, Batch 330, Loss: 2.9874\n",
      "Epoch 9/30, Batch 340, Loss: 3.0897\n",
      "Epoch 9/30, Batch 350, Loss: 3.1188\n",
      "Epoch 9/30, Batch 360, Loss: 2.9241\n",
      "Epoch 9/30, Batch 370, Loss: 3.0666\n",
      "Epoch 9/30, Batch 380, Loss: 3.0216\n",
      "Epoch 9/30, Batch 390, Loss: 3.0942\n",
      "Epoch 9/30, Batch 400, Loss: 3.1487\n",
      "Epoch 9/30, Batch 410, Loss: 2.9311\n",
      "Epoch 9/30, Batch 420, Loss: 2.9903\n",
      "Epoch 9/30, Batch 430, Loss: 2.9968\n",
      "Epoch 9/30, Batch 440, Loss: 3.0106\n",
      "Epoch 9/30, Batch 450, Loss: 3.1371\n",
      "Epoch 9/30, Batch 460, Loss: 3.0109\n",
      "Epoch 9/30, Batch 470, Loss: 3.0007\n",
      "Epoch 9/30, Batch 480, Loss: 2.9676\n",
      "Epoch 9/30, Batch 490, Loss: 2.9064\n",
      "Epoch 9/30, Batch 500, Loss: 3.2128\n",
      "Epoch 9/30, Batch 510, Loss: 3.0547\n",
      "Epoch 9/30, Batch 520, Loss: 3.0477\n",
      "Epoch 9/30, Batch 530, Loss: 2.9741\n",
      "Epoch 9/30, Batch 540, Loss: 3.0723\n",
      "Epoch 9/30, Batch 550, Loss: 2.8565\n",
      "Epoch 9/30, Batch 560, Loss: 3.1134\n",
      "Epoch 9/30, Batch 570, Loss: 3.0728\n",
      "Epoch 9/30, Batch 580, Loss: 2.9292\n",
      "Epoch 9/30, Batch 590, Loss: 2.9179\n",
      "Epoch 9/30, Batch 600, Loss: 3.0233\n",
      "Epoch 9/30, Batch 610, Loss: 3.0763\n",
      "Epoch 9/30, Batch 620, Loss: 3.0363\n",
      "Epoch 9/30, Batch 630, Loss: 2.9161\n",
      "Epoch 9/30, Batch 640, Loss: 2.9779\n",
      "Epoch 9/30, Batch 650, Loss: 2.9457\n",
      "Epoch 9/30, Batch 660, Loss: 2.9338\n",
      "Epoch 9/30, Batch 670, Loss: 2.9989\n",
      "Epoch 9/30, Batch 680, Loss: 3.0024\n",
      "Epoch 9/30, Batch 690, Loss: 3.1232\n",
      "Epoch 9/30, Batch 700, Loss: 3.0954\n",
      "Epoch 9/30, Batch 710, Loss: 2.9999\n",
      "Epoch 9/30, Batch 720, Loss: 3.0000\n",
      "Epoch 9/30, Batch 730, Loss: 2.9645\n",
      "Epoch 9/30, Batch 740, Loss: 2.9024\n",
      "Epoch 9/30, Batch 750, Loss: 3.0683\n",
      "Epoch 9/30, Batch 760, Loss: 2.9611\n",
      "Epoch 9/30, Batch 770, Loss: 2.9946\n",
      "Epoch 9/30, Batch 780, Loss: 3.0074\n",
      "Epoch 9/30, Batch 790, Loss: 3.1276\n",
      "Epoch 9/30, Batch 800, Loss: 3.0272\n",
      "Epoch 9/30, Batch 810, Loss: 3.0731\n",
      "Epoch 9/30, Batch 820, Loss: 3.0095\n",
      "Epoch 9/30, Batch 830, Loss: 3.0411\n",
      "Epoch 9/30, Batch 840, Loss: 2.9949\n",
      "Epoch 9/30, Batch 850, Loss: 3.2553\n",
      "Epoch 9/30, Batch 860, Loss: 2.9926\n",
      "Epoch 9/30, Batch 870, Loss: 3.0604\n",
      "Epoch 9/30, Batch 880, Loss: 2.9979\n",
      "Epoch 9/30, Batch 890, Loss: 2.9779\n",
      "Epoch 9/30, Batch 900, Loss: 3.0509\n",
      "Epoch 9/30, Batch 910, Loss: 2.9848\n",
      "Epoch 9/30, Batch 920, Loss: 3.0370\n",
      "Epoch 9/30, Batch 930, Loss: 3.1771\n",
      "Epoch 9/30, Batch 940, Loss: 3.0629\n",
      "Epoch 9/30, Batch 950, Loss: 3.1313\n",
      "Epoch 9/30, Batch 960, Loss: 2.9947\n",
      "Epoch 9/30, Batch 970, Loss: 3.1124\n",
      "Epoch 9/30, Batch 980, Loss: 3.0857\n",
      "Epoch 9/30, Batch 990, Loss: 3.0437\n",
      "Epoch 9/30, Batch 1000, Loss: 3.0294\n",
      "Epoch 9/30, Batch 1010, Loss: 3.0303\n",
      "Epoch 9/30, Batch 1020, Loss: 3.2324\n",
      "Epoch 9/30, Batch 1030, Loss: 3.1584\n",
      "Epoch 9/30, Batch 1040, Loss: 2.9781\n",
      "Epoch 9/30, Batch 1050, Loss: 2.9915\n",
      "Epoch 9/30, Batch 1060, Loss: 2.9930\n",
      "Epoch 9/30, Batch 1070, Loss: 3.1117\n",
      "Epoch 9/30, Batch 1080, Loss: 3.1730\n",
      "Epoch 9/30, Batch 1090, Loss: 3.0326\n",
      "Epoch 9/30, Batch 1100, Loss: 3.0053\n",
      "Epoch 9/30, Batch 1110, Loss: 2.9435\n",
      "Epoch 9/30, Batch 1120, Loss: 2.8795\n",
      "Epoch 9/30, Batch 1130, Loss: 3.1675\n",
      "Epoch 9/30, Batch 1140, Loss: 3.0547\n",
      "Epoch 9/30, Batch 1150, Loss: 2.9797\n",
      "Epoch 9/30, Batch 1160, Loss: 3.1244\n",
      "Epoch 9/30, Batch 1170, Loss: 3.1357\n",
      "Epoch 9/30, Batch 1180, Loss: 3.0136\n",
      "finish  save model of epoch : 8!\n",
      "epoch using time 1032.421\n",
      "Epoch 10/30, Batch 10, Loss: 3.0232\n",
      "Epoch 10/30, Batch 20, Loss: 2.9762\n",
      "Epoch 10/30, Batch 30, Loss: 2.9387\n",
      "Epoch 10/30, Batch 40, Loss: 2.9595\n",
      "Epoch 10/30, Batch 50, Loss: 3.0671\n",
      "Epoch 10/30, Batch 60, Loss: 2.9391\n",
      "Epoch 10/30, Batch 70, Loss: 3.0918\n",
      "Epoch 10/30, Batch 80, Loss: 2.9191\n",
      "Epoch 10/30, Batch 90, Loss: 3.0184\n",
      "Epoch 10/30, Batch 100, Loss: 3.0143\n",
      "Epoch 10/30, Batch 110, Loss: 3.0717\n",
      "Epoch 10/30, Batch 120, Loss: 2.9909\n",
      "Epoch 10/30, Batch 130, Loss: 2.9672\n",
      "Epoch 10/30, Batch 140, Loss: 2.9031\n",
      "Epoch 10/30, Batch 150, Loss: 2.9970\n",
      "Epoch 10/30, Batch 160, Loss: 2.9341\n",
      "Epoch 10/30, Batch 170, Loss: 2.9972\n",
      "Epoch 10/30, Batch 180, Loss: 2.9744\n",
      "Epoch 10/30, Batch 190, Loss: 2.9457\n",
      "Epoch 10/30, Batch 200, Loss: 3.0585\n",
      "Epoch 10/30, Batch 210, Loss: 3.0425\n",
      "Epoch 10/30, Batch 220, Loss: 2.9612\n",
      "Epoch 10/30, Batch 230, Loss: 2.8935\n",
      "Epoch 10/30, Batch 240, Loss: 3.0144\n",
      "Epoch 10/30, Batch 250, Loss: 3.0458\n",
      "Epoch 10/30, Batch 260, Loss: 2.8969\n",
      "Epoch 10/30, Batch 270, Loss: 2.8880\n",
      "Epoch 10/30, Batch 280, Loss: 2.9088\n",
      "Epoch 10/30, Batch 290, Loss: 2.9980\n",
      "Epoch 10/30, Batch 300, Loss: 3.0043\n",
      "Epoch 10/30, Batch 310, Loss: 3.1279\n",
      "Epoch 10/30, Batch 320, Loss: 3.1455\n",
      "Epoch 10/30, Batch 330, Loss: 2.9482\n",
      "Epoch 10/30, Batch 340, Loss: 3.0616\n",
      "Epoch 10/30, Batch 350, Loss: 2.9822\n",
      "Epoch 10/30, Batch 360, Loss: 3.0478\n",
      "Epoch 10/30, Batch 370, Loss: 2.9789\n",
      "Epoch 10/30, Batch 380, Loss: 3.0626\n",
      "Epoch 10/30, Batch 390, Loss: 3.1513\n",
      "Epoch 10/30, Batch 400, Loss: 3.0447\n",
      "Epoch 10/30, Batch 410, Loss: 3.1022\n",
      "Epoch 10/30, Batch 420, Loss: 2.9391\n",
      "Epoch 10/30, Batch 430, Loss: 2.9958\n",
      "Epoch 10/30, Batch 440, Loss: 3.0484\n",
      "Epoch 10/30, Batch 450, Loss: 2.9513\n",
      "Epoch 10/30, Batch 460, Loss: 3.0954\n",
      "Epoch 10/30, Batch 470, Loss: 2.9755\n",
      "Epoch 10/30, Batch 480, Loss: 3.0201\n",
      "Epoch 10/30, Batch 490, Loss: 2.9279\n",
      "Epoch 10/30, Batch 500, Loss: 3.0437\n",
      "Epoch 10/30, Batch 510, Loss: 3.0311\n",
      "Epoch 10/30, Batch 520, Loss: 3.0684\n",
      "Epoch 10/30, Batch 530, Loss: 3.0225\n",
      "Epoch 10/30, Batch 540, Loss: 2.9207\n",
      "Epoch 10/30, Batch 550, Loss: 3.0986\n",
      "Epoch 10/30, Batch 560, Loss: 2.9364\n",
      "Epoch 10/30, Batch 570, Loss: 2.9485\n",
      "Epoch 10/30, Batch 580, Loss: 3.0608\n",
      "Epoch 10/30, Batch 590, Loss: 2.9966\n",
      "Epoch 10/30, Batch 600, Loss: 3.0579\n",
      "Epoch 10/30, Batch 610, Loss: 2.9034\n",
      "Epoch 10/30, Batch 620, Loss: 2.9984\n",
      "Epoch 10/30, Batch 630, Loss: 3.0654\n",
      "Epoch 10/30, Batch 640, Loss: 3.0624\n",
      "Epoch 10/30, Batch 650, Loss: 2.9336\n",
      "Epoch 10/30, Batch 660, Loss: 2.8990\n",
      "Epoch 10/30, Batch 670, Loss: 2.8979\n",
      "Epoch 10/30, Batch 680, Loss: 3.1002\n",
      "Epoch 10/30, Batch 690, Loss: 2.9756\n",
      "Epoch 10/30, Batch 700, Loss: 2.9638\n",
      "Epoch 10/30, Batch 710, Loss: 2.9958\n",
      "Epoch 10/30, Batch 720, Loss: 3.1220\n",
      "Epoch 10/30, Batch 730, Loss: 2.9891\n",
      "Epoch 10/30, Batch 740, Loss: 3.0780\n",
      "Epoch 10/30, Batch 750, Loss: 3.0220\n",
      "Epoch 10/30, Batch 760, Loss: 3.0213\n",
      "Epoch 10/30, Batch 770, Loss: 3.0085\n",
      "Epoch 10/30, Batch 780, Loss: 3.2082\n",
      "Epoch 10/30, Batch 790, Loss: 2.9865\n",
      "Epoch 10/30, Batch 800, Loss: 3.0541\n",
      "Epoch 10/30, Batch 810, Loss: 2.9358\n",
      "Epoch 10/30, Batch 820, Loss: 3.0723\n",
      "Epoch 10/30, Batch 830, Loss: 3.0724\n",
      "Epoch 10/30, Batch 840, Loss: 3.1799\n",
      "Epoch 10/30, Batch 850, Loss: 2.9231\n",
      "Epoch 10/30, Batch 860, Loss: 3.0142\n",
      "Epoch 10/30, Batch 870, Loss: 3.0038\n",
      "Epoch 10/30, Batch 880, Loss: 2.9054\n",
      "Epoch 10/30, Batch 890, Loss: 2.9137\n",
      "Epoch 10/30, Batch 900, Loss: 3.1425\n",
      "Epoch 10/30, Batch 910, Loss: 3.0131\n",
      "Epoch 10/30, Batch 920, Loss: 3.0453\n",
      "Epoch 10/30, Batch 930, Loss: 3.0423\n",
      "Epoch 10/30, Batch 940, Loss: 2.9042\n",
      "Epoch 10/30, Batch 950, Loss: 2.9510\n",
      "Epoch 10/30, Batch 960, Loss: 3.0613\n",
      "Epoch 10/30, Batch 970, Loss: 3.0432\n",
      "Epoch 10/30, Batch 980, Loss: 3.1103\n",
      "Epoch 10/30, Batch 990, Loss: 2.9485\n",
      "Epoch 10/30, Batch 1000, Loss: 2.9453\n",
      "Epoch 10/30, Batch 1010, Loss: 3.0869\n",
      "Epoch 10/30, Batch 1020, Loss: 3.1066\n",
      "Epoch 10/30, Batch 1030, Loss: 3.0008\n",
      "Epoch 10/30, Batch 1040, Loss: 2.9141\n",
      "Epoch 10/30, Batch 1050, Loss: 2.9223\n",
      "Epoch 10/30, Batch 1060, Loss: 3.1019\n",
      "Epoch 10/30, Batch 1070, Loss: 3.0059\n",
      "Epoch 10/30, Batch 1080, Loss: 3.0596\n",
      "Epoch 10/30, Batch 1090, Loss: 3.0480\n",
      "Epoch 10/30, Batch 1100, Loss: 3.0768\n",
      "Epoch 10/30, Batch 1110, Loss: 2.9312\n",
      "Epoch 10/30, Batch 1120, Loss: 3.0339\n",
      "Epoch 10/30, Batch 1130, Loss: 2.9727\n",
      "Epoch 10/30, Batch 1140, Loss: 3.0689\n",
      "Epoch 10/30, Batch 1150, Loss: 2.9080\n",
      "Epoch 10/30, Batch 1160, Loss: 3.0958\n",
      "Epoch 10/30, Batch 1170, Loss: 2.9572\n",
      "Epoch 10/30, Batch 1180, Loss: 2.9399\n",
      "finish  save model of epoch : 9!\n",
      "epoch using time 1041.110\n",
      "Epoch 11/30, Batch 10, Loss: 3.0303\n",
      "Epoch 11/30, Batch 20, Loss: 2.8823\n",
      "Epoch 11/30, Batch 30, Loss: 2.8290\n",
      "Epoch 11/30, Batch 40, Loss: 2.8108\n",
      "Epoch 11/30, Batch 50, Loss: 3.0988\n",
      "Epoch 11/30, Batch 60, Loss: 3.0527\n",
      "Epoch 11/30, Batch 70, Loss: 3.0496\n",
      "Epoch 11/30, Batch 80, Loss: 2.8970\n",
      "Epoch 11/30, Batch 90, Loss: 3.0074\n",
      "Epoch 11/30, Batch 100, Loss: 3.0972\n",
      "Epoch 11/30, Batch 110, Loss: 3.0358\n",
      "Epoch 11/30, Batch 120, Loss: 2.8928\n",
      "Epoch 11/30, Batch 130, Loss: 3.0066\n",
      "Epoch 11/30, Batch 140, Loss: 2.9576\n",
      "Epoch 11/30, Batch 150, Loss: 3.0207\n",
      "Epoch 11/30, Batch 160, Loss: 2.9254\n",
      "Epoch 11/30, Batch 170, Loss: 3.0515\n",
      "Epoch 11/30, Batch 180, Loss: 2.8795\n",
      "Epoch 11/30, Batch 190, Loss: 2.9889\n",
      "Epoch 11/30, Batch 200, Loss: 3.0041\n",
      "Epoch 11/30, Batch 210, Loss: 2.7885\n",
      "Epoch 11/30, Batch 220, Loss: 2.9766\n",
      "Epoch 11/30, Batch 230, Loss: 3.0576\n",
      "Epoch 11/30, Batch 240, Loss: 3.0242\n",
      "Epoch 11/30, Batch 250, Loss: 2.9753\n",
      "Epoch 11/30, Batch 260, Loss: 3.1417\n",
      "Epoch 11/30, Batch 270, Loss: 2.8957\n",
      "Epoch 11/30, Batch 280, Loss: 2.9747\n",
      "Epoch 11/30, Batch 290, Loss: 2.9027\n",
      "Epoch 11/30, Batch 300, Loss: 2.9752\n",
      "Epoch 11/30, Batch 310, Loss: 2.9826\n",
      "Epoch 11/30, Batch 320, Loss: 3.0161\n",
      "Epoch 11/30, Batch 330, Loss: 3.0861\n",
      "Epoch 11/30, Batch 340, Loss: 2.8817\n",
      "Epoch 11/30, Batch 350, Loss: 2.9599\n",
      "Epoch 11/30, Batch 360, Loss: 2.9187\n",
      "Epoch 11/30, Batch 370, Loss: 2.8721\n",
      "Epoch 11/30, Batch 380, Loss: 2.8024\n",
      "Epoch 11/30, Batch 390, Loss: 2.9505\n",
      "Epoch 11/30, Batch 400, Loss: 2.8843\n",
      "Epoch 11/30, Batch 410, Loss: 2.9666\n",
      "Epoch 11/30, Batch 420, Loss: 2.8689\n",
      "Epoch 11/30, Batch 430, Loss: 2.9322\n",
      "Epoch 11/30, Batch 440, Loss: 2.9905\n",
      "Epoch 11/30, Batch 450, Loss: 3.0363\n",
      "Epoch 11/30, Batch 460, Loss: 2.9923\n",
      "Epoch 11/30, Batch 470, Loss: 3.0091\n",
      "Epoch 11/30, Batch 480, Loss: 2.8346\n",
      "Epoch 11/30, Batch 490, Loss: 3.0563\n",
      "Epoch 11/30, Batch 500, Loss: 2.9390\n",
      "Epoch 11/30, Batch 510, Loss: 3.0482\n",
      "Epoch 11/30, Batch 520, Loss: 3.0859\n",
      "Epoch 11/30, Batch 530, Loss: 2.9661\n",
      "Epoch 11/30, Batch 540, Loss: 2.9126\n",
      "Epoch 11/30, Batch 550, Loss: 2.9529\n",
      "Epoch 11/30, Batch 560, Loss: 3.0766\n",
      "Epoch 11/30, Batch 570, Loss: 3.0586\n",
      "Epoch 11/30, Batch 580, Loss: 3.0527\n",
      "Epoch 11/30, Batch 590, Loss: 3.0034\n",
      "Epoch 11/30, Batch 600, Loss: 2.9243\n",
      "Epoch 11/30, Batch 610, Loss: 3.1212\n",
      "Epoch 11/30, Batch 620, Loss: 3.0717\n",
      "Epoch 11/30, Batch 630, Loss: 2.9899\n",
      "Epoch 11/30, Batch 640, Loss: 2.8904\n",
      "Epoch 11/30, Batch 650, Loss: 3.0667\n",
      "Epoch 11/30, Batch 660, Loss: 2.9404\n",
      "Epoch 11/30, Batch 670, Loss: 2.8968\n",
      "Epoch 11/30, Batch 680, Loss: 2.8932\n",
      "Epoch 11/30, Batch 690, Loss: 2.8850\n",
      "Epoch 11/30, Batch 700, Loss: 3.0097\n",
      "Epoch 11/30, Batch 710, Loss: 2.8285\n",
      "Epoch 11/30, Batch 720, Loss: 2.9631\n",
      "Epoch 11/30, Batch 730, Loss: 3.0165\n",
      "Epoch 11/30, Batch 740, Loss: 2.8159\n",
      "Epoch 11/30, Batch 750, Loss: 2.8407\n",
      "Epoch 11/30, Batch 760, Loss: 2.9454\n",
      "Epoch 11/30, Batch 770, Loss: 3.0023\n",
      "Epoch 11/30, Batch 780, Loss: 2.9863\n",
      "Epoch 11/30, Batch 790, Loss: 2.9325\n",
      "Epoch 11/30, Batch 800, Loss: 2.8640\n",
      "Epoch 11/30, Batch 810, Loss: 2.9471\n",
      "Epoch 11/30, Batch 820, Loss: 3.1021\n",
      "Epoch 11/30, Batch 830, Loss: 3.0324\n",
      "Epoch 11/30, Batch 840, Loss: 3.1093\n",
      "Epoch 11/30, Batch 850, Loss: 3.2411\n",
      "Epoch 11/30, Batch 860, Loss: 3.0329\n",
      "Epoch 11/30, Batch 870, Loss: 2.9434\n",
      "Epoch 11/30, Batch 880, Loss: 2.9217\n",
      "Epoch 11/30, Batch 890, Loss: 2.9833\n",
      "Epoch 11/30, Batch 900, Loss: 2.9126\n",
      "Epoch 11/30, Batch 910, Loss: 2.9734\n",
      "Epoch 11/30, Batch 920, Loss: 2.9613\n",
      "Epoch 11/30, Batch 930, Loss: 2.9831\n",
      "Epoch 11/30, Batch 940, Loss: 3.0347\n",
      "Epoch 11/30, Batch 950, Loss: 2.9488\n",
      "Epoch 11/30, Batch 960, Loss: 3.0453\n",
      "Epoch 11/30, Batch 970, Loss: 2.9721\n",
      "Epoch 11/30, Batch 980, Loss: 3.1634\n",
      "Epoch 11/30, Batch 990, Loss: 3.0397\n",
      "Epoch 11/30, Batch 1000, Loss: 2.9664\n",
      "Epoch 11/30, Batch 1010, Loss: 3.0508\n",
      "Epoch 11/30, Batch 1020, Loss: 2.9182\n",
      "Epoch 11/30, Batch 1030, Loss: 2.9107\n",
      "Epoch 11/30, Batch 1040, Loss: 2.9593\n",
      "Epoch 11/30, Batch 1050, Loss: 2.8453\n",
      "Epoch 11/30, Batch 1060, Loss: 2.9711\n",
      "Epoch 11/30, Batch 1070, Loss: 2.8927\n",
      "Epoch 11/30, Batch 1080, Loss: 3.1142\n",
      "Epoch 11/30, Batch 1090, Loss: 3.0276\n",
      "Epoch 11/30, Batch 1100, Loss: 3.1063\n",
      "Epoch 11/30, Batch 1110, Loss: 3.0519\n",
      "Epoch 11/30, Batch 1120, Loss: 3.0092\n",
      "Epoch 11/30, Batch 1130, Loss: 3.0520\n",
      "Epoch 11/30, Batch 1140, Loss: 2.8889\n",
      "Epoch 11/30, Batch 1150, Loss: 2.9948\n",
      "Epoch 11/30, Batch 1160, Loss: 2.9471\n",
      "Epoch 11/30, Batch 1170, Loss: 2.9563\n",
      "Epoch 11/30, Batch 1180, Loss: 2.9958\n",
      "finish  save model of epoch : 10!\n",
      "epoch using time 1074.601\n",
      "Epoch 12/30, Batch 10, Loss: 3.1922\n",
      "Epoch 12/30, Batch 20, Loss: 2.8896\n",
      "Epoch 12/30, Batch 30, Loss: 3.0428\n",
      "Epoch 12/30, Batch 40, Loss: 2.8503\n",
      "Epoch 12/30, Batch 50, Loss: 3.0664\n",
      "Epoch 12/30, Batch 60, Loss: 2.9002\n",
      "Epoch 12/30, Batch 70, Loss: 2.8995\n",
      "Epoch 12/30, Batch 80, Loss: 2.8989\n",
      "Epoch 12/30, Batch 90, Loss: 2.9078\n",
      "Epoch 12/30, Batch 100, Loss: 2.8250\n",
      "Epoch 12/30, Batch 110, Loss: 2.8842\n",
      "Epoch 12/30, Batch 120, Loss: 2.8856\n",
      "Epoch 12/30, Batch 130, Loss: 2.9915\n",
      "Epoch 12/30, Batch 140, Loss: 2.7883\n",
      "Epoch 12/30, Batch 150, Loss: 3.0546\n",
      "Epoch 12/30, Batch 160, Loss: 2.9560\n",
      "Epoch 12/30, Batch 170, Loss: 2.9824\n",
      "Epoch 12/30, Batch 180, Loss: 2.9869\n",
      "Epoch 12/30, Batch 190, Loss: 3.0164\n",
      "Epoch 12/30, Batch 200, Loss: 3.0018\n",
      "Epoch 12/30, Batch 210, Loss: 2.8210\n",
      "Epoch 12/30, Batch 220, Loss: 2.9700\n",
      "Epoch 12/30, Batch 230, Loss: 2.8204\n",
      "Epoch 12/30, Batch 240, Loss: 2.8280\n",
      "Epoch 12/30, Batch 250, Loss: 2.8704\n",
      "Epoch 12/30, Batch 260, Loss: 2.8450\n",
      "Epoch 12/30, Batch 270, Loss: 2.8793\n",
      "Epoch 12/30, Batch 280, Loss: 2.8448\n",
      "Epoch 12/30, Batch 290, Loss: 2.9862\n",
      "Epoch 12/30, Batch 300, Loss: 2.9385\n",
      "Epoch 12/30, Batch 310, Loss: 3.0760\n",
      "Epoch 12/30, Batch 320, Loss: 2.9977\n",
      "Epoch 12/30, Batch 330, Loss: 2.9769\n",
      "Epoch 12/30, Batch 340, Loss: 2.9762\n",
      "Epoch 12/30, Batch 350, Loss: 2.9067\n",
      "Epoch 12/30, Batch 360, Loss: 3.0189\n",
      "Epoch 12/30, Batch 370, Loss: 3.0104\n",
      "Epoch 12/30, Batch 380, Loss: 2.9882\n",
      "Epoch 12/30, Batch 390, Loss: 2.9873\n",
      "Epoch 12/30, Batch 400, Loss: 2.9968\n",
      "Epoch 12/30, Batch 410, Loss: 2.9912\n",
      "Epoch 12/30, Batch 420, Loss: 3.1006\n",
      "Epoch 12/30, Batch 430, Loss: 2.8944\n",
      "Epoch 12/30, Batch 440, Loss: 2.9644\n",
      "Epoch 12/30, Batch 450, Loss: 3.0435\n",
      "Epoch 12/30, Batch 460, Loss: 2.9782\n",
      "Epoch 12/30, Batch 470, Loss: 2.9404\n",
      "Epoch 12/30, Batch 480, Loss: 2.9850\n",
      "Epoch 12/30, Batch 490, Loss: 3.0627\n",
      "Epoch 12/30, Batch 500, Loss: 2.8646\n",
      "Epoch 12/30, Batch 510, Loss: 3.1216\n",
      "Epoch 12/30, Batch 520, Loss: 2.8793\n",
      "Epoch 12/30, Batch 530, Loss: 3.0404\n",
      "Epoch 12/30, Batch 540, Loss: 2.8598\n",
      "Epoch 12/30, Batch 550, Loss: 3.0190\n",
      "Epoch 12/30, Batch 560, Loss: 2.9431\n",
      "Epoch 12/30, Batch 570, Loss: 2.9412\n",
      "Epoch 12/30, Batch 580, Loss: 3.0722\n",
      "Epoch 12/30, Batch 590, Loss: 2.9182\n",
      "Epoch 12/30, Batch 600, Loss: 2.9314\n",
      "Epoch 12/30, Batch 610, Loss: 2.9528\n",
      "Epoch 12/30, Batch 620, Loss: 3.0069\n",
      "Epoch 12/30, Batch 630, Loss: 2.9757\n",
      "Epoch 12/30, Batch 640, Loss: 2.8795\n",
      "Epoch 12/30, Batch 650, Loss: 3.1590\n",
      "Epoch 12/30, Batch 660, Loss: 2.9343\n",
      "Epoch 12/30, Batch 670, Loss: 2.9811\n",
      "Epoch 12/30, Batch 680, Loss: 2.9979\n",
      "Epoch 12/30, Batch 690, Loss: 2.9041\n",
      "Epoch 12/30, Batch 700, Loss: 3.1507\n",
      "Epoch 12/30, Batch 710, Loss: 2.9680\n",
      "Epoch 12/30, Batch 720, Loss: 2.9506\n",
      "Epoch 12/30, Batch 730, Loss: 3.0432\n",
      "Epoch 12/30, Batch 740, Loss: 3.0830\n",
      "Epoch 12/30, Batch 750, Loss: 2.9481\n",
      "Epoch 12/30, Batch 760, Loss: 3.0703\n",
      "Epoch 12/30, Batch 770, Loss: 3.1122\n",
      "Epoch 12/30, Batch 780, Loss: 2.9861\n",
      "Epoch 12/30, Batch 790, Loss: 3.1221\n",
      "Epoch 12/30, Batch 800, Loss: 2.8642\n",
      "Epoch 12/30, Batch 810, Loss: 2.9619\n",
      "Epoch 12/30, Batch 820, Loss: 3.0491\n",
      "Epoch 12/30, Batch 830, Loss: 2.8765\n",
      "Epoch 12/30, Batch 840, Loss: 2.8814\n",
      "Epoch 12/30, Batch 850, Loss: 3.0725\n",
      "Epoch 12/30, Batch 860, Loss: 3.0203\n",
      "Epoch 12/30, Batch 870, Loss: 3.0337\n",
      "Epoch 12/30, Batch 880, Loss: 3.0460\n",
      "Epoch 12/30, Batch 890, Loss: 3.0098\n",
      "Epoch 12/30, Batch 900, Loss: 3.0231\n",
      "Epoch 12/30, Batch 910, Loss: 2.9209\n",
      "Epoch 12/30, Batch 920, Loss: 2.9508\n",
      "Epoch 12/30, Batch 930, Loss: 2.9002\n",
      "Epoch 12/30, Batch 940, Loss: 2.9792\n",
      "Epoch 12/30, Batch 950, Loss: 2.9580\n",
      "Epoch 12/30, Batch 960, Loss: 2.9146\n",
      "Epoch 12/30, Batch 970, Loss: 3.0511\n",
      "Epoch 12/30, Batch 980, Loss: 2.9246\n",
      "Epoch 12/30, Batch 990, Loss: 3.0966\n",
      "Epoch 12/30, Batch 1000, Loss: 3.0701\n",
      "Epoch 12/30, Batch 1010, Loss: 2.9808\n",
      "Epoch 12/30, Batch 1020, Loss: 2.9759\n",
      "Epoch 12/30, Batch 1030, Loss: 3.0113\n",
      "Epoch 12/30, Batch 1040, Loss: 3.0824\n",
      "Epoch 12/30, Batch 1050, Loss: 2.9504\n",
      "Epoch 12/30, Batch 1060, Loss: 2.9220\n",
      "Epoch 12/30, Batch 1070, Loss: 2.8104\n",
      "Epoch 12/30, Batch 1080, Loss: 2.9848\n",
      "Epoch 12/30, Batch 1090, Loss: 2.9550\n",
      "Epoch 12/30, Batch 1100, Loss: 3.0788\n",
      "Epoch 12/30, Batch 1110, Loss: 3.0965\n",
      "Epoch 12/30, Batch 1120, Loss: 2.9597\n",
      "Epoch 12/30, Batch 1130, Loss: 3.0587\n",
      "Epoch 12/30, Batch 1140, Loss: 2.9843\n",
      "Epoch 12/30, Batch 1150, Loss: 3.0013\n",
      "Epoch 12/30, Batch 1160, Loss: 3.0254\n",
      "Epoch 12/30, Batch 1170, Loss: 2.9958\n",
      "Epoch 12/30, Batch 1180, Loss: 3.0656\n",
      "finish  save model of epoch : 11!\n",
      "epoch using time 1179.724\n",
      "Epoch 13/30, Batch 10, Loss: 2.8561\n",
      "Epoch 13/30, Batch 20, Loss: 2.9360\n",
      "Epoch 13/30, Batch 30, Loss: 2.9046\n",
      "Epoch 13/30, Batch 40, Loss: 2.9042\n",
      "Epoch 13/30, Batch 50, Loss: 2.9272\n",
      "Epoch 13/30, Batch 60, Loss: 2.8528\n",
      "Epoch 13/30, Batch 70, Loss: 2.8731\n",
      "Epoch 13/30, Batch 80, Loss: 2.9703\n",
      "Epoch 13/30, Batch 90, Loss: 2.8939\n",
      "Epoch 13/30, Batch 100, Loss: 2.8816\n",
      "Epoch 13/30, Batch 110, Loss: 2.9331\n",
      "Epoch 13/30, Batch 120, Loss: 2.9194\n",
      "Epoch 13/30, Batch 130, Loss: 2.8386\n",
      "Epoch 13/30, Batch 140, Loss: 2.9852\n",
      "Epoch 13/30, Batch 150, Loss: 2.8689\n",
      "Epoch 13/30, Batch 160, Loss: 2.9043\n",
      "Epoch 13/30, Batch 170, Loss: 2.9828\n",
      "Epoch 13/30, Batch 180, Loss: 2.9060\n",
      "Epoch 13/30, Batch 190, Loss: 2.8363\n",
      "Epoch 13/30, Batch 200, Loss: 2.8702\n",
      "Epoch 13/30, Batch 210, Loss: 2.8090\n",
      "Epoch 13/30, Batch 220, Loss: 3.0158\n",
      "Epoch 13/30, Batch 230, Loss: 3.0005\n",
      "Epoch 13/30, Batch 240, Loss: 3.1084\n",
      "Epoch 13/30, Batch 250, Loss: 3.0623\n",
      "Epoch 13/30, Batch 260, Loss: 2.9392\n",
      "Epoch 13/30, Batch 270, Loss: 2.9554\n",
      "Epoch 13/30, Batch 280, Loss: 2.9145\n",
      "Epoch 13/30, Batch 290, Loss: 3.0336\n",
      "Epoch 13/30, Batch 300, Loss: 2.8519\n",
      "Epoch 13/30, Batch 310, Loss: 2.9941\n",
      "Epoch 13/30, Batch 320, Loss: 2.8996\n",
      "Epoch 13/30, Batch 330, Loss: 3.1442\n",
      "Epoch 13/30, Batch 340, Loss: 2.7848\n",
      "Epoch 13/30, Batch 350, Loss: 2.9784\n",
      "Epoch 13/30, Batch 360, Loss: 2.7877\n",
      "Epoch 13/30, Batch 370, Loss: 2.8878\n",
      "Epoch 13/30, Batch 380, Loss: 3.0369\n",
      "Epoch 13/30, Batch 390, Loss: 3.0187\n",
      "Epoch 13/30, Batch 400, Loss: 3.0349\n",
      "Epoch 13/30, Batch 410, Loss: 2.9292\n",
      "Epoch 13/30, Batch 420, Loss: 2.8843\n",
      "Epoch 13/30, Batch 430, Loss: 3.0254\n",
      "Epoch 13/30, Batch 440, Loss: 2.9055\n",
      "Epoch 13/30, Batch 450, Loss: 2.9303\n",
      "Epoch 13/30, Batch 460, Loss: 2.9264\n",
      "Epoch 13/30, Batch 470, Loss: 2.9906\n",
      "Epoch 13/30, Batch 480, Loss: 2.9984\n",
      "Epoch 13/30, Batch 490, Loss: 2.9988\n",
      "Epoch 13/30, Batch 500, Loss: 2.9443\n",
      "Epoch 13/30, Batch 510, Loss: 3.0191\n",
      "Epoch 13/30, Batch 520, Loss: 2.9346\n",
      "Epoch 13/30, Batch 530, Loss: 2.9506\n",
      "Epoch 13/30, Batch 540, Loss: 2.9730\n",
      "Epoch 13/30, Batch 550, Loss: 2.8955\n",
      "Epoch 13/30, Batch 560, Loss: 2.8910\n",
      "Epoch 13/30, Batch 570, Loss: 2.8545\n",
      "Epoch 13/30, Batch 580, Loss: 2.9112\n",
      "Epoch 13/30, Batch 590, Loss: 2.8971\n",
      "Epoch 13/30, Batch 600, Loss: 2.9447\n",
      "Epoch 13/30, Batch 610, Loss: 2.9076\n",
      "Epoch 13/30, Batch 620, Loss: 3.0332\n",
      "Epoch 13/30, Batch 630, Loss: 2.9423\n",
      "Epoch 13/30, Batch 640, Loss: 2.9772\n",
      "Epoch 13/30, Batch 650, Loss: 3.0098\n",
      "Epoch 13/30, Batch 660, Loss: 2.9473\n",
      "Epoch 13/30, Batch 670, Loss: 3.0225\n",
      "Epoch 13/30, Batch 680, Loss: 3.1405\n",
      "Epoch 13/30, Batch 690, Loss: 2.9940\n",
      "Epoch 13/30, Batch 700, Loss: 2.9744\n",
      "Epoch 13/30, Batch 710, Loss: 2.9342\n",
      "Epoch 13/30, Batch 720, Loss: 2.9551\n",
      "Epoch 13/30, Batch 730, Loss: 3.1860\n",
      "Epoch 13/30, Batch 740, Loss: 3.1189\n",
      "Epoch 13/30, Batch 750, Loss: 2.9064\n",
      "Epoch 13/30, Batch 760, Loss: 2.9101\n",
      "Epoch 13/30, Batch 770, Loss: 2.9808\n",
      "Epoch 13/30, Batch 780, Loss: 2.9971\n",
      "Epoch 13/30, Batch 790, Loss: 3.0196\n",
      "Epoch 13/30, Batch 800, Loss: 3.1036\n",
      "Epoch 13/30, Batch 810, Loss: 2.9166\n",
      "Epoch 13/30, Batch 820, Loss: 3.0741\n",
      "Epoch 13/30, Batch 830, Loss: 2.9145\n",
      "Epoch 13/30, Batch 840, Loss: 3.1238\n",
      "Epoch 13/30, Batch 850, Loss: 2.9778\n",
      "Epoch 13/30, Batch 860, Loss: 3.0915\n",
      "Epoch 13/30, Batch 870, Loss: 2.9982\n",
      "Epoch 13/30, Batch 880, Loss: 2.8209\n",
      "Epoch 13/30, Batch 890, Loss: 3.0849\n",
      "Epoch 13/30, Batch 900, Loss: 2.9196\n",
      "Epoch 13/30, Batch 910, Loss: 3.0433\n",
      "Epoch 13/30, Batch 920, Loss: 2.9200\n",
      "Epoch 13/30, Batch 930, Loss: 3.0739\n",
      "Epoch 13/30, Batch 940, Loss: 2.9192\n",
      "Epoch 13/30, Batch 950, Loss: 2.9897\n",
      "Epoch 13/30, Batch 960, Loss: 2.9954\n",
      "Epoch 13/30, Batch 970, Loss: 2.9289\n",
      "Epoch 13/30, Batch 980, Loss: 3.0169\n",
      "Epoch 13/30, Batch 990, Loss: 2.9533\n",
      "Epoch 13/30, Batch 1000, Loss: 2.9496\n",
      "Epoch 13/30, Batch 1010, Loss: 2.9455\n",
      "Epoch 13/30, Batch 1020, Loss: 3.0565\n",
      "Epoch 13/30, Batch 1030, Loss: 2.8769\n",
      "Epoch 13/30, Batch 1040, Loss: 3.0123\n",
      "Epoch 13/30, Batch 1050, Loss: 3.0088\n",
      "Epoch 13/30, Batch 1060, Loss: 2.8934\n",
      "Epoch 13/30, Batch 1070, Loss: 2.9155\n",
      "Epoch 13/30, Batch 1080, Loss: 3.0544\n",
      "Epoch 13/30, Batch 1090, Loss: 2.9117\n",
      "Epoch 13/30, Batch 1100, Loss: 2.9696\n",
      "Epoch 13/30, Batch 1110, Loss: 2.9855\n",
      "Epoch 13/30, Batch 1120, Loss: 2.9322\n",
      "Epoch 13/30, Batch 1130, Loss: 2.9713\n",
      "Epoch 13/30, Batch 1140, Loss: 2.9632\n",
      "Epoch 13/30, Batch 1150, Loss: 2.9384\n",
      "Epoch 13/30, Batch 1160, Loss: 2.9548\n",
      "Epoch 13/30, Batch 1170, Loss: 2.9710\n",
      "Epoch 13/30, Batch 1180, Loss: 2.9752\n",
      "finish  save model of epoch : 12!\n",
      "epoch using time 1240.015\n",
      "Epoch 14/30, Batch 10, Loss: 2.9875\n",
      "Epoch 14/30, Batch 20, Loss: 2.8398\n",
      "Epoch 14/30, Batch 30, Loss: 3.0298\n",
      "Epoch 14/30, Batch 40, Loss: 2.9149\n",
      "Epoch 14/30, Batch 50, Loss: 3.0252\n",
      "Epoch 14/30, Batch 60, Loss: 2.8361\n",
      "Epoch 14/30, Batch 70, Loss: 2.9204\n",
      "Epoch 14/30, Batch 80, Loss: 2.9385\n",
      "Epoch 14/30, Batch 90, Loss: 3.1068\n",
      "Epoch 14/30, Batch 100, Loss: 2.8765\n",
      "Epoch 14/30, Batch 110, Loss: 2.9265\n",
      "Epoch 14/30, Batch 120, Loss: 2.9801\n",
      "Epoch 14/30, Batch 130, Loss: 2.9568\n",
      "Epoch 14/30, Batch 140, Loss: 2.8180\n",
      "Epoch 14/30, Batch 150, Loss: 3.0451\n",
      "Epoch 14/30, Batch 160, Loss: 2.9530\n",
      "Epoch 14/30, Batch 170, Loss: 2.9896\n",
      "Epoch 14/30, Batch 180, Loss: 2.9759\n",
      "Epoch 14/30, Batch 190, Loss: 2.8534\n",
      "Epoch 14/30, Batch 200, Loss: 2.9376\n",
      "Epoch 14/30, Batch 210, Loss: 2.9576\n",
      "Epoch 14/30, Batch 220, Loss: 3.0322\n",
      "Epoch 14/30, Batch 230, Loss: 2.9452\n",
      "Epoch 14/30, Batch 240, Loss: 2.8200\n",
      "Epoch 14/30, Batch 250, Loss: 2.9092\n",
      "Epoch 14/30, Batch 260, Loss: 2.9285\n",
      "Epoch 14/30, Batch 270, Loss: 3.0065\n",
      "Epoch 14/30, Batch 280, Loss: 2.9299\n",
      "Epoch 14/30, Batch 290, Loss: 2.9818\n",
      "Epoch 14/30, Batch 300, Loss: 2.9676\n",
      "Epoch 14/30, Batch 310, Loss: 2.9029\n",
      "Epoch 14/30, Batch 320, Loss: 2.9827\n",
      "Epoch 14/30, Batch 330, Loss: 2.9526\n",
      "Epoch 14/30, Batch 340, Loss: 2.8326\n",
      "Epoch 14/30, Batch 350, Loss: 2.8761\n",
      "Epoch 14/30, Batch 360, Loss: 2.8522\n",
      "Epoch 14/30, Batch 370, Loss: 3.0327\n",
      "Epoch 14/30, Batch 380, Loss: 2.7903\n",
      "Epoch 14/30, Batch 390, Loss: 2.9514\n",
      "Epoch 14/30, Batch 400, Loss: 2.8594\n",
      "Epoch 14/30, Batch 410, Loss: 2.9339\n",
      "Epoch 14/30, Batch 420, Loss: 2.8904\n",
      "Epoch 14/30, Batch 430, Loss: 2.9471\n",
      "Epoch 14/30, Batch 440, Loss: 2.9351\n",
      "Epoch 14/30, Batch 450, Loss: 3.1017\n",
      "Epoch 14/30, Batch 460, Loss: 2.9158\n",
      "Epoch 14/30, Batch 470, Loss: 2.9200\n",
      "Epoch 14/30, Batch 480, Loss: 3.0715\n",
      "Epoch 14/30, Batch 490, Loss: 2.8344\n",
      "Epoch 14/30, Batch 500, Loss: 2.7762\n",
      "Epoch 14/30, Batch 510, Loss: 2.8109\n",
      "Epoch 14/30, Batch 520, Loss: 2.8654\n",
      "Epoch 14/30, Batch 530, Loss: 2.9548\n",
      "Epoch 14/30, Batch 540, Loss: 2.8276\n",
      "Epoch 14/30, Batch 550, Loss: 2.9169\n",
      "Epoch 14/30, Batch 560, Loss: 2.9026\n",
      "Epoch 14/30, Batch 570, Loss: 2.8772\n",
      "Epoch 14/30, Batch 580, Loss: 3.0199\n",
      "Epoch 14/30, Batch 590, Loss: 2.9160\n",
      "Epoch 14/30, Batch 600, Loss: 3.0262\n",
      "Epoch 14/30, Batch 610, Loss: 2.9749\n",
      "Epoch 14/30, Batch 620, Loss: 2.9420\n",
      "Epoch 14/30, Batch 630, Loss: 2.9608\n",
      "Epoch 14/30, Batch 640, Loss: 3.0056\n",
      "Epoch 14/30, Batch 650, Loss: 2.9503\n",
      "Epoch 14/30, Batch 660, Loss: 2.9947\n",
      "Epoch 14/30, Batch 670, Loss: 2.9694\n",
      "Epoch 14/30, Batch 680, Loss: 2.9551\n",
      "Epoch 14/30, Batch 690, Loss: 2.9436\n",
      "Epoch 14/30, Batch 700, Loss: 3.1441\n",
      "Epoch 14/30, Batch 710, Loss: 3.0797\n",
      "Epoch 14/30, Batch 720, Loss: 3.0593\n",
      "Epoch 14/30, Batch 730, Loss: 2.9385\n",
      "Epoch 14/30, Batch 740, Loss: 3.0516\n",
      "Epoch 14/30, Batch 750, Loss: 2.9771\n",
      "Epoch 14/30, Batch 760, Loss: 2.9529\n",
      "Epoch 14/30, Batch 770, Loss: 2.9799\n",
      "Epoch 14/30, Batch 780, Loss: 2.9617\n",
      "Epoch 14/30, Batch 790, Loss: 3.0382\n",
      "Epoch 14/30, Batch 800, Loss: 3.0894\n",
      "Epoch 14/30, Batch 810, Loss: 2.9840\n",
      "Epoch 14/30, Batch 820, Loss: 2.9838\n",
      "Epoch 14/30, Batch 830, Loss: 2.9602\n",
      "Epoch 14/30, Batch 840, Loss: 3.0304\n",
      "Epoch 14/30, Batch 850, Loss: 2.9450\n",
      "Epoch 14/30, Batch 860, Loss: 3.0001\n",
      "Epoch 14/30, Batch 870, Loss: 3.0465\n",
      "Epoch 14/30, Batch 880, Loss: 2.9950\n",
      "Epoch 14/30, Batch 890, Loss: 3.0004\n",
      "Epoch 14/30, Batch 900, Loss: 3.0182\n",
      "Epoch 14/30, Batch 910, Loss: 2.9941\n",
      "Epoch 14/30, Batch 920, Loss: 2.9046\n",
      "Epoch 14/30, Batch 930, Loss: 3.0644\n",
      "Epoch 14/30, Batch 940, Loss: 2.9807\n",
      "Epoch 14/30, Batch 950, Loss: 2.9099\n",
      "Epoch 14/30, Batch 960, Loss: 2.9427\n",
      "Epoch 14/30, Batch 970, Loss: 2.7820\n",
      "Epoch 14/30, Batch 980, Loss: 2.9647\n",
      "Epoch 14/30, Batch 990, Loss: 2.9963\n",
      "Epoch 14/30, Batch 1000, Loss: 2.9400\n",
      "Epoch 14/30, Batch 1010, Loss: 2.8974\n",
      "Epoch 14/30, Batch 1020, Loss: 3.0835\n",
      "Epoch 14/30, Batch 1030, Loss: 3.0417\n",
      "Epoch 14/30, Batch 1040, Loss: 3.0460\n",
      "Epoch 14/30, Batch 1050, Loss: 2.8741\n",
      "Epoch 14/30, Batch 1060, Loss: 2.9268\n",
      "Epoch 14/30, Batch 1070, Loss: 3.0480\n",
      "Epoch 14/30, Batch 1080, Loss: 2.9490\n",
      "Epoch 14/30, Batch 1090, Loss: 3.1078\n",
      "Epoch 14/30, Batch 1100, Loss: 3.0122\n",
      "Epoch 14/30, Batch 1110, Loss: 2.9786\n",
      "Epoch 14/30, Batch 1120, Loss: 3.0964\n",
      "Epoch 14/30, Batch 1130, Loss: 2.8919\n",
      "Epoch 14/30, Batch 1140, Loss: 2.9429\n",
      "Epoch 14/30, Batch 1150, Loss: 3.0156\n",
      "Epoch 14/30, Batch 1160, Loss: 3.0185\n",
      "Epoch 14/30, Batch 1170, Loss: 3.0014\n",
      "Epoch 14/30, Batch 1180, Loss: 3.0223\n",
      "finish  save model of epoch : 13!\n",
      "epoch using time 1354.648\n",
      "Epoch 15/30, Batch 10, Loss: 2.8725\n",
      "Epoch 15/30, Batch 20, Loss: 2.9295\n",
      "Epoch 15/30, Batch 30, Loss: 3.0024\n",
      "Epoch 15/30, Batch 40, Loss: 2.7929\n",
      "Epoch 15/30, Batch 50, Loss: 2.9119\n",
      "Epoch 15/30, Batch 60, Loss: 2.8774\n",
      "Epoch 15/30, Batch 70, Loss: 3.0052\n",
      "Epoch 15/30, Batch 80, Loss: 2.9244\n",
      "Epoch 15/30, Batch 90, Loss: 2.8112\n",
      "Epoch 15/30, Batch 100, Loss: 3.0334\n",
      "Epoch 15/30, Batch 110, Loss: 2.8614\n",
      "Epoch 15/30, Batch 120, Loss: 2.8599\n",
      "Epoch 15/30, Batch 130, Loss: 2.9454\n",
      "Epoch 15/30, Batch 140, Loss: 2.9930\n",
      "Epoch 15/30, Batch 150, Loss: 2.9204\n",
      "Epoch 15/30, Batch 160, Loss: 2.9471\n",
      "Epoch 15/30, Batch 170, Loss: 2.9470\n",
      "Epoch 15/30, Batch 180, Loss: 2.8653\n",
      "Epoch 15/30, Batch 190, Loss: 2.9701\n",
      "Epoch 15/30, Batch 200, Loss: 2.7764\n",
      "Epoch 15/30, Batch 210, Loss: 2.7775\n",
      "Epoch 15/30, Batch 220, Loss: 2.8269\n",
      "Epoch 15/30, Batch 230, Loss: 2.8579\n",
      "Epoch 15/30, Batch 240, Loss: 2.8583\n",
      "Epoch 15/30, Batch 250, Loss: 2.9351\n",
      "Epoch 15/30, Batch 260, Loss: 2.9771\n",
      "Epoch 15/30, Batch 270, Loss: 3.1086\n",
      "Epoch 15/30, Batch 280, Loss: 2.8978\n",
      "Epoch 15/30, Batch 290, Loss: 2.9028\n",
      "Epoch 15/30, Batch 300, Loss: 2.9989\n",
      "Epoch 15/30, Batch 310, Loss: 3.0767\n",
      "Epoch 15/30, Batch 320, Loss: 2.9342\n",
      "Epoch 15/30, Batch 330, Loss: 2.8759\n",
      "Epoch 15/30, Batch 340, Loss: 2.7922\n",
      "Epoch 15/30, Batch 350, Loss: 2.8104\n",
      "Epoch 15/30, Batch 360, Loss: 2.8786\n",
      "Epoch 15/30, Batch 370, Loss: 2.9687\n",
      "Epoch 15/30, Batch 380, Loss: 2.8960\n",
      "Epoch 15/30, Batch 390, Loss: 2.9494\n",
      "Epoch 15/30, Batch 400, Loss: 2.9142\n",
      "Epoch 15/30, Batch 410, Loss: 3.0517\n",
      "Epoch 15/30, Batch 420, Loss: 2.8362\n",
      "Epoch 15/30, Batch 430, Loss: 2.8984\n",
      "Epoch 15/30, Batch 440, Loss: 2.9485\n",
      "Epoch 15/30, Batch 450, Loss: 3.0084\n",
      "Epoch 15/30, Batch 460, Loss: 2.8912\n",
      "Epoch 15/30, Batch 470, Loss: 2.9895\n",
      "Epoch 15/30, Batch 480, Loss: 3.0015\n",
      "Epoch 15/30, Batch 490, Loss: 2.9867\n",
      "Epoch 15/30, Batch 500, Loss: 2.9078\n",
      "Epoch 15/30, Batch 510, Loss: 2.8651\n",
      "Epoch 15/30, Batch 520, Loss: 2.8791\n",
      "Epoch 15/30, Batch 530, Loss: 2.9718\n",
      "Epoch 15/30, Batch 540, Loss: 2.9537\n",
      "Epoch 15/30, Batch 550, Loss: 2.9742\n",
      "Epoch 15/30, Batch 560, Loss: 3.0217\n",
      "Epoch 15/30, Batch 570, Loss: 3.0569\n",
      "Epoch 15/30, Batch 580, Loss: 3.0097\n",
      "Epoch 15/30, Batch 590, Loss: 2.8307\n",
      "Epoch 15/30, Batch 600, Loss: 2.9007\n",
      "Epoch 15/30, Batch 610, Loss: 2.9198\n",
      "Epoch 15/30, Batch 620, Loss: 2.9196\n",
      "Epoch 15/30, Batch 630, Loss: 2.9157\n",
      "Epoch 15/30, Batch 640, Loss: 2.9160\n",
      "Epoch 15/30, Batch 650, Loss: 3.0326\n",
      "Epoch 15/30, Batch 660, Loss: 2.9762\n",
      "Epoch 15/30, Batch 670, Loss: 2.9731\n",
      "Epoch 15/30, Batch 680, Loss: 2.8286\n",
      "Epoch 15/30, Batch 690, Loss: 2.9762\n",
      "Epoch 15/30, Batch 700, Loss: 3.0667\n",
      "Epoch 15/30, Batch 710, Loss: 2.8735\n",
      "Epoch 15/30, Batch 720, Loss: 2.9517\n",
      "Epoch 15/30, Batch 730, Loss: 2.7826\n",
      "Epoch 15/30, Batch 740, Loss: 2.9945\n",
      "Epoch 15/30, Batch 750, Loss: 2.9697\n",
      "Epoch 15/30, Batch 760, Loss: 2.8657\n",
      "Epoch 15/30, Batch 770, Loss: 2.8123\n",
      "Epoch 15/30, Batch 780, Loss: 2.9722\n",
      "Epoch 15/30, Batch 790, Loss: 3.1073\n",
      "Epoch 15/30, Batch 800, Loss: 2.9796\n",
      "Epoch 15/30, Batch 810, Loss: 2.9164\n",
      "Epoch 15/30, Batch 820, Loss: 2.8879\n",
      "Epoch 15/30, Batch 830, Loss: 3.1018\n",
      "Epoch 15/30, Batch 840, Loss: 2.8730\n",
      "Epoch 15/30, Batch 850, Loss: 3.1573\n",
      "Epoch 15/30, Batch 860, Loss: 2.7810\n",
      "Epoch 15/30, Batch 870, Loss: 2.8774\n",
      "Epoch 15/30, Batch 880, Loss: 2.9472\n",
      "Epoch 15/30, Batch 890, Loss: 2.8431\n",
      "Epoch 15/30, Batch 900, Loss: 3.0121\n",
      "Epoch 15/30, Batch 910, Loss: 3.0439\n",
      "Epoch 15/30, Batch 920, Loss: 2.9926\n",
      "Epoch 15/30, Batch 930, Loss: 2.8951\n",
      "Epoch 15/30, Batch 940, Loss: 3.0954\n",
      "Epoch 15/30, Batch 950, Loss: 3.0557\n",
      "Epoch 15/30, Batch 960, Loss: 3.0433\n",
      "Epoch 15/30, Batch 970, Loss: 3.0115\n",
      "Epoch 15/30, Batch 980, Loss: 3.0511\n",
      "Epoch 15/30, Batch 990, Loss: 3.0142\n",
      "Epoch 15/30, Batch 1000, Loss: 2.9219\n",
      "Epoch 15/30, Batch 1010, Loss: 2.9946\n",
      "Epoch 15/30, Batch 1020, Loss: 2.9234\n",
      "Epoch 15/30, Batch 1030, Loss: 2.9622\n",
      "Epoch 15/30, Batch 1040, Loss: 2.9554\n",
      "Epoch 15/30, Batch 1050, Loss: 3.0163\n",
      "Epoch 15/30, Batch 1060, Loss: 3.0172\n",
      "Epoch 15/30, Batch 1070, Loss: 3.1547\n",
      "Epoch 15/30, Batch 1080, Loss: 3.0280\n",
      "Epoch 15/30, Batch 1090, Loss: 3.0118\n",
      "Epoch 15/30, Batch 1100, Loss: 2.9592\n",
      "Epoch 15/30, Batch 1110, Loss: 2.9405\n",
      "Epoch 15/30, Batch 1120, Loss: 2.9549\n",
      "Epoch 15/30, Batch 1130, Loss: 3.0226\n",
      "Epoch 15/30, Batch 1140, Loss: 3.0409\n",
      "Epoch 15/30, Batch 1150, Loss: 3.0381\n",
      "Epoch 15/30, Batch 1160, Loss: 2.8133\n",
      "Epoch 15/30, Batch 1170, Loss: 2.9044\n",
      "Epoch 15/30, Batch 1180, Loss: 2.9304\n",
      "finish  save model of epoch : 14!\n",
      "epoch using time 1568.257\n",
      "Epoch 16/30, Batch 10, Loss: 2.8646\n",
      "Epoch 16/30, Batch 20, Loss: 2.8206\n",
      "Epoch 16/30, Batch 30, Loss: 2.9305\n",
      "Epoch 16/30, Batch 40, Loss: 3.0664\n",
      "Epoch 16/30, Batch 50, Loss: 2.9181\n",
      "Epoch 16/30, Batch 60, Loss: 2.9079\n",
      "Epoch 16/30, Batch 70, Loss: 2.9166\n",
      "Epoch 16/30, Batch 80, Loss: 2.8500\n",
      "Epoch 16/30, Batch 90, Loss: 2.9553\n",
      "Epoch 16/30, Batch 100, Loss: 2.8498\n",
      "Epoch 16/30, Batch 110, Loss: 2.9207\n",
      "Epoch 16/30, Batch 120, Loss: 2.8529\n",
      "Epoch 16/30, Batch 130, Loss: 3.0245\n",
      "Epoch 16/30, Batch 140, Loss: 2.8089\n",
      "Epoch 16/30, Batch 150, Loss: 2.9067\n",
      "Epoch 16/30, Batch 160, Loss: 2.8990\n",
      "Epoch 16/30, Batch 170, Loss: 2.9203\n",
      "Epoch 16/30, Batch 180, Loss: 2.9530\n",
      "Epoch 16/30, Batch 190, Loss: 2.9780\n",
      "Epoch 16/30, Batch 200, Loss: 2.8799\n",
      "Epoch 16/30, Batch 210, Loss: 2.7928\n",
      "Epoch 16/30, Batch 220, Loss: 2.9055\n",
      "Epoch 16/30, Batch 230, Loss: 2.9315\n",
      "Epoch 16/30, Batch 240, Loss: 2.9434\n",
      "Epoch 16/30, Batch 250, Loss: 2.7822\n",
      "Epoch 16/30, Batch 260, Loss: 2.8776\n",
      "Epoch 16/30, Batch 270, Loss: 2.9966\n",
      "Epoch 16/30, Batch 280, Loss: 2.9126\n",
      "Epoch 16/30, Batch 290, Loss: 2.7422\n",
      "Epoch 16/30, Batch 300, Loss: 2.9097\n",
      "Epoch 16/30, Batch 310, Loss: 2.9219\n",
      "Epoch 16/30, Batch 320, Loss: 2.9557\n",
      "Epoch 16/30, Batch 330, Loss: 2.8835\n",
      "Epoch 16/30, Batch 340, Loss: 2.7958\n",
      "Epoch 16/30, Batch 350, Loss: 3.0464\n",
      "Epoch 16/30, Batch 360, Loss: 2.8680\n",
      "Epoch 16/30, Batch 370, Loss: 2.8672\n",
      "Epoch 16/30, Batch 380, Loss: 2.8754\n",
      "Epoch 16/30, Batch 390, Loss: 3.0710\n",
      "Epoch 16/30, Batch 400, Loss: 2.9179\n",
      "Epoch 16/30, Batch 410, Loss: 2.9667\n",
      "Epoch 16/30, Batch 420, Loss: 2.9692\n",
      "Epoch 16/30, Batch 430, Loss: 2.9462\n",
      "Epoch 16/30, Batch 440, Loss: 2.8050\n",
      "Epoch 16/30, Batch 450, Loss: 2.8626\n",
      "Epoch 16/30, Batch 460, Loss: 2.8337\n",
      "Epoch 16/30, Batch 470, Loss: 2.8588\n",
      "Epoch 16/30, Batch 480, Loss: 2.9565\n",
      "Epoch 16/30, Batch 490, Loss: 2.9529\n",
      "Epoch 16/30, Batch 500, Loss: 2.7736\n",
      "Epoch 16/30, Batch 510, Loss: 3.0064\n",
      "Epoch 16/30, Batch 520, Loss: 2.9947\n",
      "Epoch 16/30, Batch 530, Loss: 2.9198\n",
      "Epoch 16/30, Batch 540, Loss: 2.9673\n",
      "Epoch 16/30, Batch 550, Loss: 2.9828\n",
      "Epoch 16/30, Batch 560, Loss: 2.8667\n",
      "Epoch 16/30, Batch 570, Loss: 2.9871\n",
      "Epoch 16/30, Batch 580, Loss: 2.9157\n",
      "Epoch 16/30, Batch 590, Loss: 3.0419\n",
      "Epoch 16/30, Batch 600, Loss: 2.9492\n",
      "Epoch 16/30, Batch 610, Loss: 2.8800\n",
      "Epoch 16/30, Batch 620, Loss: 2.7580\n",
      "Epoch 16/30, Batch 630, Loss: 2.8984\n",
      "Epoch 16/30, Batch 640, Loss: 2.7992\n",
      "Epoch 16/30, Batch 650, Loss: 3.0245\n",
      "Epoch 16/30, Batch 660, Loss: 2.9185\n",
      "Epoch 16/30, Batch 670, Loss: 2.9482\n",
      "Epoch 16/30, Batch 680, Loss: 3.0522\n",
      "Epoch 16/30, Batch 690, Loss: 2.8903\n",
      "Epoch 16/30, Batch 700, Loss: 2.9721\n",
      "Epoch 16/30, Batch 710, Loss: 2.9565\n",
      "Epoch 16/30, Batch 720, Loss: 2.9716\n",
      "Epoch 16/30, Batch 730, Loss: 3.0564\n",
      "Epoch 16/30, Batch 740, Loss: 2.8288\n",
      "Epoch 16/30, Batch 750, Loss: 3.0142\n",
      "Epoch 16/30, Batch 760, Loss: 2.8775\n",
      "Epoch 16/30, Batch 770, Loss: 2.8893\n",
      "Epoch 16/30, Batch 780, Loss: 2.9191\n",
      "Epoch 16/30, Batch 790, Loss: 3.0277\n",
      "Epoch 16/30, Batch 800, Loss: 2.9739\n",
      "Epoch 16/30, Batch 810, Loss: 2.8128\n",
      "Epoch 16/30, Batch 820, Loss: 3.0156\n",
      "Epoch 16/30, Batch 830, Loss: 2.9162\n",
      "Epoch 16/30, Batch 840, Loss: 2.9567\n",
      "Epoch 16/30, Batch 850, Loss: 2.9135\n",
      "Epoch 16/30, Batch 860, Loss: 2.9609\n",
      "Epoch 16/30, Batch 870, Loss: 3.0494\n",
      "Epoch 16/30, Batch 880, Loss: 2.9864\n",
      "Epoch 16/30, Batch 890, Loss: 3.0298\n",
      "Epoch 16/30, Batch 900, Loss: 3.0448\n",
      "Epoch 16/30, Batch 910, Loss: 3.0566\n",
      "Epoch 16/30, Batch 920, Loss: 2.9377\n",
      "Epoch 16/30, Batch 930, Loss: 2.9991\n",
      "Epoch 16/30, Batch 940, Loss: 2.9652\n",
      "Epoch 16/30, Batch 950, Loss: 2.8851\n",
      "Epoch 16/30, Batch 960, Loss: 2.8742\n",
      "Epoch 16/30, Batch 970, Loss: 2.9427\n",
      "Epoch 16/30, Batch 980, Loss: 2.8264\n",
      "Epoch 16/30, Batch 990, Loss: 2.9745\n",
      "Epoch 16/30, Batch 1000, Loss: 2.9242\n",
      "Epoch 16/30, Batch 1010, Loss: 2.9170\n",
      "Epoch 16/30, Batch 1020, Loss: 2.8521\n",
      "Epoch 16/30, Batch 1030, Loss: 3.0424\n",
      "Epoch 16/30, Batch 1040, Loss: 2.9567\n",
      "Epoch 16/30, Batch 1050, Loss: 2.8875\n",
      "Epoch 16/30, Batch 1060, Loss: 3.0340\n",
      "Epoch 16/30, Batch 1070, Loss: 3.0772\n",
      "Epoch 16/30, Batch 1080, Loss: 2.9409\n",
      "Epoch 16/30, Batch 1090, Loss: 2.9014\n",
      "Epoch 16/30, Batch 1100, Loss: 2.9474\n",
      "Epoch 16/30, Batch 1110, Loss: 2.9500\n",
      "Epoch 16/30, Batch 1120, Loss: 2.9170\n",
      "Epoch 16/30, Batch 1130, Loss: 2.9928\n",
      "Epoch 16/30, Batch 1140, Loss: 2.9562\n",
      "Epoch 16/30, Batch 1150, Loss: 3.0168\n",
      "Epoch 16/30, Batch 1160, Loss: 3.0311\n",
      "Epoch 16/30, Batch 1170, Loss: 3.0745\n",
      "Epoch 16/30, Batch 1180, Loss: 2.9074\n",
      "finish  save model of epoch : 15!\n",
      "epoch using time 1539.965\n",
      "Epoch 17/30, Batch 10, Loss: 2.8033\n",
      "Epoch 17/30, Batch 20, Loss: 2.9168\n",
      "Epoch 17/30, Batch 30, Loss: 2.8879\n",
      "Epoch 17/30, Batch 40, Loss: 2.8414\n",
      "Epoch 17/30, Batch 50, Loss: 2.8520\n",
      "Epoch 17/30, Batch 60, Loss: 2.8764\n",
      "Epoch 17/30, Batch 70, Loss: 2.8686\n",
      "Epoch 17/30, Batch 80, Loss: 2.8807\n",
      "Epoch 17/30, Batch 90, Loss: 2.8103\n",
      "Epoch 17/30, Batch 100, Loss: 2.8725\n",
      "Epoch 17/30, Batch 110, Loss: 2.9114\n",
      "Epoch 17/30, Batch 120, Loss: 3.0101\n",
      "Epoch 17/30, Batch 130, Loss: 2.9514\n",
      "Epoch 17/30, Batch 140, Loss: 3.0546\n",
      "Epoch 17/30, Batch 150, Loss: 2.8759\n",
      "Epoch 17/30, Batch 160, Loss: 2.8818\n",
      "Epoch 17/30, Batch 170, Loss: 2.9021\n",
      "Epoch 17/30, Batch 180, Loss: 2.8899\n",
      "Epoch 17/30, Batch 190, Loss: 2.8262\n",
      "Epoch 17/30, Batch 200, Loss: 2.9475\n",
      "Epoch 17/30, Batch 210, Loss: 2.8759\n",
      "Epoch 17/30, Batch 220, Loss: 3.0199\n",
      "Epoch 17/30, Batch 230, Loss: 2.8499\n",
      "Epoch 17/30, Batch 240, Loss: 3.0080\n",
      "Epoch 17/30, Batch 250, Loss: 2.7772\n",
      "Epoch 17/30, Batch 260, Loss: 2.9640\n",
      "Epoch 17/30, Batch 270, Loss: 3.0115\n",
      "Epoch 17/30, Batch 280, Loss: 2.7957\n",
      "Epoch 17/30, Batch 290, Loss: 2.8038\n",
      "Epoch 17/30, Batch 300, Loss: 2.8819\n",
      "Epoch 17/30, Batch 310, Loss: 2.8384\n",
      "Epoch 17/30, Batch 320, Loss: 2.8123\n",
      "Epoch 17/30, Batch 330, Loss: 2.7651\n",
      "Epoch 17/30, Batch 340, Loss: 2.9372\n",
      "Epoch 17/30, Batch 350, Loss: 2.8840\n",
      "Epoch 17/30, Batch 360, Loss: 2.8442\n",
      "Epoch 17/30, Batch 370, Loss: 2.9817\n",
      "Epoch 17/30, Batch 380, Loss: 2.9448\n",
      "Epoch 17/30, Batch 390, Loss: 2.8014\n",
      "Epoch 17/30, Batch 400, Loss: 2.9265\n",
      "Epoch 17/30, Batch 410, Loss: 2.8935\n",
      "Epoch 17/30, Batch 420, Loss: 3.0053\n",
      "Epoch 17/30, Batch 430, Loss: 2.8553\n",
      "Epoch 17/30, Batch 440, Loss: 2.8912\n",
      "Epoch 17/30, Batch 450, Loss: 2.7924\n",
      "Epoch 17/30, Batch 460, Loss: 2.9393\n",
      "Epoch 17/30, Batch 470, Loss: 2.7463\n",
      "Epoch 17/30, Batch 480, Loss: 2.9763\n",
      "Epoch 17/30, Batch 490, Loss: 2.9025\n",
      "Epoch 17/30, Batch 500, Loss: 2.8689\n",
      "Epoch 17/30, Batch 510, Loss: 2.9932\n",
      "Epoch 17/30, Batch 520, Loss: 3.0361\n",
      "Epoch 17/30, Batch 530, Loss: 2.8763\n",
      "Epoch 17/30, Batch 540, Loss: 2.8169\n",
      "Epoch 17/30, Batch 550, Loss: 2.9242\n",
      "Epoch 17/30, Batch 560, Loss: 2.9665\n",
      "Epoch 17/30, Batch 570, Loss: 2.8541\n",
      "Epoch 17/30, Batch 580, Loss: 2.8811\n",
      "Epoch 17/30, Batch 590, Loss: 2.9008\n",
      "Epoch 17/30, Batch 600, Loss: 2.8057\n",
      "Epoch 17/30, Batch 610, Loss: 2.8207\n",
      "Epoch 17/30, Batch 620, Loss: 2.8840\n",
      "Epoch 17/30, Batch 630, Loss: 2.9850\n",
      "Epoch 17/30, Batch 640, Loss: 2.8422\n",
      "Epoch 17/30, Batch 650, Loss: 2.9602\n",
      "Epoch 17/30, Batch 660, Loss: 2.9184\n",
      "Epoch 17/30, Batch 670, Loss: 2.9917\n",
      "Epoch 17/30, Batch 680, Loss: 2.8901\n",
      "Epoch 17/30, Batch 690, Loss: 3.0006\n",
      "Epoch 17/30, Batch 700, Loss: 3.0349\n",
      "Epoch 17/30, Batch 710, Loss: 2.8009\n",
      "Epoch 17/30, Batch 720, Loss: 2.8625\n",
      "Epoch 17/30, Batch 730, Loss: 2.9580\n",
      "Epoch 17/30, Batch 740, Loss: 2.9436\n",
      "Epoch 17/30, Batch 750, Loss: 2.9283\n",
      "Epoch 17/30, Batch 760, Loss: 2.8175\n",
      "Epoch 17/30, Batch 770, Loss: 2.7945\n",
      "Epoch 17/30, Batch 780, Loss: 2.9386\n",
      "Epoch 17/30, Batch 790, Loss: 3.0285\n",
      "Epoch 17/30, Batch 800, Loss: 2.8494\n",
      "Epoch 17/30, Batch 810, Loss: 2.8574\n",
      "Epoch 17/30, Batch 820, Loss: 2.7218\n",
      "Epoch 17/30, Batch 830, Loss: 2.9043\n",
      "Epoch 17/30, Batch 840, Loss: 2.9312\n",
      "Epoch 17/30, Batch 850, Loss: 3.0404\n",
      "Epoch 17/30, Batch 860, Loss: 2.9028\n",
      "Epoch 17/30, Batch 870, Loss: 3.1509\n",
      "Epoch 17/30, Batch 880, Loss: 2.9246\n",
      "Epoch 17/30, Batch 890, Loss: 3.0306\n",
      "Epoch 17/30, Batch 900, Loss: 2.8455\n",
      "Epoch 17/30, Batch 910, Loss: 3.0448\n",
      "Epoch 17/30, Batch 920, Loss: 2.9518\n",
      "Epoch 17/30, Batch 930, Loss: 2.8778\n",
      "Epoch 17/30, Batch 940, Loss: 2.7369\n",
      "Epoch 17/30, Batch 950, Loss: 2.9785\n",
      "Epoch 17/30, Batch 960, Loss: 2.9607\n",
      "Epoch 17/30, Batch 970, Loss: 3.0290\n",
      "Epoch 17/30, Batch 980, Loss: 2.8974\n",
      "Epoch 17/30, Batch 990, Loss: 3.1000\n",
      "Epoch 17/30, Batch 1000, Loss: 2.8125\n",
      "Epoch 17/30, Batch 1010, Loss: 2.8893\n",
      "Epoch 17/30, Batch 1020, Loss: 3.0473\n",
      "Epoch 17/30, Batch 1030, Loss: 2.8565\n",
      "Epoch 17/30, Batch 1040, Loss: 2.9940\n",
      "Epoch 17/30, Batch 1050, Loss: 2.8516\n",
      "Epoch 17/30, Batch 1060, Loss: 2.9929\n",
      "Epoch 17/30, Batch 1070, Loss: 2.8999\n",
      "Epoch 17/30, Batch 1080, Loss: 2.9976\n",
      "Epoch 17/30, Batch 1090, Loss: 3.0327\n",
      "Epoch 17/30, Batch 1100, Loss: 2.8908\n",
      "Epoch 17/30, Batch 1110, Loss: 2.9484\n",
      "Epoch 17/30, Batch 1120, Loss: 3.0130\n",
      "Epoch 17/30, Batch 1130, Loss: 2.8383\n",
      "Epoch 17/30, Batch 1140, Loss: 2.9086\n",
      "Epoch 17/30, Batch 1150, Loss: 2.8269\n",
      "Epoch 17/30, Batch 1160, Loss: 3.0008\n",
      "Epoch 17/30, Batch 1170, Loss: 3.0045\n",
      "Epoch 17/30, Batch 1180, Loss: 2.8997\n",
      "finish  save model of epoch : 16!\n",
      "epoch using time 1708.389\n",
      "Epoch 18/30, Batch 10, Loss: 2.8207\n",
      "Epoch 18/30, Batch 20, Loss: 2.8364\n",
      "Epoch 18/30, Batch 30, Loss: 2.9591\n",
      "Epoch 18/30, Batch 40, Loss: 2.9513\n",
      "Epoch 18/30, Batch 50, Loss: 2.9663\n",
      "Epoch 18/30, Batch 60, Loss: 2.9578\n",
      "Epoch 18/30, Batch 70, Loss: 2.8575\n",
      "Epoch 18/30, Batch 80, Loss: 3.0027\n",
      "Epoch 18/30, Batch 90, Loss: 2.9058\n",
      "Epoch 18/30, Batch 100, Loss: 2.8443\n",
      "Epoch 18/30, Batch 110, Loss: 2.8147\n",
      "Epoch 18/30, Batch 120, Loss: 2.8915\n",
      "Epoch 18/30, Batch 130, Loss: 2.9383\n",
      "Epoch 18/30, Batch 140, Loss: 2.8831\n",
      "Epoch 18/30, Batch 150, Loss: 2.9060\n",
      "Epoch 18/30, Batch 160, Loss: 2.8482\n",
      "Epoch 18/30, Batch 170, Loss: 2.8250\n",
      "Epoch 18/30, Batch 180, Loss: 3.0090\n",
      "Epoch 18/30, Batch 190, Loss: 2.8631\n",
      "Epoch 18/30, Batch 200, Loss: 2.8878\n",
      "Epoch 18/30, Batch 210, Loss: 2.9282\n",
      "Epoch 18/30, Batch 220, Loss: 2.8223\n",
      "Epoch 18/30, Batch 230, Loss: 2.8365\n",
      "Epoch 18/30, Batch 240, Loss: 2.9157\n",
      "Epoch 18/30, Batch 250, Loss: 2.9300\n",
      "Epoch 18/30, Batch 260, Loss: 2.9835\n",
      "Epoch 18/30, Batch 270, Loss: 2.9518\n",
      "Epoch 18/30, Batch 280, Loss: 2.8296\n",
      "Epoch 18/30, Batch 290, Loss: 2.8289\n",
      "Epoch 18/30, Batch 300, Loss: 2.9202\n",
      "Epoch 18/30, Batch 310, Loss: 2.8737\n",
      "Epoch 18/30, Batch 320, Loss: 2.9323\n",
      "Epoch 18/30, Batch 330, Loss: 2.9305\n",
      "Epoch 18/30, Batch 340, Loss: 2.9080\n",
      "Epoch 18/30, Batch 350, Loss: 2.9196\n",
      "Epoch 18/30, Batch 360, Loss: 2.9233\n",
      "Epoch 18/30, Batch 370, Loss: 2.8840\n",
      "Epoch 18/30, Batch 380, Loss: 2.7885\n",
      "Epoch 18/30, Batch 390, Loss: 2.8388\n",
      "Epoch 18/30, Batch 400, Loss: 3.0029\n",
      "Epoch 18/30, Batch 410, Loss: 3.0903\n",
      "Epoch 18/30, Batch 420, Loss: 2.8843\n",
      "Epoch 18/30, Batch 430, Loss: 2.7508\n",
      "Epoch 18/30, Batch 440, Loss: 2.9416\n",
      "Epoch 18/30, Batch 450, Loss: 2.9669\n",
      "Epoch 18/30, Batch 460, Loss: 2.9171\n",
      "Epoch 18/30, Batch 470, Loss: 3.0328\n",
      "Epoch 18/30, Batch 480, Loss: 2.9954\n",
      "Epoch 18/30, Batch 490, Loss: 2.8688\n",
      "Epoch 18/30, Batch 500, Loss: 3.0459\n",
      "Epoch 18/30, Batch 510, Loss: 2.9416\n",
      "Epoch 18/30, Batch 520, Loss: 2.9392\n",
      "Epoch 18/30, Batch 530, Loss: 2.8916\n",
      "Epoch 18/30, Batch 540, Loss: 2.8882\n",
      "Epoch 18/30, Batch 550, Loss: 2.9206\n",
      "Epoch 18/30, Batch 560, Loss: 2.9563\n",
      "Epoch 18/30, Batch 570, Loss: 2.8578\n",
      "Epoch 18/30, Batch 580, Loss: 2.9340\n",
      "Epoch 18/30, Batch 590, Loss: 2.8782\n",
      "Epoch 18/30, Batch 600, Loss: 2.9834\n",
      "Epoch 18/30, Batch 610, Loss: 2.9741\n",
      "Epoch 18/30, Batch 620, Loss: 2.9457\n",
      "Epoch 18/30, Batch 630, Loss: 3.0744\n",
      "Epoch 18/30, Batch 640, Loss: 2.8498\n",
      "Epoch 18/30, Batch 650, Loss: 2.7589\n",
      "Epoch 18/30, Batch 660, Loss: 3.0129\n",
      "Epoch 18/30, Batch 670, Loss: 2.9473\n",
      "Epoch 18/30, Batch 680, Loss: 2.9220\n",
      "Epoch 18/30, Batch 690, Loss: 2.9398\n",
      "Epoch 18/30, Batch 700, Loss: 2.8150\n",
      "Epoch 18/30, Batch 710, Loss: 2.9839\n",
      "Epoch 18/30, Batch 720, Loss: 2.9513\n",
      "Epoch 18/30, Batch 730, Loss: 3.0128\n",
      "Epoch 18/30, Batch 740, Loss: 2.8821\n",
      "Epoch 18/30, Batch 750, Loss: 2.9197\n",
      "Epoch 18/30, Batch 760, Loss: 2.8763\n",
      "Epoch 18/30, Batch 770, Loss: 2.8863\n",
      "Epoch 18/30, Batch 780, Loss: 2.8371\n",
      "Epoch 18/30, Batch 790, Loss: 2.8911\n",
      "Epoch 18/30, Batch 800, Loss: 3.0217\n",
      "Epoch 18/30, Batch 810, Loss: 2.8085\n",
      "Epoch 18/30, Batch 820, Loss: 2.9021\n",
      "Epoch 18/30, Batch 830, Loss: 2.8784\n",
      "Epoch 18/30, Batch 840, Loss: 3.0232\n",
      "Epoch 18/30, Batch 850, Loss: 2.9026\n",
      "Epoch 18/30, Batch 860, Loss: 2.8501\n",
      "Epoch 18/30, Batch 870, Loss: 3.1231\n",
      "Epoch 18/30, Batch 880, Loss: 3.0376\n",
      "Epoch 18/30, Batch 890, Loss: 2.8958\n",
      "Epoch 18/30, Batch 900, Loss: 3.0279\n",
      "Epoch 18/30, Batch 910, Loss: 2.8701\n",
      "Epoch 18/30, Batch 920, Loss: 2.9022\n",
      "Epoch 18/30, Batch 930, Loss: 2.9029\n",
      "Epoch 18/30, Batch 940, Loss: 2.9454\n",
      "Epoch 18/30, Batch 950, Loss: 2.8920\n",
      "Epoch 18/30, Batch 960, Loss: 2.9295\n",
      "Epoch 18/30, Batch 970, Loss: 3.0480\n",
      "Epoch 18/30, Batch 980, Loss: 3.0303\n",
      "Epoch 18/30, Batch 990, Loss: 2.9443\n",
      "Epoch 18/30, Batch 1000, Loss: 2.9459\n",
      "Epoch 18/30, Batch 1010, Loss: 2.7897\n",
      "Epoch 18/30, Batch 1020, Loss: 2.9384\n",
      "Epoch 18/30, Batch 1030, Loss: 2.9700\n",
      "Epoch 18/30, Batch 1040, Loss: 2.9252\n",
      "Epoch 18/30, Batch 1050, Loss: 2.9591\n",
      "Epoch 18/30, Batch 1060, Loss: 3.0277\n",
      "Epoch 18/30, Batch 1070, Loss: 2.7763\n",
      "Epoch 18/30, Batch 1080, Loss: 2.9147\n",
      "Epoch 18/30, Batch 1090, Loss: 2.8370\n",
      "Epoch 18/30, Batch 1100, Loss: 2.9516\n",
      "Epoch 18/30, Batch 1110, Loss: 2.9456\n",
      "Epoch 18/30, Batch 1120, Loss: 3.0367\n",
      "Epoch 18/30, Batch 1130, Loss: 2.9990\n",
      "Epoch 18/30, Batch 1140, Loss: 2.9489\n",
      "Epoch 18/30, Batch 1150, Loss: 2.9337\n",
      "Epoch 18/30, Batch 1160, Loss: 2.8430\n",
      "Epoch 18/30, Batch 1170, Loss: 2.9290\n",
      "Epoch 18/30, Batch 1180, Loss: 3.0130\n",
      "finish  save model of epoch : 17!\n",
      "epoch using time 2060.596\n",
      "Epoch 19/30, Batch 10, Loss: 2.8021\n",
      "Epoch 19/30, Batch 20, Loss: 2.8740\n",
      "Epoch 19/30, Batch 30, Loss: 2.8671\n",
      "Epoch 19/30, Batch 40, Loss: 2.8050\n",
      "Epoch 19/30, Batch 50, Loss: 2.6786\n",
      "Epoch 19/30, Batch 60, Loss: 2.8185\n",
      "Epoch 19/30, Batch 70, Loss: 2.9639\n",
      "Epoch 19/30, Batch 80, Loss: 2.9490\n",
      "Epoch 19/30, Batch 90, Loss: 2.7670\n",
      "Epoch 19/30, Batch 100, Loss: 3.0073\n",
      "Epoch 19/30, Batch 110, Loss: 2.7072\n",
      "Epoch 19/30, Batch 120, Loss: 2.9079\n",
      "Epoch 19/30, Batch 130, Loss: 2.9046\n",
      "Epoch 19/30, Batch 140, Loss: 2.9207\n",
      "Epoch 19/30, Batch 150, Loss: 2.8244\n",
      "Epoch 19/30, Batch 160, Loss: 2.8678\n",
      "Epoch 19/30, Batch 170, Loss: 2.8976\n",
      "Epoch 19/30, Batch 180, Loss: 2.8437\n",
      "Epoch 19/30, Batch 190, Loss: 2.9366\n",
      "Epoch 19/30, Batch 200, Loss: 2.9266\n",
      "Epoch 19/30, Batch 210, Loss: 2.9158\n",
      "Epoch 19/30, Batch 220, Loss: 2.7780\n",
      "Epoch 19/30, Batch 230, Loss: 2.8918\n",
      "Epoch 19/30, Batch 240, Loss: 2.8599\n",
      "Epoch 19/30, Batch 250, Loss: 2.8177\n",
      "Epoch 19/30, Batch 260, Loss: 2.9150\n",
      "Epoch 19/30, Batch 270, Loss: 2.9595\n",
      "Epoch 19/30, Batch 280, Loss: 2.8874\n",
      "Epoch 19/30, Batch 290, Loss: 2.7882\n",
      "Epoch 19/30, Batch 300, Loss: 2.8982\n",
      "Epoch 19/30, Batch 310, Loss: 2.9326\n",
      "Epoch 19/30, Batch 320, Loss: 3.0336\n",
      "Epoch 19/30, Batch 330, Loss: 2.9346\n",
      "Epoch 19/30, Batch 340, Loss: 2.7974\n",
      "Epoch 19/30, Batch 350, Loss: 2.9634\n",
      "Epoch 19/30, Batch 360, Loss: 2.9240\n",
      "Epoch 19/30, Batch 370, Loss: 2.9427\n",
      "Epoch 19/30, Batch 380, Loss: 2.7773\n",
      "Epoch 19/30, Batch 390, Loss: 3.0434\n",
      "Epoch 19/30, Batch 400, Loss: 2.8905\n",
      "Epoch 19/30, Batch 410, Loss: 2.7753\n",
      "Epoch 19/30, Batch 420, Loss: 2.8537\n",
      "Epoch 19/30, Batch 430, Loss: 2.8894\n",
      "Epoch 19/30, Batch 440, Loss: 2.8761\n",
      "Epoch 19/30, Batch 450, Loss: 3.0394\n",
      "Epoch 19/30, Batch 460, Loss: 2.9401\n",
      "Epoch 19/30, Batch 470, Loss: 2.8610\n",
      "Epoch 19/30, Batch 480, Loss: 2.8630\n",
      "Epoch 19/30, Batch 490, Loss: 2.9617\n",
      "Epoch 19/30, Batch 500, Loss: 2.9396\n",
      "Epoch 19/30, Batch 510, Loss: 2.9065\n",
      "Epoch 19/30, Batch 520, Loss: 3.0108\n",
      "Epoch 19/30, Batch 530, Loss: 2.8902\n",
      "Epoch 19/30, Batch 540, Loss: 3.0828\n",
      "Epoch 19/30, Batch 550, Loss: 2.7769\n",
      "Epoch 19/30, Batch 560, Loss: 3.0164\n",
      "Epoch 19/30, Batch 570, Loss: 2.8689\n",
      "Epoch 19/30, Batch 580, Loss: 2.9351\n",
      "Epoch 19/30, Batch 590, Loss: 2.9209\n",
      "Epoch 19/30, Batch 600, Loss: 2.7859\n",
      "Epoch 19/30, Batch 610, Loss: 3.0185\n",
      "Epoch 19/30, Batch 620, Loss: 2.9214\n",
      "Epoch 19/30, Batch 630, Loss: 2.7913\n",
      "Epoch 19/30, Batch 640, Loss: 2.8641\n",
      "Epoch 19/30, Batch 650, Loss: 2.7731\n",
      "Epoch 19/30, Batch 660, Loss: 3.0322\n",
      "Epoch 19/30, Batch 670, Loss: 2.7758\n",
      "Epoch 19/30, Batch 680, Loss: 2.9556\n",
      "Epoch 19/30, Batch 690, Loss: 2.7404\n",
      "Epoch 19/30, Batch 700, Loss: 2.7871\n",
      "Epoch 19/30, Batch 710, Loss: 2.8700\n",
      "Epoch 19/30, Batch 720, Loss: 2.9140\n",
      "Epoch 19/30, Batch 730, Loss: 3.0209\n",
      "Epoch 19/30, Batch 740, Loss: 2.8449\n",
      "Epoch 19/30, Batch 750, Loss: 2.8868\n",
      "Epoch 19/30, Batch 760, Loss: 2.9095\n",
      "Epoch 19/30, Batch 770, Loss: 3.0542\n",
      "Epoch 19/30, Batch 780, Loss: 2.9769\n",
      "Epoch 19/30, Batch 790, Loss: 2.7677\n",
      "Epoch 19/30, Batch 800, Loss: 2.9935\n",
      "Epoch 19/30, Batch 810, Loss: 2.8765\n",
      "Epoch 19/30, Batch 820, Loss: 2.9396\n",
      "Epoch 19/30, Batch 830, Loss: 3.1175\n",
      "Epoch 19/30, Batch 840, Loss: 2.7727\n",
      "Epoch 19/30, Batch 850, Loss: 2.9208\n",
      "Epoch 19/30, Batch 860, Loss: 2.9479\n",
      "Epoch 19/30, Batch 870, Loss: 2.9849\n",
      "Epoch 19/30, Batch 880, Loss: 3.0274\n",
      "Epoch 19/30, Batch 890, Loss: 2.8276\n",
      "Epoch 19/30, Batch 900, Loss: 2.8423\n",
      "Epoch 19/30, Batch 910, Loss: 2.9004\n",
      "Epoch 19/30, Batch 920, Loss: 2.9183\n",
      "Epoch 19/30, Batch 930, Loss: 2.8477\n",
      "Epoch 19/30, Batch 940, Loss: 2.7890\n",
      "Epoch 19/30, Batch 950, Loss: 2.7815\n",
      "Epoch 19/30, Batch 960, Loss: 2.9081\n",
      "Epoch 19/30, Batch 970, Loss: 3.1068\n",
      "Epoch 19/30, Batch 980, Loss: 2.9410\n",
      "Epoch 19/30, Batch 990, Loss: 2.9474\n",
      "Epoch 19/30, Batch 1000, Loss: 2.9250\n",
      "Epoch 19/30, Batch 1010, Loss: 2.9846\n",
      "Epoch 19/30, Batch 1020, Loss: 2.9501\n",
      "Epoch 19/30, Batch 1030, Loss: 3.0103\n",
      "Epoch 19/30, Batch 1040, Loss: 2.9078\n",
      "Epoch 19/30, Batch 1050, Loss: 2.9361\n",
      "Epoch 19/30, Batch 1060, Loss: 2.9382\n",
      "Epoch 19/30, Batch 1070, Loss: 2.8961\n",
      "Epoch 19/30, Batch 1080, Loss: 2.9606\n",
      "Epoch 19/30, Batch 1090, Loss: 2.8470\n",
      "Epoch 19/30, Batch 1100, Loss: 2.9347\n",
      "Epoch 19/30, Batch 1110, Loss: 2.9405\n",
      "Epoch 19/30, Batch 1120, Loss: 2.9174\n",
      "Epoch 19/30, Batch 1130, Loss: 2.8799\n",
      "Epoch 19/30, Batch 1140, Loss: 2.8734\n",
      "Epoch 19/30, Batch 1150, Loss: 2.9624\n",
      "Epoch 19/30, Batch 1160, Loss: 2.7787\n",
      "Epoch 19/30, Batch 1170, Loss: 2.8523\n",
      "Epoch 19/30, Batch 1180, Loss: 2.8785\n",
      "finish  save model of epoch : 18!\n",
      "epoch using time 1960.937\n",
      "Epoch 20/30, Batch 10, Loss: 2.9412\n",
      "Epoch 20/30, Batch 20, Loss: 2.8831\n",
      "Epoch 20/30, Batch 30, Loss: 3.0093\n",
      "Epoch 20/30, Batch 40, Loss: 2.7121\n",
      "Epoch 20/30, Batch 50, Loss: 2.8985\n",
      "Epoch 20/30, Batch 60, Loss: 2.8299\n",
      "Epoch 20/30, Batch 70, Loss: 2.9326\n",
      "Epoch 20/30, Batch 80, Loss: 2.8273\n",
      "Epoch 20/30, Batch 90, Loss: 2.8297\n",
      "Epoch 20/30, Batch 100, Loss: 2.8978\n",
      "Epoch 20/30, Batch 110, Loss: 2.9327\n",
      "Epoch 20/30, Batch 120, Loss: 2.8130\n",
      "Epoch 20/30, Batch 130, Loss: 2.7093\n",
      "Epoch 20/30, Batch 140, Loss: 2.8666\n",
      "Epoch 20/30, Batch 150, Loss: 2.8708\n",
      "Epoch 20/30, Batch 160, Loss: 2.8326\n",
      "Epoch 20/30, Batch 170, Loss: 2.8817\n",
      "Epoch 20/30, Batch 180, Loss: 2.9128\n",
      "Epoch 20/30, Batch 190, Loss: 2.7532\n",
      "Epoch 20/30, Batch 200, Loss: 2.8702\n",
      "Epoch 20/30, Batch 210, Loss: 2.9227\n",
      "Epoch 20/30, Batch 220, Loss: 2.8596\n",
      "Epoch 20/30, Batch 230, Loss: 3.0053\n",
      "Epoch 20/30, Batch 240, Loss: 2.9112\n",
      "Epoch 20/30, Batch 250, Loss: 2.9998\n",
      "Epoch 20/30, Batch 260, Loss: 2.9950\n",
      "Epoch 20/30, Batch 270, Loss: 2.9370\n",
      "Epoch 20/30, Batch 280, Loss: 2.7348\n",
      "Epoch 20/30, Batch 290, Loss: 2.9641\n",
      "Epoch 20/30, Batch 300, Loss: 2.9016\n",
      "Epoch 20/30, Batch 310, Loss: 2.8177\n",
      "Epoch 20/30, Batch 320, Loss: 2.8201\n",
      "Epoch 20/30, Batch 330, Loss: 2.9148\n",
      "Epoch 20/30, Batch 340, Loss: 2.8624\n",
      "Epoch 20/30, Batch 350, Loss: 2.6242\n",
      "Epoch 20/30, Batch 360, Loss: 2.7741\n",
      "Epoch 20/30, Batch 370, Loss: 2.9872\n",
      "Epoch 20/30, Batch 380, Loss: 2.9655\n",
      "Epoch 20/30, Batch 390, Loss: 2.9077\n",
      "Epoch 20/30, Batch 400, Loss: 2.7111\n",
      "Epoch 20/30, Batch 410, Loss: 2.8348\n",
      "Epoch 20/30, Batch 420, Loss: 2.8884\n",
      "Epoch 20/30, Batch 430, Loss: 2.8504\n",
      "Epoch 20/30, Batch 440, Loss: 2.8831\n",
      "Epoch 20/30, Batch 450, Loss: 3.1101\n",
      "Epoch 20/30, Batch 460, Loss: 2.7815\n",
      "Epoch 20/30, Batch 470, Loss: 2.8032\n",
      "Epoch 20/30, Batch 480, Loss: 2.8899\n",
      "Epoch 20/30, Batch 490, Loss: 2.8913\n",
      "Epoch 20/30, Batch 500, Loss: 2.9040\n",
      "Epoch 20/30, Batch 510, Loss: 2.8931\n",
      "Epoch 20/30, Batch 520, Loss: 2.9256\n",
      "Epoch 20/30, Batch 530, Loss: 2.8529\n",
      "Epoch 20/30, Batch 540, Loss: 2.9590\n",
      "Epoch 20/30, Batch 550, Loss: 2.9277\n",
      "Epoch 20/30, Batch 560, Loss: 3.0134\n",
      "Epoch 20/30, Batch 570, Loss: 2.8012\n",
      "Epoch 20/30, Batch 580, Loss: 2.8720\n",
      "Epoch 20/30, Batch 590, Loss: 2.8468\n",
      "Epoch 20/30, Batch 600, Loss: 2.8489\n",
      "Epoch 20/30, Batch 610, Loss: 2.8287\n",
      "Epoch 20/30, Batch 620, Loss: 2.8902\n",
      "Epoch 20/30, Batch 630, Loss: 2.9170\n",
      "Epoch 20/30, Batch 640, Loss: 2.8682\n",
      "Epoch 20/30, Batch 650, Loss: 3.0488\n",
      "Epoch 20/30, Batch 660, Loss: 2.8553\n",
      "Epoch 20/30, Batch 670, Loss: 2.8083\n",
      "Epoch 20/30, Batch 680, Loss: 2.9042\n",
      "Epoch 20/30, Batch 690, Loss: 2.9880\n",
      "Epoch 20/30, Batch 700, Loss: 2.8742\n",
      "Epoch 20/30, Batch 710, Loss: 2.9331\n",
      "Epoch 20/30, Batch 720, Loss: 3.0272\n",
      "Epoch 20/30, Batch 730, Loss: 2.8626\n",
      "Epoch 20/30, Batch 740, Loss: 2.7872\n",
      "Epoch 20/30, Batch 750, Loss: 2.8416\n",
      "Epoch 20/30, Batch 760, Loss: 2.9240\n",
      "Epoch 20/30, Batch 770, Loss: 2.9566\n",
      "Epoch 20/30, Batch 780, Loss: 2.9288\n",
      "Epoch 20/30, Batch 790, Loss: 2.9217\n",
      "Epoch 20/30, Batch 800, Loss: 2.8570\n",
      "Epoch 20/30, Batch 810, Loss: 2.8844\n",
      "Epoch 20/30, Batch 820, Loss: 2.9838\n",
      "Epoch 20/30, Batch 830, Loss: 2.7776\n",
      "Epoch 20/30, Batch 840, Loss: 2.8954\n",
      "Epoch 20/30, Batch 850, Loss: 2.8399\n",
      "Epoch 20/30, Batch 860, Loss: 3.0447\n",
      "Epoch 20/30, Batch 870, Loss: 2.8527\n",
      "Epoch 20/30, Batch 880, Loss: 2.9622\n",
      "Epoch 20/30, Batch 890, Loss: 2.9413\n",
      "Epoch 20/30, Batch 900, Loss: 2.9603\n",
      "Epoch 20/30, Batch 910, Loss: 2.8216\n",
      "Epoch 20/30, Batch 920, Loss: 2.9817\n",
      "Epoch 20/30, Batch 930, Loss: 2.7965\n",
      "Epoch 20/30, Batch 940, Loss: 2.9736\n",
      "Epoch 20/30, Batch 950, Loss: 2.8875\n",
      "Epoch 20/30, Batch 960, Loss: 2.8835\n",
      "Epoch 20/30, Batch 970, Loss: 2.8876\n",
      "Epoch 20/30, Batch 980, Loss: 3.1372\n",
      "Epoch 20/30, Batch 990, Loss: 2.9257\n",
      "Epoch 20/30, Batch 1000, Loss: 2.9980\n",
      "Epoch 20/30, Batch 1010, Loss: 2.8767\n",
      "Epoch 20/30, Batch 1020, Loss: 2.9350\n",
      "Epoch 20/30, Batch 1030, Loss: 2.9843\n",
      "Epoch 20/30, Batch 1040, Loss: 2.9215\n",
      "Epoch 20/30, Batch 1050, Loss: 2.9524\n",
      "Epoch 20/30, Batch 1060, Loss: 2.8971\n",
      "Epoch 20/30, Batch 1070, Loss: 2.9838\n",
      "Epoch 20/30, Batch 1080, Loss: 2.7732\n",
      "Epoch 20/30, Batch 1090, Loss: 2.9263\n",
      "Epoch 20/30, Batch 1100, Loss: 2.8610\n",
      "Epoch 20/30, Batch 1110, Loss: 2.8559\n",
      "Epoch 20/30, Batch 1120, Loss: 2.8377\n",
      "Epoch 20/30, Batch 1130, Loss: 2.9106\n",
      "Epoch 20/30, Batch 1140, Loss: 2.9082\n",
      "Epoch 20/30, Batch 1150, Loss: 2.8109\n",
      "Epoch 20/30, Batch 1160, Loss: 2.8787\n",
      "Epoch 20/30, Batch 1170, Loss: 2.8930\n",
      "Epoch 20/30, Batch 1180, Loss: 2.9803\n",
      "finish  save model of epoch : 19!\n",
      "epoch using time 5504.717\n",
      "Epoch 21/30, Batch 10, Loss: 2.9114\n",
      "Epoch 21/30, Batch 20, Loss: 2.8204\n",
      "Epoch 21/30, Batch 30, Loss: 2.6681\n",
      "Epoch 21/30, Batch 40, Loss: 2.9804\n",
      "Epoch 21/30, Batch 50, Loss: 2.7319\n",
      "Epoch 21/30, Batch 60, Loss: 2.8616\n",
      "Epoch 21/30, Batch 70, Loss: 2.8774\n",
      "Epoch 21/30, Batch 80, Loss: 2.9435\n",
      "Epoch 21/30, Batch 90, Loss: 2.6969\n",
      "Epoch 21/30, Batch 100, Loss: 2.8533\n",
      "Epoch 21/30, Batch 110, Loss: 2.6619\n",
      "Epoch 21/30, Batch 120, Loss: 2.6751\n",
      "Epoch 21/30, Batch 130, Loss: 2.8966\n",
      "Epoch 21/30, Batch 140, Loss: 2.9441\n",
      "Epoch 21/30, Batch 150, Loss: 2.8019\n",
      "Epoch 21/30, Batch 160, Loss: 2.8766\n",
      "Epoch 21/30, Batch 170, Loss: 2.8689\n",
      "Epoch 21/30, Batch 180, Loss: 2.8483\n",
      "Epoch 21/30, Batch 190, Loss: 2.8513\n",
      "Epoch 21/30, Batch 200, Loss: 2.9017\n",
      "Epoch 21/30, Batch 210, Loss: 2.9128\n",
      "Epoch 21/30, Batch 220, Loss: 2.7361\n",
      "Epoch 21/30, Batch 230, Loss: 2.7618\n",
      "Epoch 21/30, Batch 240, Loss: 2.7714\n",
      "Epoch 21/30, Batch 250, Loss: 2.8944\n",
      "Epoch 21/30, Batch 260, Loss: 2.9989\n",
      "Epoch 21/30, Batch 270, Loss: 2.8533\n",
      "Epoch 21/30, Batch 280, Loss: 2.8797\n",
      "Epoch 21/30, Batch 290, Loss: 2.9295\n",
      "Epoch 21/30, Batch 300, Loss: 2.9140\n",
      "Epoch 21/30, Batch 310, Loss: 3.0833\n",
      "Epoch 21/30, Batch 320, Loss: 2.7964\n",
      "Epoch 21/30, Batch 330, Loss: 2.7881\n",
      "Epoch 21/30, Batch 340, Loss: 2.9987\n",
      "Epoch 21/30, Batch 350, Loss: 2.9177\n",
      "Epoch 21/30, Batch 360, Loss: 2.8763\n",
      "Epoch 21/30, Batch 370, Loss: 2.9380\n",
      "Epoch 21/30, Batch 380, Loss: 3.0092\n",
      "Epoch 21/30, Batch 390, Loss: 2.9703\n",
      "Epoch 21/30, Batch 400, Loss: 2.7509\n",
      "Epoch 21/30, Batch 410, Loss: 2.8584\n",
      "Epoch 21/30, Batch 420, Loss: 2.8646\n",
      "Epoch 21/30, Batch 430, Loss: 2.9732\n",
      "Epoch 21/30, Batch 440, Loss: 2.8426\n",
      "Epoch 21/30, Batch 450, Loss: 2.9470\n",
      "Epoch 21/30, Batch 460, Loss: 2.9895\n",
      "Epoch 21/30, Batch 470, Loss: 2.8542\n",
      "Epoch 21/30, Batch 480, Loss: 2.9284\n",
      "Epoch 21/30, Batch 490, Loss: 2.8924\n",
      "Epoch 21/30, Batch 500, Loss: 2.8840\n",
      "Epoch 21/30, Batch 510, Loss: 2.8277\n",
      "Epoch 21/30, Batch 520, Loss: 2.7205\n",
      "Epoch 21/30, Batch 530, Loss: 2.8598\n",
      "Epoch 21/30, Batch 540, Loss: 2.8767\n",
      "Epoch 21/30, Batch 550, Loss: 2.8596\n",
      "Epoch 21/30, Batch 560, Loss: 2.8472\n",
      "Epoch 21/30, Batch 570, Loss: 2.6946\n",
      "Epoch 21/30, Batch 580, Loss: 2.9696\n",
      "Epoch 21/30, Batch 590, Loss: 3.0434\n",
      "Epoch 21/30, Batch 600, Loss: 2.9512\n",
      "Epoch 21/30, Batch 610, Loss: 2.7743\n",
      "Epoch 21/30, Batch 620, Loss: 2.8490\n",
      "Epoch 21/30, Batch 630, Loss: 2.8655\n",
      "Epoch 21/30, Batch 640, Loss: 2.9096\n",
      "Epoch 21/30, Batch 650, Loss: 2.7995\n",
      "Epoch 21/30, Batch 660, Loss: 2.8507\n",
      "Epoch 21/30, Batch 670, Loss: 2.8582\n",
      "Epoch 21/30, Batch 680, Loss: 2.8520\n",
      "Epoch 21/30, Batch 690, Loss: 2.8259\n",
      "Epoch 21/30, Batch 700, Loss: 2.8832\n",
      "Epoch 21/30, Batch 710, Loss: 2.8071\n",
      "Epoch 21/30, Batch 720, Loss: 2.7669\n",
      "Epoch 21/30, Batch 730, Loss: 2.8092\n",
      "Epoch 21/30, Batch 740, Loss: 2.8681\n",
      "Epoch 21/30, Batch 750, Loss: 2.9978\n",
      "Epoch 21/30, Batch 760, Loss: 2.7981\n",
      "Epoch 21/30, Batch 770, Loss: 2.8274\n",
      "Epoch 21/30, Batch 780, Loss: 2.9272\n",
      "Epoch 21/30, Batch 790, Loss: 2.9478\n",
      "Epoch 21/30, Batch 800, Loss: 2.9193\n",
      "Epoch 21/30, Batch 810, Loss: 2.8143\n",
      "Epoch 21/30, Batch 820, Loss: 2.9059\n",
      "Epoch 21/30, Batch 830, Loss: 2.9077\n",
      "Epoch 21/30, Batch 840, Loss: 2.9283\n",
      "Epoch 21/30, Batch 850, Loss: 2.9729\n",
      "Epoch 21/30, Batch 860, Loss: 2.9056\n",
      "Epoch 21/30, Batch 870, Loss: 2.7140\n",
      "Epoch 21/30, Batch 880, Loss: 2.8936\n",
      "Epoch 21/30, Batch 890, Loss: 2.9871\n",
      "Epoch 21/30, Batch 900, Loss: 2.9098\n",
      "Epoch 21/30, Batch 910, Loss: 2.9953\n",
      "Epoch 21/30, Batch 920, Loss: 2.8931\n",
      "Epoch 21/30, Batch 930, Loss: 2.9503\n",
      "Epoch 21/30, Batch 940, Loss: 2.7509\n",
      "Epoch 21/30, Batch 950, Loss: 3.0313\n",
      "Epoch 21/30, Batch 960, Loss: 2.9687\n",
      "Epoch 21/30, Batch 970, Loss: 2.8803\n",
      "Epoch 21/30, Batch 980, Loss: 2.8051\n",
      "Epoch 21/30, Batch 990, Loss: 2.8852\n",
      "Epoch 21/30, Batch 1000, Loss: 2.9174\n",
      "Epoch 21/30, Batch 1010, Loss: 3.0762\n",
      "Epoch 21/30, Batch 1020, Loss: 2.9541\n",
      "Epoch 21/30, Batch 1030, Loss: 2.9585\n",
      "Epoch 21/30, Batch 1040, Loss: 3.0157\n",
      "Epoch 21/30, Batch 1050, Loss: 2.8449\n",
      "Epoch 21/30, Batch 1060, Loss: 2.8167\n",
      "Epoch 21/30, Batch 1070, Loss: 2.9045\n",
      "Epoch 21/30, Batch 1080, Loss: 2.8724\n",
      "Epoch 21/30, Batch 1090, Loss: 2.9036\n",
      "Epoch 21/30, Batch 1100, Loss: 2.9300\n",
      "Epoch 21/30, Batch 1110, Loss: 2.8051\n",
      "Epoch 21/30, Batch 1120, Loss: 3.0032\n",
      "Epoch 21/30, Batch 1130, Loss: 2.7693\n",
      "Epoch 21/30, Batch 1140, Loss: 2.8719\n",
      "Epoch 21/30, Batch 1150, Loss: 3.0033\n",
      "Epoch 21/30, Batch 1160, Loss: 2.9959\n",
      "Epoch 21/30, Batch 1170, Loss: 2.8501\n",
      "Epoch 21/30, Batch 1180, Loss: 2.8431\n",
      "finish  save model of epoch : 20!\n",
      "epoch using time 1897.762\n",
      "Epoch 22/30, Batch 10, Loss: 2.7910\n",
      "Epoch 22/30, Batch 20, Loss: 2.7820\n",
      "Epoch 22/30, Batch 30, Loss: 2.8859\n",
      "Epoch 22/30, Batch 40, Loss: 2.9080\n",
      "Epoch 22/30, Batch 50, Loss: 2.8939\n",
      "Epoch 22/30, Batch 60, Loss: 2.8091\n",
      "Epoch 22/30, Batch 70, Loss: 2.7872\n",
      "Epoch 22/30, Batch 80, Loss: 3.0218\n",
      "Epoch 22/30, Batch 90, Loss: 2.7623\n",
      "Epoch 22/30, Batch 100, Loss: 2.7879\n",
      "Epoch 22/30, Batch 110, Loss: 2.8279\n",
      "Epoch 22/30, Batch 120, Loss: 2.8284\n",
      "Epoch 22/30, Batch 130, Loss: 2.7108\n",
      "Epoch 22/30, Batch 140, Loss: 2.9240\n",
      "Epoch 22/30, Batch 150, Loss: 2.8734\n",
      "Epoch 22/30, Batch 160, Loss: 2.9736\n",
      "Epoch 22/30, Batch 170, Loss: 2.9118\n",
      "Epoch 22/30, Batch 180, Loss: 2.7999\n",
      "Epoch 22/30, Batch 190, Loss: 2.9749\n",
      "Epoch 22/30, Batch 200, Loss: 2.7601\n",
      "Epoch 22/30, Batch 210, Loss: 2.8870\n",
      "Epoch 22/30, Batch 220, Loss: 2.8503\n",
      "Epoch 22/30, Batch 230, Loss: 2.8446\n",
      "Epoch 22/30, Batch 240, Loss: 2.8022\n",
      "Epoch 22/30, Batch 250, Loss: 2.8921\n",
      "Epoch 22/30, Batch 260, Loss: 2.7632\n",
      "Epoch 22/30, Batch 270, Loss: 2.7814\n",
      "Epoch 22/30, Batch 280, Loss: 2.7752\n",
      "Epoch 22/30, Batch 290, Loss: 2.8468\n",
      "Epoch 22/30, Batch 300, Loss: 2.7843\n",
      "Epoch 22/30, Batch 310, Loss: 2.8442\n",
      "Epoch 22/30, Batch 320, Loss: 2.7109\n",
      "Epoch 22/30, Batch 330, Loss: 2.8536\n",
      "Epoch 22/30, Batch 340, Loss: 2.8315\n",
      "Epoch 22/30, Batch 350, Loss: 2.9192\n",
      "Epoch 22/30, Batch 360, Loss: 2.9301\n",
      "Epoch 22/30, Batch 370, Loss: 2.8431\n",
      "Epoch 22/30, Batch 380, Loss: 2.8212\n",
      "Epoch 22/30, Batch 390, Loss: 2.8862\n",
      "Epoch 22/30, Batch 400, Loss: 2.7520\n",
      "Epoch 22/30, Batch 410, Loss: 2.9602\n",
      "Epoch 22/30, Batch 420, Loss: 2.8715\n",
      "Epoch 22/30, Batch 430, Loss: 2.9092\n",
      "Epoch 22/30, Batch 440, Loss: 2.8964\n",
      "Epoch 22/30, Batch 450, Loss: 2.9588\n",
      "Epoch 22/30, Batch 460, Loss: 2.7774\n",
      "Epoch 22/30, Batch 470, Loss: 2.8238\n",
      "Epoch 22/30, Batch 480, Loss: 2.8497\n",
      "Epoch 22/30, Batch 490, Loss: 2.9308\n",
      "Epoch 22/30, Batch 500, Loss: 2.8657\n",
      "Epoch 22/30, Batch 510, Loss: 2.8923\n",
      "Epoch 22/30, Batch 520, Loss: 2.9468\n",
      "Epoch 22/30, Batch 530, Loss: 2.7944\n",
      "Epoch 22/30, Batch 540, Loss: 2.8923\n",
      "Epoch 22/30, Batch 550, Loss: 2.8775\n",
      "Epoch 22/30, Batch 560, Loss: 2.8493\n",
      "Epoch 22/30, Batch 570, Loss: 2.8974\n",
      "Epoch 22/30, Batch 580, Loss: 2.9111\n",
      "Epoch 22/30, Batch 590, Loss: 2.9632\n",
      "Epoch 22/30, Batch 600, Loss: 2.8826\n",
      "Epoch 22/30, Batch 610, Loss: 2.8274\n",
      "Epoch 22/30, Batch 620, Loss: 2.7456\n",
      "Epoch 22/30, Batch 630, Loss: 2.9340\n",
      "Epoch 22/30, Batch 640, Loss: 2.9179\n",
      "Epoch 22/30, Batch 650, Loss: 2.9344\n",
      "Epoch 22/30, Batch 660, Loss: 2.9191\n",
      "Epoch 22/30, Batch 670, Loss: 2.9017\n",
      "Epoch 22/30, Batch 680, Loss: 2.8330\n",
      "Epoch 22/30, Batch 690, Loss: 2.8554\n",
      "Epoch 22/30, Batch 700, Loss: 2.8551\n",
      "Epoch 22/30, Batch 710, Loss: 3.0423\n",
      "Epoch 22/30, Batch 720, Loss: 2.9708\n",
      "Epoch 22/30, Batch 730, Loss: 2.9221\n",
      "Epoch 22/30, Batch 740, Loss: 2.8612\n",
      "Epoch 22/30, Batch 750, Loss: 2.8220\n",
      "Epoch 22/30, Batch 760, Loss: 2.9390\n",
      "Epoch 22/30, Batch 770, Loss: 2.9470\n",
      "Epoch 22/30, Batch 780, Loss: 2.7954\n",
      "Epoch 22/30, Batch 790, Loss: 2.9494\n",
      "Epoch 22/30, Batch 800, Loss: 2.8005\n",
      "Epoch 22/30, Batch 810, Loss: 2.8693\n",
      "Epoch 22/30, Batch 820, Loss: 2.9417\n",
      "Epoch 22/30, Batch 830, Loss: 2.8521\n",
      "Epoch 22/30, Batch 840, Loss: 2.9183\n",
      "Epoch 22/30, Batch 850, Loss: 2.8458\n",
      "Epoch 22/30, Batch 860, Loss: 2.8197\n",
      "Epoch 22/30, Batch 870, Loss: 2.8708\n",
      "Epoch 22/30, Batch 880, Loss: 2.9443\n",
      "Epoch 22/30, Batch 890, Loss: 2.9740\n",
      "Epoch 22/30, Batch 900, Loss: 2.9402\n",
      "Epoch 22/30, Batch 910, Loss: 2.8278\n",
      "Epoch 22/30, Batch 920, Loss: 2.7933\n",
      "Epoch 22/30, Batch 930, Loss: 3.0041\n",
      "Epoch 22/30, Batch 940, Loss: 2.9741\n",
      "Epoch 22/30, Batch 950, Loss: 2.9485\n",
      "Epoch 22/30, Batch 960, Loss: 2.8479\n",
      "Epoch 22/30, Batch 970, Loss: 2.8148\n",
      "Epoch 22/30, Batch 980, Loss: 2.8913\n",
      "Epoch 22/30, Batch 990, Loss: 2.9859\n",
      "Epoch 22/30, Batch 1000, Loss: 3.0126\n",
      "Epoch 22/30, Batch 1010, Loss: 2.8775\n",
      "Epoch 22/30, Batch 1020, Loss: 2.8337\n",
      "Epoch 22/30, Batch 1030, Loss: 2.8295\n",
      "Epoch 22/30, Batch 1040, Loss: 2.8392\n",
      "Epoch 22/30, Batch 1050, Loss: 3.0006\n",
      "Epoch 22/30, Batch 1060, Loss: 3.0021\n",
      "Epoch 22/30, Batch 1070, Loss: 2.9235\n",
      "Epoch 22/30, Batch 1080, Loss: 2.8647\n",
      "Epoch 22/30, Batch 1090, Loss: 3.0924\n",
      "Epoch 22/30, Batch 1100, Loss: 2.9657\n",
      "Epoch 22/30, Batch 1110, Loss: 2.7074\n",
      "Epoch 22/30, Batch 1120, Loss: 2.9421\n",
      "Epoch 22/30, Batch 1130, Loss: 2.8686\n",
      "Epoch 22/30, Batch 1140, Loss: 2.8958\n",
      "Epoch 22/30, Batch 1150, Loss: 2.8762\n",
      "Epoch 22/30, Batch 1160, Loss: 2.8232\n",
      "Epoch 22/30, Batch 1170, Loss: 2.9961\n",
      "Epoch 22/30, Batch 1180, Loss: 2.9577\n",
      "finish  save model of epoch : 21!\n",
      "epoch using time 2025.384\n",
      "Epoch 23/30, Batch 10, Loss: 2.8476\n",
      "Epoch 23/30, Batch 20, Loss: 2.8042\n",
      "Epoch 23/30, Batch 30, Loss: 2.7524\n",
      "Epoch 23/30, Batch 40, Loss: 2.8430\n",
      "Epoch 23/30, Batch 50, Loss: 2.8532\n",
      "Epoch 23/30, Batch 60, Loss: 2.7003\n",
      "Epoch 23/30, Batch 70, Loss: 3.0561\n",
      "Epoch 23/30, Batch 80, Loss: 2.8556\n",
      "Epoch 23/30, Batch 90, Loss: 2.9571\n",
      "Epoch 23/30, Batch 100, Loss: 2.8722\n",
      "Epoch 23/30, Batch 110, Loss: 2.6466\n",
      "Epoch 23/30, Batch 120, Loss: 2.8371\n",
      "Epoch 23/30, Batch 130, Loss: 2.8326\n",
      "Epoch 23/30, Batch 140, Loss: 2.8673\n",
      "Epoch 23/30, Batch 150, Loss: 2.8603\n",
      "Epoch 23/30, Batch 160, Loss: 2.9194\n",
      "Epoch 23/30, Batch 170, Loss: 2.7983\n",
      "Epoch 23/30, Batch 180, Loss: 2.9211\n",
      "Epoch 23/30, Batch 190, Loss: 2.8095\n",
      "Epoch 23/30, Batch 200, Loss: 2.7686\n",
      "Epoch 23/30, Batch 210, Loss: 2.9448\n",
      "Epoch 23/30, Batch 220, Loss: 2.8490\n",
      "Epoch 23/30, Batch 230, Loss: 2.7863\n",
      "Epoch 23/30, Batch 240, Loss: 2.8120\n",
      "Epoch 23/30, Batch 250, Loss: 2.7907\n",
      "Epoch 23/30, Batch 260, Loss: 2.8229\n",
      "Epoch 23/30, Batch 270, Loss: 2.8111\n",
      "Epoch 23/30, Batch 280, Loss: 2.9698\n",
      "Epoch 23/30, Batch 290, Loss: 2.8291\n",
      "Epoch 23/30, Batch 300, Loss: 2.8871\n",
      "Epoch 23/30, Batch 310, Loss: 2.8312\n",
      "Epoch 23/30, Batch 320, Loss: 3.0615\n",
      "Epoch 23/30, Batch 330, Loss: 2.9765\n",
      "Epoch 23/30, Batch 340, Loss: 2.7220\n",
      "Epoch 23/30, Batch 350, Loss: 2.7848\n",
      "Epoch 23/30, Batch 360, Loss: 2.9135\n",
      "Epoch 23/30, Batch 370, Loss: 2.7821\n",
      "Epoch 23/30, Batch 380, Loss: 2.8350\n",
      "Epoch 23/30, Batch 390, Loss: 2.9615\n",
      "Epoch 23/30, Batch 400, Loss: 2.9111\n",
      "Epoch 23/30, Batch 410, Loss: 2.9534\n",
      "Epoch 23/30, Batch 420, Loss: 2.8126\n",
      "Epoch 23/30, Batch 430, Loss: 2.7497\n",
      "Epoch 23/30, Batch 440, Loss: 2.8451\n",
      "Epoch 23/30, Batch 450, Loss: 2.8527\n",
      "Epoch 23/30, Batch 460, Loss: 2.8944\n",
      "Epoch 23/30, Batch 470, Loss: 2.9001\n",
      "Epoch 23/30, Batch 480, Loss: 2.8570\n",
      "Epoch 23/30, Batch 490, Loss: 2.9848\n",
      "Epoch 23/30, Batch 500, Loss: 3.0318\n",
      "Epoch 23/30, Batch 510, Loss: 2.9184\n",
      "Epoch 23/30, Batch 520, Loss: 2.9041\n",
      "Epoch 23/30, Batch 530, Loss: 2.8772\n",
      "Epoch 23/30, Batch 540, Loss: 2.9025\n",
      "Epoch 23/30, Batch 550, Loss: 2.9599\n",
      "Epoch 23/30, Batch 560, Loss: 2.9061\n",
      "Epoch 23/30, Batch 570, Loss: 2.8403\n",
      "Epoch 23/30, Batch 580, Loss: 2.8689\n",
      "Epoch 23/30, Batch 590, Loss: 2.8136\n",
      "Epoch 23/30, Batch 600, Loss: 2.9650\n",
      "Epoch 23/30, Batch 610, Loss: 2.7553\n",
      "Epoch 23/30, Batch 620, Loss: 2.9150\n",
      "Epoch 23/30, Batch 630, Loss: 2.8862\n",
      "Epoch 23/30, Batch 640, Loss: 2.9968\n",
      "Epoch 23/30, Batch 650, Loss: 2.9122\n",
      "Epoch 23/30, Batch 660, Loss: 2.9175\n",
      "Epoch 23/30, Batch 670, Loss: 2.9499\n",
      "Epoch 23/30, Batch 680, Loss: 2.8587\n",
      "Epoch 23/30, Batch 690, Loss: 2.8096\n",
      "Epoch 23/30, Batch 700, Loss: 2.8130\n",
      "Epoch 23/30, Batch 710, Loss: 2.8247\n",
      "Epoch 23/30, Batch 720, Loss: 2.9469\n",
      "Epoch 23/30, Batch 730, Loss: 2.7331\n",
      "Epoch 23/30, Batch 740, Loss: 2.8996\n",
      "Epoch 23/30, Batch 750, Loss: 2.8118\n",
      "Epoch 23/30, Batch 760, Loss: 2.9935\n",
      "Epoch 23/30, Batch 770, Loss: 2.8144\n",
      "Epoch 23/30, Batch 780, Loss: 2.8938\n",
      "Epoch 23/30, Batch 790, Loss: 2.8879\n",
      "Epoch 23/30, Batch 800, Loss: 3.0411\n",
      "Epoch 23/30, Batch 810, Loss: 2.8665\n",
      "Epoch 23/30, Batch 820, Loss: 2.7914\n",
      "Epoch 23/30, Batch 830, Loss: 2.9002\n",
      "Epoch 23/30, Batch 840, Loss: 2.8953\n",
      "Epoch 23/30, Batch 850, Loss: 2.8394\n",
      "Epoch 23/30, Batch 860, Loss: 2.9619\n",
      "Epoch 23/30, Batch 870, Loss: 2.7990\n",
      "Epoch 23/30, Batch 880, Loss: 2.9957\n",
      "Epoch 23/30, Batch 890, Loss: 2.9087\n",
      "Epoch 23/30, Batch 900, Loss: 2.8871\n",
      "Epoch 23/30, Batch 910, Loss: 2.9009\n",
      "Epoch 23/30, Batch 920, Loss: 2.7917\n",
      "Epoch 23/30, Batch 930, Loss: 2.8937\n",
      "Epoch 23/30, Batch 940, Loss: 2.9464\n",
      "Epoch 23/30, Batch 950, Loss: 2.9313\n",
      "Epoch 23/30, Batch 960, Loss: 2.8322\n",
      "Epoch 23/30, Batch 970, Loss: 2.8453\n",
      "Epoch 23/30, Batch 980, Loss: 2.9344\n",
      "Epoch 23/30, Batch 990, Loss: 2.9044\n",
      "Epoch 23/30, Batch 1000, Loss: 2.8122\n",
      "Epoch 23/30, Batch 1010, Loss: 2.8278\n",
      "Epoch 23/30, Batch 1020, Loss: 2.8708\n",
      "Epoch 23/30, Batch 1030, Loss: 2.8453\n",
      "Epoch 23/30, Batch 1040, Loss: 2.8759\n",
      "Epoch 23/30, Batch 1050, Loss: 2.9982\n",
      "Epoch 23/30, Batch 1060, Loss: 2.8601\n",
      "Epoch 23/30, Batch 1070, Loss: 2.9098\n",
      "Epoch 23/30, Batch 1080, Loss: 2.8763\n",
      "Epoch 23/30, Batch 1090, Loss: 3.0307\n",
      "Epoch 23/30, Batch 1100, Loss: 2.9126\n",
      "Epoch 23/30, Batch 1110, Loss: 2.9095\n",
      "Epoch 23/30, Batch 1120, Loss: 2.8685\n",
      "Epoch 23/30, Batch 1130, Loss: 2.8761\n",
      "Epoch 23/30, Batch 1140, Loss: 2.8617\n",
      "Epoch 23/30, Batch 1150, Loss: 2.9329\n",
      "Epoch 23/30, Batch 1160, Loss: 2.8207\n",
      "Epoch 23/30, Batch 1170, Loss: 2.8744\n",
      "Epoch 23/30, Batch 1180, Loss: 2.8727\n",
      "finish  save model of epoch : 22!\n",
      "epoch using time 2069.777\n",
      "Epoch 24/30, Batch 10, Loss: 2.8165\n",
      "Epoch 24/30, Batch 20, Loss: 2.7786\n",
      "Epoch 24/30, Batch 30, Loss: 2.7804\n",
      "Epoch 24/30, Batch 40, Loss: 2.8684\n",
      "Epoch 24/30, Batch 50, Loss: 2.8915\n",
      "Epoch 24/30, Batch 60, Loss: 2.8274\n",
      "Epoch 24/30, Batch 70, Loss: 2.8728\n",
      "Epoch 24/30, Batch 80, Loss: 2.8505\n",
      "Epoch 24/30, Batch 90, Loss: 2.8164\n",
      "Epoch 24/30, Batch 100, Loss: 2.8478\n",
      "Epoch 24/30, Batch 110, Loss: 2.7549\n",
      "Epoch 24/30, Batch 120, Loss: 2.7820\n",
      "Epoch 24/30, Batch 130, Loss: 2.8400\n",
      "Epoch 24/30, Batch 140, Loss: 2.8168\n",
      "Epoch 24/30, Batch 150, Loss: 2.8862\n",
      "Epoch 24/30, Batch 160, Loss: 2.8004\n",
      "Epoch 24/30, Batch 170, Loss: 2.8725\n",
      "Epoch 24/30, Batch 180, Loss: 2.8090\n",
      "Epoch 24/30, Batch 190, Loss: 2.8534\n",
      "Epoch 24/30, Batch 200, Loss: 2.9248\n",
      "Epoch 24/30, Batch 210, Loss: 2.7435\n",
      "Epoch 24/30, Batch 220, Loss: 2.7100\n",
      "Epoch 24/30, Batch 230, Loss: 2.8759\n",
      "Epoch 24/30, Batch 240, Loss: 2.9430\n",
      "Epoch 24/30, Batch 250, Loss: 2.7809\n",
      "Epoch 24/30, Batch 260, Loss: 2.8293\n",
      "Epoch 24/30, Batch 270, Loss: 2.9002\n",
      "Epoch 24/30, Batch 280, Loss: 2.7548\n",
      "Epoch 24/30, Batch 290, Loss: 2.9080\n",
      "Epoch 24/30, Batch 300, Loss: 2.8584\n",
      "Epoch 24/30, Batch 310, Loss: 2.8564\n",
      "Epoch 24/30, Batch 320, Loss: 2.9305\n",
      "Epoch 24/30, Batch 330, Loss: 2.9104\n",
      "Epoch 24/30, Batch 340, Loss: 2.9736\n",
      "Epoch 24/30, Batch 350, Loss: 2.7623\n",
      "Epoch 24/30, Batch 360, Loss: 2.9959\n",
      "Epoch 24/30, Batch 370, Loss: 2.9521\n",
      "Epoch 24/30, Batch 380, Loss: 2.9301\n",
      "Epoch 24/30, Batch 390, Loss: 2.9428\n",
      "Epoch 24/30, Batch 400, Loss: 2.8487\n",
      "Epoch 24/30, Batch 410, Loss: 2.9376\n",
      "Epoch 24/30, Batch 420, Loss: 2.8615\n",
      "Epoch 24/30, Batch 430, Loss: 2.9192\n",
      "Epoch 24/30, Batch 440, Loss: 2.8342\n",
      "Epoch 24/30, Batch 450, Loss: 2.7978\n",
      "Epoch 24/30, Batch 460, Loss: 2.7612\n",
      "Epoch 24/30, Batch 470, Loss: 2.7876\n",
      "Epoch 24/30, Batch 480, Loss: 2.8716\n",
      "Epoch 24/30, Batch 490, Loss: 2.8765\n",
      "Epoch 24/30, Batch 500, Loss: 2.8361\n",
      "Epoch 24/30, Batch 510, Loss: 2.7801\n",
      "Epoch 24/30, Batch 520, Loss: 2.7270\n",
      "Epoch 24/30, Batch 530, Loss: 2.8841\n",
      "Epoch 24/30, Batch 540, Loss: 2.9396\n",
      "Epoch 24/30, Batch 550, Loss: 2.8558\n",
      "Epoch 24/30, Batch 560, Loss: 2.8448\n",
      "Epoch 24/30, Batch 570, Loss: 2.8137\n",
      "Epoch 24/30, Batch 580, Loss: 2.7696\n",
      "Epoch 24/30, Batch 590, Loss: 2.7478\n",
      "Epoch 24/30, Batch 600, Loss: 2.8209\n",
      "Epoch 24/30, Batch 610, Loss: 2.8429\n",
      "Epoch 24/30, Batch 620, Loss: 3.0540\n",
      "Epoch 24/30, Batch 630, Loss: 2.9319\n",
      "Epoch 24/30, Batch 640, Loss: 2.8814\n",
      "Epoch 24/30, Batch 650, Loss: 2.8384\n",
      "Epoch 24/30, Batch 660, Loss: 2.8449\n",
      "Epoch 24/30, Batch 670, Loss: 2.9160\n",
      "Epoch 24/30, Batch 680, Loss: 2.9048\n",
      "Epoch 24/30, Batch 690, Loss: 2.8887\n",
      "Epoch 24/30, Batch 700, Loss: 2.8991\n",
      "Epoch 24/30, Batch 710, Loss: 2.7436\n",
      "Epoch 24/30, Batch 720, Loss: 2.7900\n",
      "Epoch 24/30, Batch 730, Loss: 2.8231\n",
      "Epoch 24/30, Batch 740, Loss: 2.9684\n",
      "Epoch 24/30, Batch 750, Loss: 3.0278\n",
      "Epoch 24/30, Batch 760, Loss: 2.9440\n",
      "Epoch 24/30, Batch 770, Loss: 2.8178\n",
      "Epoch 24/30, Batch 780, Loss: 2.9464\n",
      "Epoch 24/30, Batch 790, Loss: 2.7064\n",
      "Epoch 24/30, Batch 800, Loss: 2.9034\n",
      "Epoch 24/30, Batch 810, Loss: 2.8583\n",
      "Epoch 24/30, Batch 820, Loss: 2.9292\n",
      "Epoch 24/30, Batch 830, Loss: 2.8991\n",
      "Epoch 24/30, Batch 840, Loss: 3.0020\n",
      "Epoch 24/30, Batch 850, Loss: 2.8967\n",
      "Epoch 24/30, Batch 860, Loss: 2.9098\n",
      "Epoch 24/30, Batch 870, Loss: 2.9641\n",
      "Epoch 24/30, Batch 880, Loss: 2.8310\n",
      "Epoch 24/30, Batch 890, Loss: 2.8085\n",
      "Epoch 24/30, Batch 900, Loss: 2.8361\n",
      "Epoch 24/30, Batch 910, Loss: 2.8703\n",
      "Epoch 24/30, Batch 920, Loss: 2.8004\n",
      "Epoch 24/30, Batch 930, Loss: 2.8815\n",
      "Epoch 24/30, Batch 940, Loss: 2.8851\n",
      "Epoch 24/30, Batch 950, Loss: 2.7759\n",
      "Epoch 24/30, Batch 960, Loss: 2.8622\n",
      "Epoch 24/30, Batch 970, Loss: 2.8755\n",
      "Epoch 24/30, Batch 980, Loss: 2.8454\n",
      "Epoch 24/30, Batch 990, Loss: 2.8781\n",
      "Epoch 24/30, Batch 1000, Loss: 2.8972\n",
      "Epoch 24/30, Batch 1010, Loss: 2.8477\n",
      "Epoch 24/30, Batch 1020, Loss: 2.8685\n",
      "Epoch 24/30, Batch 1030, Loss: 2.8840\n",
      "Epoch 24/30, Batch 1040, Loss: 2.8647\n",
      "Epoch 24/30, Batch 1050, Loss: 2.7969\n",
      "Epoch 24/30, Batch 1060, Loss: 3.0127\n",
      "Epoch 24/30, Batch 1070, Loss: 2.9218\n",
      "Epoch 24/30, Batch 1080, Loss: 2.8534\n",
      "Epoch 24/30, Batch 1090, Loss: 2.9599\n",
      "Epoch 24/30, Batch 1100, Loss: 2.8287\n",
      "Epoch 24/30, Batch 1110, Loss: 2.8894\n",
      "Epoch 24/30, Batch 1120, Loss: 2.8237\n",
      "Epoch 24/30, Batch 1130, Loss: 2.8790\n",
      "Epoch 24/30, Batch 1140, Loss: 2.8473\n",
      "Epoch 24/30, Batch 1150, Loss: 2.8282\n",
      "Epoch 24/30, Batch 1160, Loss: 2.9604\n",
      "Epoch 24/30, Batch 1170, Loss: 2.7772\n",
      "Epoch 24/30, Batch 1180, Loss: 2.8144\n",
      "finish  save model of epoch : 23!\n",
      "epoch using time 1985.777\n",
      "Epoch 25/30, Batch 10, Loss: 3.0475\n",
      "Epoch 25/30, Batch 20, Loss: 2.7509\n",
      "Epoch 25/30, Batch 30, Loss: 2.8379\n",
      "Epoch 25/30, Batch 40, Loss: 2.9417\n",
      "Epoch 25/30, Batch 50, Loss: 2.7294\n",
      "Epoch 25/30, Batch 60, Loss: 2.8313\n",
      "Epoch 25/30, Batch 70, Loss: 2.8858\n",
      "Epoch 25/30, Batch 80, Loss: 2.8371\n",
      "Epoch 25/30, Batch 90, Loss: 2.8851\n",
      "Epoch 25/30, Batch 100, Loss: 2.8131\n",
      "Epoch 25/30, Batch 110, Loss: 2.7582\n",
      "Epoch 25/30, Batch 120, Loss: 2.7548\n",
      "Epoch 25/30, Batch 130, Loss: 2.8790\n",
      "Epoch 25/30, Batch 140, Loss: 2.8067\n",
      "Epoch 25/30, Batch 150, Loss: 2.8181\n",
      "Epoch 25/30, Batch 160, Loss: 2.9789\n",
      "Epoch 25/30, Batch 170, Loss: 2.7265\n",
      "Epoch 25/30, Batch 180, Loss: 2.9021\n",
      "Epoch 25/30, Batch 190, Loss: 2.8142\n",
      "Epoch 25/30, Batch 200, Loss: 2.7336\n",
      "Epoch 25/30, Batch 210, Loss: 2.9708\n",
      "Epoch 25/30, Batch 220, Loss: 2.8677\n",
      "Epoch 25/30, Batch 230, Loss: 2.8067\n",
      "Epoch 25/30, Batch 240, Loss: 2.9023\n",
      "Epoch 25/30, Batch 250, Loss: 2.9032\n",
      "Epoch 25/30, Batch 260, Loss: 2.7798\n",
      "Epoch 25/30, Batch 270, Loss: 2.8611\n",
      "Epoch 25/30, Batch 280, Loss: 2.9179\n",
      "Epoch 25/30, Batch 290, Loss: 2.7408\n",
      "Epoch 25/30, Batch 300, Loss: 2.9011\n",
      "Epoch 25/30, Batch 310, Loss: 2.8071\n",
      "Epoch 25/30, Batch 320, Loss: 2.8726\n",
      "Epoch 25/30, Batch 330, Loss: 2.7132\n",
      "Epoch 25/30, Batch 340, Loss: 2.9908\n",
      "Epoch 25/30, Batch 350, Loss: 2.7858\n",
      "Epoch 25/30, Batch 360, Loss: 2.7910\n",
      "Epoch 25/30, Batch 370, Loss: 2.9303\n",
      "Epoch 25/30, Batch 380, Loss: 2.8818\n",
      "Epoch 25/30, Batch 390, Loss: 2.7367\n",
      "Epoch 25/30, Batch 400, Loss: 2.9536\n",
      "Epoch 25/30, Batch 410, Loss: 2.8722\n",
      "Epoch 25/30, Batch 420, Loss: 2.9013\n",
      "Epoch 25/30, Batch 430, Loss: 2.8900\n",
      "Epoch 25/30, Batch 440, Loss: 2.9334\n",
      "Epoch 25/30, Batch 450, Loss: 2.8015\n",
      "Epoch 25/30, Batch 460, Loss: 2.8915\n",
      "Epoch 25/30, Batch 470, Loss: 2.8565\n",
      "Epoch 25/30, Batch 480, Loss: 2.8017\n",
      "Epoch 25/30, Batch 490, Loss: 2.8803\n",
      "Epoch 25/30, Batch 500, Loss: 2.6892\n",
      "Epoch 25/30, Batch 510, Loss: 2.7997\n",
      "Epoch 25/30, Batch 520, Loss: 2.9137\n",
      "Epoch 25/30, Batch 530, Loss: 2.8771\n",
      "Epoch 25/30, Batch 540, Loss: 2.9918\n",
      "Epoch 25/30, Batch 550, Loss: 2.8410\n",
      "Epoch 25/30, Batch 560, Loss: 2.9520\n",
      "Epoch 25/30, Batch 570, Loss: 2.7920\n",
      "Epoch 25/30, Batch 580, Loss: 2.8145\n",
      "Epoch 25/30, Batch 590, Loss: 3.0371\n",
      "Epoch 25/30, Batch 600, Loss: 2.8171\n",
      "Epoch 25/30, Batch 610, Loss: 2.8344\n",
      "Epoch 25/30, Batch 620, Loss: 2.7641\n",
      "Epoch 25/30, Batch 630, Loss: 2.9391\n",
      "Epoch 25/30, Batch 640, Loss: 2.8830\n",
      "Epoch 25/30, Batch 650, Loss: 2.8250\n",
      "Epoch 25/30, Batch 660, Loss: 2.8132\n",
      "Epoch 25/30, Batch 670, Loss: 2.9373\n",
      "Epoch 25/30, Batch 680, Loss: 2.9800\n",
      "Epoch 25/30, Batch 690, Loss: 2.6875\n",
      "Epoch 25/30, Batch 700, Loss: 2.8178\n",
      "Epoch 25/30, Batch 710, Loss: 2.8074\n",
      "Epoch 25/30, Batch 720, Loss: 2.7763\n",
      "Epoch 25/30, Batch 730, Loss: 2.8342\n",
      "Epoch 25/30, Batch 740, Loss: 2.9913\n",
      "Epoch 25/30, Batch 750, Loss: 2.8893\n",
      "Epoch 25/30, Batch 760, Loss: 2.7859\n",
      "Epoch 25/30, Batch 770, Loss: 2.8741\n",
      "Epoch 25/30, Batch 780, Loss: 2.8965\n",
      "Epoch 25/30, Batch 790, Loss: 2.9049\n",
      "Epoch 25/30, Batch 800, Loss: 2.8670\n",
      "Epoch 25/30, Batch 810, Loss: 2.8794\n",
      "Epoch 25/30, Batch 820, Loss: 2.8630\n",
      "Epoch 25/30, Batch 830, Loss: 2.8938\n",
      "Epoch 25/30, Batch 840, Loss: 2.8882\n",
      "Epoch 25/30, Batch 850, Loss: 2.9066\n",
      "Epoch 25/30, Batch 860, Loss: 2.8298\n",
      "Epoch 25/30, Batch 870, Loss: 2.8194\n",
      "Epoch 25/30, Batch 880, Loss: 2.8821\n",
      "Epoch 25/30, Batch 890, Loss: 2.7509\n",
      "Epoch 25/30, Batch 900, Loss: 2.8505\n",
      "Epoch 25/30, Batch 910, Loss: 2.8739\n",
      "Epoch 25/30, Batch 920, Loss: 2.8771\n",
      "Epoch 25/30, Batch 930, Loss: 2.9820\n",
      "Epoch 25/30, Batch 940, Loss: 3.0358\n",
      "Epoch 25/30, Batch 950, Loss: 2.8705\n",
      "Epoch 25/30, Batch 960, Loss: 2.9268\n",
      "Epoch 25/30, Batch 970, Loss: 2.7960\n",
      "Epoch 25/30, Batch 980, Loss: 2.9491\n",
      "Epoch 25/30, Batch 990, Loss: 2.8237\n",
      "Epoch 25/30, Batch 1000, Loss: 2.8946\n",
      "Epoch 25/30, Batch 1010, Loss: 2.8787\n",
      "Epoch 25/30, Batch 1020, Loss: 3.0040\n",
      "Epoch 25/30, Batch 1030, Loss: 2.7645\n",
      "Epoch 25/30, Batch 1040, Loss: 2.8311\n",
      "Epoch 25/30, Batch 1050, Loss: 3.0040\n",
      "Epoch 25/30, Batch 1060, Loss: 2.8448\n",
      "Epoch 25/30, Batch 1070, Loss: 2.9421\n",
      "Epoch 25/30, Batch 1080, Loss: 2.8295\n",
      "Epoch 25/30, Batch 1090, Loss: 2.8846\n",
      "Epoch 25/30, Batch 1100, Loss: 2.8330\n",
      "Epoch 25/30, Batch 1110, Loss: 2.8509\n",
      "Epoch 25/30, Batch 1120, Loss: 2.9140\n",
      "Epoch 25/30, Batch 1130, Loss: 2.9245\n",
      "Epoch 25/30, Batch 1140, Loss: 2.8451\n",
      "Epoch 25/30, Batch 1150, Loss: 2.9310\n",
      "Epoch 25/30, Batch 1160, Loss: 2.7285\n",
      "Epoch 25/30, Batch 1170, Loss: 2.7592\n",
      "Epoch 25/30, Batch 1180, Loss: 2.9844\n",
      "finish  save model of epoch : 24!\n",
      "epoch using time 1997.686\n",
      "Epoch 26/30, Batch 10, Loss: 2.8385\n",
      "Epoch 26/30, Batch 20, Loss: 2.7503\n",
      "Epoch 26/30, Batch 30, Loss: 2.8203\n",
      "Epoch 26/30, Batch 40, Loss: 2.8759\n",
      "Epoch 26/30, Batch 50, Loss: 2.7586\n",
      "Epoch 26/30, Batch 60, Loss: 2.7792\n",
      "Epoch 26/30, Batch 70, Loss: 2.7762\n",
      "Epoch 26/30, Batch 80, Loss: 2.7867\n",
      "Epoch 26/30, Batch 90, Loss: 2.7638\n",
      "Epoch 26/30, Batch 100, Loss: 2.8337\n",
      "Epoch 26/30, Batch 110, Loss: 2.8266\n",
      "Epoch 26/30, Batch 120, Loss: 2.8174\n",
      "Epoch 26/30, Batch 130, Loss: 2.9374\n",
      "Epoch 26/30, Batch 140, Loss: 2.9285\n",
      "Epoch 26/30, Batch 150, Loss: 2.8356\n",
      "Epoch 26/30, Batch 160, Loss: 2.7900\n",
      "Epoch 26/30, Batch 170, Loss: 2.8281\n",
      "Epoch 26/30, Batch 180, Loss: 2.8068\n",
      "Epoch 26/30, Batch 190, Loss: 2.8989\n",
      "Epoch 26/30, Batch 200, Loss: 2.7648\n",
      "Epoch 26/30, Batch 210, Loss: 2.8531\n",
      "Epoch 26/30, Batch 220, Loss: 2.9189\n",
      "Epoch 26/30, Batch 230, Loss: 2.8198\n",
      "Epoch 26/30, Batch 240, Loss: 2.8533\n",
      "Epoch 26/30, Batch 250, Loss: 2.8988\n",
      "Epoch 26/30, Batch 260, Loss: 2.8186\n",
      "Epoch 26/30, Batch 270, Loss: 2.8661\n",
      "Epoch 26/30, Batch 280, Loss: 2.9784\n",
      "Epoch 26/30, Batch 290, Loss: 2.8855\n",
      "Epoch 26/30, Batch 300, Loss: 2.9070\n",
      "Epoch 26/30, Batch 310, Loss: 2.8144\n",
      "Epoch 26/30, Batch 320, Loss: 2.8769\n",
      "Epoch 26/30, Batch 330, Loss: 2.7982\n",
      "Epoch 26/30, Batch 340, Loss: 2.9019\n",
      "Epoch 26/30, Batch 350, Loss: 2.7840\n",
      "Epoch 26/30, Batch 360, Loss: 2.7816\n",
      "Epoch 26/30, Batch 370, Loss: 2.7224\n",
      "Epoch 26/30, Batch 380, Loss: 2.8224\n",
      "Epoch 26/30, Batch 390, Loss: 2.7772\n",
      "Epoch 26/30, Batch 400, Loss: 2.8958\n",
      "Epoch 26/30, Batch 410, Loss: 2.9426\n",
      "Epoch 26/30, Batch 420, Loss: 2.9665\n",
      "Epoch 26/30, Batch 430, Loss: 2.9063\n",
      "Epoch 26/30, Batch 440, Loss: 2.9076\n",
      "Epoch 26/30, Batch 450, Loss: 2.9027\n",
      "Epoch 26/30, Batch 460, Loss: 2.9419\n",
      "Epoch 26/30, Batch 470, Loss: 2.7482\n",
      "Epoch 26/30, Batch 480, Loss: 2.8821\n",
      "Epoch 26/30, Batch 490, Loss: 2.8620\n",
      "Epoch 26/30, Batch 500, Loss: 2.8536\n",
      "Epoch 26/30, Batch 510, Loss: 2.9413\n",
      "Epoch 26/30, Batch 520, Loss: 2.8739\n",
      "Epoch 26/30, Batch 530, Loss: 2.9279\n",
      "Epoch 26/30, Batch 540, Loss: 2.8338\n",
      "Epoch 26/30, Batch 550, Loss: 2.8343\n",
      "Epoch 26/30, Batch 560, Loss: 2.8386\n",
      "Epoch 26/30, Batch 570, Loss: 2.8720\n",
      "Epoch 26/30, Batch 580, Loss: 2.8824\n",
      "Epoch 26/30, Batch 590, Loss: 2.8694\n",
      "Epoch 26/30, Batch 600, Loss: 2.9699\n",
      "Epoch 26/30, Batch 610, Loss: 2.8857\n",
      "Epoch 26/30, Batch 620, Loss: 2.8511\n",
      "Epoch 26/30, Batch 630, Loss: 2.7910\n",
      "Epoch 26/30, Batch 640, Loss: 2.8376\n",
      "Epoch 26/30, Batch 650, Loss: 2.8753\n",
      "Epoch 26/30, Batch 660, Loss: 2.9145\n",
      "Epoch 26/30, Batch 670, Loss: 2.8795\n",
      "Epoch 26/30, Batch 680, Loss: 2.8750\n",
      "Epoch 26/30, Batch 690, Loss: 2.8995\n",
      "Epoch 26/30, Batch 700, Loss: 2.7759\n",
      "Epoch 26/30, Batch 710, Loss: 2.7025\n",
      "Epoch 26/30, Batch 720, Loss: 2.8473\n",
      "Epoch 26/30, Batch 730, Loss: 2.9713\n",
      "Epoch 26/30, Batch 740, Loss: 2.8064\n",
      "Epoch 26/30, Batch 750, Loss: 2.8419\n",
      "Epoch 26/30, Batch 760, Loss: 2.8202\n",
      "Epoch 26/30, Batch 770, Loss: 2.9321\n",
      "Epoch 26/30, Batch 780, Loss: 2.7872\n",
      "Epoch 26/30, Batch 790, Loss: 2.8194\n",
      "Epoch 26/30, Batch 800, Loss: 2.9134\n",
      "Epoch 26/30, Batch 810, Loss: 2.9191\n",
      "Epoch 26/30, Batch 820, Loss: 3.0394\n",
      "Epoch 26/30, Batch 830, Loss: 2.9504\n",
      "Epoch 26/30, Batch 840, Loss: 2.8181\n",
      "Epoch 26/30, Batch 850, Loss: 2.8516\n",
      "Epoch 26/30, Batch 860, Loss: 2.7892\n",
      "Epoch 26/30, Batch 870, Loss: 2.9847\n",
      "Epoch 26/30, Batch 880, Loss: 2.8706\n",
      "Epoch 26/30, Batch 890, Loss: 2.8544\n",
      "Epoch 26/30, Batch 900, Loss: 2.9136\n",
      "Epoch 26/30, Batch 910, Loss: 2.9264\n",
      "Epoch 26/30, Batch 920, Loss: 2.9366\n",
      "Epoch 26/30, Batch 930, Loss: 2.8939\n",
      "Epoch 26/30, Batch 940, Loss: 2.7654\n",
      "Epoch 26/30, Batch 950, Loss: 2.8315\n",
      "Epoch 26/30, Batch 960, Loss: 2.8994\n",
      "Epoch 26/30, Batch 970, Loss: 2.9583\n",
      "Epoch 26/30, Batch 980, Loss: 2.8975\n",
      "Epoch 26/30, Batch 990, Loss: 2.8559\n",
      "Epoch 26/30, Batch 1000, Loss: 2.8708\n",
      "Epoch 26/30, Batch 1010, Loss: 2.6559\n",
      "Epoch 26/30, Batch 1020, Loss: 2.7681\n",
      "Epoch 26/30, Batch 1030, Loss: 2.8731\n",
      "Epoch 26/30, Batch 1040, Loss: 2.9535\n",
      "Epoch 26/30, Batch 1050, Loss: 2.8448\n",
      "Epoch 26/30, Batch 1060, Loss: 2.9657\n",
      "Epoch 26/30, Batch 1070, Loss: 3.0433\n",
      "Epoch 26/30, Batch 1080, Loss: 3.0114\n",
      "Epoch 26/30, Batch 1090, Loss: 2.8702\n",
      "Epoch 26/30, Batch 1100, Loss: 2.9223\n",
      "Epoch 26/30, Batch 1110, Loss: 2.9679\n",
      "Epoch 26/30, Batch 1120, Loss: 2.9183\n",
      "Epoch 26/30, Batch 1130, Loss: 2.9008\n",
      "Epoch 26/30, Batch 1140, Loss: 2.8051\n",
      "Epoch 26/30, Batch 1150, Loss: 2.8598\n",
      "Epoch 26/30, Batch 1160, Loss: 2.8278\n",
      "Epoch 26/30, Batch 1170, Loss: 2.9112\n",
      "Epoch 26/30, Batch 1180, Loss: 2.8920\n",
      "finish  save model of epoch : 25!\n",
      "epoch using time 2082.809\n",
      "Epoch 27/30, Batch 10, Loss: 2.7364\n",
      "Epoch 27/30, Batch 20, Loss: 2.7790\n",
      "Epoch 27/30, Batch 30, Loss: 2.8491\n",
      "Epoch 27/30, Batch 40, Loss: 2.6922\n",
      "Epoch 27/30, Batch 50, Loss: 2.8377\n",
      "Epoch 27/30, Batch 60, Loss: 2.9093\n",
      "Epoch 27/30, Batch 70, Loss: 2.8838\n",
      "Epoch 27/30, Batch 80, Loss: 2.6915\n",
      "Epoch 27/30, Batch 90, Loss: 2.8092\n",
      "Epoch 27/30, Batch 100, Loss: 2.6942\n",
      "Epoch 27/30, Batch 110, Loss: 2.6502\n",
      "Epoch 27/30, Batch 120, Loss: 2.8395\n",
      "Epoch 27/30, Batch 130, Loss: 2.6606\n",
      "Epoch 27/30, Batch 140, Loss: 2.8479\n",
      "Epoch 27/30, Batch 150, Loss: 2.7265\n",
      "Epoch 27/30, Batch 160, Loss: 2.9582\n",
      "Epoch 27/30, Batch 170, Loss: 2.7845\n",
      "Epoch 27/30, Batch 180, Loss: 2.8490\n",
      "Epoch 27/30, Batch 190, Loss: 2.9088\n",
      "Epoch 27/30, Batch 200, Loss: 2.8553\n",
      "Epoch 27/30, Batch 210, Loss: 2.8328\n",
      "Epoch 27/30, Batch 220, Loss: 2.7755\n",
      "Epoch 27/30, Batch 230, Loss: 2.8875\n",
      "Epoch 27/30, Batch 240, Loss: 2.8201\n",
      "Epoch 27/30, Batch 250, Loss: 2.9404\n",
      "Epoch 27/30, Batch 260, Loss: 2.7463\n",
      "Epoch 27/30, Batch 270, Loss: 2.7528\n",
      "Epoch 27/30, Batch 280, Loss: 2.8334\n",
      "Epoch 27/30, Batch 290, Loss: 2.8476\n",
      "Epoch 27/30, Batch 300, Loss: 2.8450\n",
      "Epoch 27/30, Batch 310, Loss: 2.8460\n",
      "Epoch 27/30, Batch 320, Loss: 2.7915\n",
      "Epoch 27/30, Batch 330, Loss: 2.8856\n",
      "Epoch 27/30, Batch 340, Loss: 2.7742\n",
      "Epoch 27/30, Batch 350, Loss: 2.7712\n",
      "Epoch 27/30, Batch 360, Loss: 2.8048\n",
      "Epoch 27/30, Batch 370, Loss: 2.7745\n",
      "Epoch 27/30, Batch 380, Loss: 2.8419\n",
      "Epoch 27/30, Batch 390, Loss: 2.8431\n",
      "Epoch 27/30, Batch 400, Loss: 2.9902\n",
      "Epoch 27/30, Batch 410, Loss: 2.7969\n",
      "Epoch 27/30, Batch 420, Loss: 2.9589\n",
      "Epoch 27/30, Batch 430, Loss: 2.8137\n",
      "Epoch 27/30, Batch 440, Loss: 2.9275\n",
      "Epoch 27/30, Batch 450, Loss: 3.0012\n",
      "Epoch 27/30, Batch 460, Loss: 2.8884\n",
      "Epoch 27/30, Batch 470, Loss: 2.7996\n",
      "Epoch 27/30, Batch 480, Loss: 2.9187\n",
      "Epoch 27/30, Batch 490, Loss: 2.7405\n",
      "Epoch 27/30, Batch 500, Loss: 2.7742\n",
      "Epoch 27/30, Batch 510, Loss: 2.9074\n",
      "Epoch 27/30, Batch 520, Loss: 2.9940\n",
      "Epoch 27/30, Batch 530, Loss: 2.9086\n",
      "Epoch 27/30, Batch 540, Loss: 2.8560\n",
      "Epoch 27/30, Batch 550, Loss: 2.7214\n",
      "Epoch 27/30, Batch 560, Loss: 2.7772\n",
      "Epoch 27/30, Batch 570, Loss: 2.9743\n",
      "Epoch 27/30, Batch 580, Loss: 2.7927\n",
      "Epoch 27/30, Batch 590, Loss: 2.8459\n",
      "Epoch 27/30, Batch 600, Loss: 2.8052\n",
      "Epoch 27/30, Batch 610, Loss: 2.8277\n",
      "Epoch 27/30, Batch 620, Loss: 2.7733\n",
      "Epoch 27/30, Batch 630, Loss: 2.9162\n",
      "Epoch 27/30, Batch 640, Loss: 2.7411\n",
      "Epoch 27/30, Batch 650, Loss: 2.7488\n",
      "Epoch 27/30, Batch 660, Loss: 2.8743\n",
      "Epoch 27/30, Batch 670, Loss: 3.0185\n",
      "Epoch 27/30, Batch 680, Loss: 2.6831\n",
      "Epoch 27/30, Batch 690, Loss: 2.8930\n",
      "Epoch 27/30, Batch 700, Loss: 2.8084\n",
      "Epoch 27/30, Batch 710, Loss: 2.8500\n",
      "Epoch 27/30, Batch 720, Loss: 2.8963\n",
      "Epoch 27/30, Batch 730, Loss: 2.8122\n",
      "Epoch 27/30, Batch 740, Loss: 2.8005\n",
      "Epoch 27/30, Batch 750, Loss: 2.8084\n",
      "Epoch 27/30, Batch 760, Loss: 2.9704\n",
      "Epoch 27/30, Batch 770, Loss: 2.8444\n",
      "Epoch 27/30, Batch 780, Loss: 2.8633\n",
      "Epoch 27/30, Batch 790, Loss: 2.9337\n",
      "Epoch 27/30, Batch 800, Loss: 2.9325\n",
      "Epoch 27/30, Batch 810, Loss: 2.9544\n",
      "Epoch 27/30, Batch 820, Loss: 2.8104\n",
      "Epoch 27/30, Batch 830, Loss: 2.8860\n",
      "Epoch 27/30, Batch 840, Loss: 2.8099\n",
      "Epoch 27/30, Batch 850, Loss: 2.7953\n",
      "Epoch 27/30, Batch 860, Loss: 2.8988\n",
      "Epoch 27/30, Batch 870, Loss: 2.7442\n",
      "Epoch 27/30, Batch 880, Loss: 2.8535\n",
      "Epoch 27/30, Batch 890, Loss: 2.7167\n",
      "Epoch 27/30, Batch 900, Loss: 2.8382\n",
      "Epoch 27/30, Batch 910, Loss: 2.8076\n",
      "Epoch 27/30, Batch 920, Loss: 2.8095\n",
      "Epoch 27/30, Batch 930, Loss: 2.9390\n",
      "Epoch 27/30, Batch 940, Loss: 2.8335\n",
      "Epoch 27/30, Batch 950, Loss: 2.8583\n",
      "Epoch 27/30, Batch 960, Loss: 2.8793\n",
      "Epoch 27/30, Batch 970, Loss: 2.8863\n",
      "Epoch 27/30, Batch 980, Loss: 2.9119\n",
      "Epoch 27/30, Batch 990, Loss: 2.7849\n",
      "Epoch 27/30, Batch 1000, Loss: 2.8651\n",
      "Epoch 27/30, Batch 1010, Loss: 2.8557\n",
      "Epoch 27/30, Batch 1020, Loss: 2.9803\n",
      "Epoch 27/30, Batch 1030, Loss: 2.8864\n",
      "Epoch 27/30, Batch 1040, Loss: 2.9301\n",
      "Epoch 27/30, Batch 1050, Loss: 3.0310\n",
      "Epoch 27/30, Batch 1060, Loss: 2.8729\n",
      "Epoch 27/30, Batch 1070, Loss: 2.7864\n",
      "Epoch 27/30, Batch 1080, Loss: 2.7219\n",
      "Epoch 27/30, Batch 1090, Loss: 2.8829\n",
      "Epoch 27/30, Batch 1100, Loss: 2.9310\n",
      "Epoch 27/30, Batch 1110, Loss: 2.9488\n",
      "Epoch 27/30, Batch 1120, Loss: 2.7521\n",
      "Epoch 27/30, Batch 1130, Loss: 2.8504\n",
      "Epoch 27/30, Batch 1140, Loss: 3.0002\n",
      "Epoch 27/30, Batch 1150, Loss: 2.8784\n",
      "Epoch 27/30, Batch 1160, Loss: 2.8052\n",
      "Epoch 27/30, Batch 1170, Loss: 2.8241\n",
      "Epoch 27/30, Batch 1180, Loss: 3.0177\n",
      "finish  save model of epoch : 26!\n",
      "epoch using time 2188.981\n",
      "Epoch 28/30, Batch 10, Loss: 2.7494\n",
      "Epoch 28/30, Batch 20, Loss: 2.6686\n",
      "Epoch 28/30, Batch 30, Loss: 2.9324\n",
      "Epoch 28/30, Batch 40, Loss: 2.6879\n",
      "Epoch 28/30, Batch 50, Loss: 2.8659\n",
      "Epoch 28/30, Batch 60, Loss: 2.7110\n",
      "Epoch 28/30, Batch 70, Loss: 2.8131\n",
      "Epoch 28/30, Batch 80, Loss: 2.8393\n",
      "Epoch 28/30, Batch 90, Loss: 2.7931\n",
      "Epoch 28/30, Batch 100, Loss: 2.7827\n",
      "Epoch 28/30, Batch 110, Loss: 2.8645\n",
      "Epoch 28/30, Batch 120, Loss: 2.7500\n",
      "Epoch 28/30, Batch 130, Loss: 2.8079\n",
      "Epoch 28/30, Batch 140, Loss: 2.7898\n",
      "Epoch 28/30, Batch 150, Loss: 2.8298\n",
      "Epoch 28/30, Batch 160, Loss: 2.8715\n",
      "Epoch 28/30, Batch 170, Loss: 2.8036\n",
      "Epoch 28/30, Batch 180, Loss: 2.8466\n",
      "Epoch 28/30, Batch 190, Loss: 2.9327\n",
      "Epoch 28/30, Batch 200, Loss: 2.8108\n",
      "Epoch 28/30, Batch 210, Loss: 2.8266\n",
      "Epoch 28/30, Batch 220, Loss: 2.8313\n",
      "Epoch 28/30, Batch 230, Loss: 2.8904\n",
      "Epoch 28/30, Batch 240, Loss: 2.8757\n",
      "Epoch 28/30, Batch 250, Loss: 2.7266\n",
      "Epoch 28/30, Batch 260, Loss: 2.9535\n",
      "Epoch 28/30, Batch 270, Loss: 2.7717\n",
      "Epoch 28/30, Batch 280, Loss: 2.7602\n",
      "Epoch 28/30, Batch 290, Loss: 2.8219\n",
      "Epoch 28/30, Batch 300, Loss: 2.8094\n",
      "Epoch 28/30, Batch 310, Loss: 2.7845\n",
      "Epoch 28/30, Batch 320, Loss: 2.7788\n",
      "Epoch 28/30, Batch 330, Loss: 2.8768\n",
      "Epoch 28/30, Batch 340, Loss: 2.7219\n",
      "Epoch 28/30, Batch 350, Loss: 2.8422\n",
      "Epoch 28/30, Batch 360, Loss: 2.8665\n",
      "Epoch 28/30, Batch 370, Loss: 2.8904\n",
      "Epoch 28/30, Batch 380, Loss: 2.6864\n",
      "Epoch 28/30, Batch 390, Loss: 2.7983\n",
      "Epoch 28/30, Batch 400, Loss: 2.8552\n",
      "Epoch 28/30, Batch 410, Loss: 2.8367\n",
      "Epoch 28/30, Batch 420, Loss: 2.8376\n",
      "Epoch 28/30, Batch 430, Loss: 2.9423\n",
      "Epoch 28/30, Batch 440, Loss: 2.8437\n",
      "Epoch 28/30, Batch 450, Loss: 2.9940\n",
      "Epoch 28/30, Batch 460, Loss: 2.8927\n",
      "Epoch 28/30, Batch 470, Loss: 2.7455\n",
      "Epoch 28/30, Batch 480, Loss: 2.7640\n",
      "Epoch 28/30, Batch 490, Loss: 2.9095\n",
      "Epoch 28/30, Batch 500, Loss: 2.7937\n",
      "Epoch 28/30, Batch 510, Loss: 2.8041\n",
      "Epoch 28/30, Batch 520, Loss: 2.9749\n",
      "Epoch 28/30, Batch 530, Loss: 2.8610\n",
      "Epoch 28/30, Batch 540, Loss: 2.9545\n",
      "Epoch 28/30, Batch 550, Loss: 2.8437\n",
      "Epoch 28/30, Batch 560, Loss: 2.8490\n",
      "Epoch 28/30, Batch 570, Loss: 2.9477\n",
      "Epoch 28/30, Batch 580, Loss: 2.8631\n",
      "Epoch 28/30, Batch 590, Loss: 2.8638\n",
      "Epoch 28/30, Batch 600, Loss: 2.8297\n",
      "Epoch 28/30, Batch 610, Loss: 3.0035\n",
      "Epoch 28/30, Batch 620, Loss: 2.8757\n",
      "Epoch 28/30, Batch 630, Loss: 2.7731\n",
      "Epoch 28/30, Batch 640, Loss: 2.8927\n",
      "Epoch 28/30, Batch 650, Loss: 2.8121\n",
      "Epoch 28/30, Batch 660, Loss: 2.7520\n",
      "Epoch 28/30, Batch 670, Loss: 2.8422\n",
      "Epoch 28/30, Batch 680, Loss: 2.7741\n",
      "Epoch 28/30, Batch 690, Loss: 2.9062\n",
      "Epoch 28/30, Batch 700, Loss: 2.7769\n",
      "Epoch 28/30, Batch 710, Loss: 2.8685\n",
      "Epoch 28/30, Batch 720, Loss: 2.7499\n",
      "Epoch 28/30, Batch 730, Loss: 2.8345\n",
      "Epoch 28/30, Batch 740, Loss: 2.9043\n",
      "Epoch 28/30, Batch 750, Loss: 3.0109\n",
      "Epoch 28/30, Batch 760, Loss: 2.8093\n",
      "Epoch 28/30, Batch 770, Loss: 2.8545\n",
      "Epoch 28/30, Batch 780, Loss: 2.7599\n",
      "Epoch 28/30, Batch 790, Loss: 2.8805\n",
      "Epoch 28/30, Batch 800, Loss: 3.0817\n",
      "Epoch 28/30, Batch 810, Loss: 2.9791\n",
      "Epoch 28/30, Batch 820, Loss: 2.8799\n",
      "Epoch 28/30, Batch 830, Loss: 2.8194\n",
      "Epoch 28/30, Batch 840, Loss: 2.8961\n",
      "Epoch 28/30, Batch 850, Loss: 3.0398\n",
      "Epoch 28/30, Batch 860, Loss: 2.8973\n",
      "Epoch 28/30, Batch 870, Loss: 2.8873\n",
      "Epoch 28/30, Batch 880, Loss: 2.8073\n",
      "Epoch 28/30, Batch 890, Loss: 3.0463\n",
      "Epoch 28/30, Batch 900, Loss: 2.9038\n",
      "Epoch 28/30, Batch 910, Loss: 2.7914\n",
      "Epoch 28/30, Batch 920, Loss: 2.8414\n",
      "Epoch 28/30, Batch 930, Loss: 2.8735\n",
      "Epoch 28/30, Batch 940, Loss: 2.9205\n",
      "Epoch 28/30, Batch 950, Loss: 2.9454\n",
      "Epoch 28/30, Batch 960, Loss: 2.8588\n",
      "Epoch 28/30, Batch 970, Loss: 2.8964\n",
      "Epoch 28/30, Batch 980, Loss: 2.7913\n",
      "Epoch 28/30, Batch 990, Loss: 3.0191\n",
      "Epoch 28/30, Batch 1000, Loss: 2.8733\n",
      "Epoch 28/30, Batch 1010, Loss: 2.9761\n",
      "Epoch 28/30, Batch 1020, Loss: 2.8605\n",
      "Epoch 28/30, Batch 1030, Loss: 2.9051\n",
      "Epoch 28/30, Batch 1040, Loss: 2.7968\n",
      "Epoch 28/30, Batch 1050, Loss: 2.9288\n",
      "Epoch 28/30, Batch 1060, Loss: 2.7319\n",
      "Epoch 28/30, Batch 1070, Loss: 2.9220\n",
      "Epoch 28/30, Batch 1080, Loss: 2.8655\n",
      "Epoch 28/30, Batch 1090, Loss: 2.9208\n",
      "Epoch 28/30, Batch 1100, Loss: 2.8732\n",
      "Epoch 28/30, Batch 1110, Loss: 2.8297\n",
      "Epoch 28/30, Batch 1120, Loss: 2.8268\n",
      "Epoch 28/30, Batch 1130, Loss: 2.9492\n",
      "Epoch 28/30, Batch 1140, Loss: 2.8458\n",
      "Epoch 28/30, Batch 1150, Loss: 2.7260\n",
      "Epoch 28/30, Batch 1160, Loss: 2.8562\n",
      "Epoch 28/30, Batch 1170, Loss: 2.8520\n",
      "Epoch 28/30, Batch 1180, Loss: 2.8524\n",
      "finish  save model of epoch : 27!\n",
      "epoch using time 2259.403\n",
      "Epoch 29/30, Batch 10, Loss: 2.7627\n",
      "Epoch 29/30, Batch 20, Loss: 2.8893\n",
      "Epoch 29/30, Batch 30, Loss: 2.8732\n",
      "Epoch 29/30, Batch 40, Loss: 2.8730\n",
      "Epoch 29/30, Batch 50, Loss: 2.8894\n",
      "Epoch 29/30, Batch 60, Loss: 2.7805\n",
      "Epoch 29/30, Batch 70, Loss: 2.8203\n",
      "Epoch 29/30, Batch 80, Loss: 2.8409\n",
      "Epoch 29/30, Batch 90, Loss: 2.8385\n",
      "Epoch 29/30, Batch 100, Loss: 2.9456\n",
      "Epoch 29/30, Batch 110, Loss: 2.7476\n",
      "Epoch 29/30, Batch 120, Loss: 2.9103\n",
      "Epoch 29/30, Batch 130, Loss: 2.9315\n",
      "Epoch 29/30, Batch 140, Loss: 2.8176\n",
      "Epoch 29/30, Batch 150, Loss: 2.8306\n",
      "Epoch 29/30, Batch 160, Loss: 2.8073\n",
      "Epoch 29/30, Batch 170, Loss: 2.8320\n",
      "Epoch 29/30, Batch 180, Loss: 2.8107\n",
      "Epoch 29/30, Batch 190, Loss: 2.7992\n",
      "Epoch 29/30, Batch 200, Loss: 2.8197\n",
      "Epoch 29/30, Batch 210, Loss: 2.9239\n",
      "Epoch 29/30, Batch 220, Loss: 2.7189\n",
      "Epoch 29/30, Batch 230, Loss: 2.7411\n",
      "Epoch 29/30, Batch 240, Loss: 2.8904\n",
      "Epoch 29/30, Batch 250, Loss: 2.7493\n",
      "Epoch 29/30, Batch 260, Loss: 2.9028\n",
      "Epoch 29/30, Batch 270, Loss: 2.9277\n",
      "Epoch 29/30, Batch 280, Loss: 2.8339\n",
      "Epoch 29/30, Batch 290, Loss: 2.7906\n",
      "Epoch 29/30, Batch 300, Loss: 2.9658\n",
      "Epoch 29/30, Batch 310, Loss: 2.8273\n",
      "Epoch 29/30, Batch 320, Loss: 2.7671\n",
      "Epoch 29/30, Batch 330, Loss: 2.6709\n",
      "Epoch 29/30, Batch 340, Loss: 2.8600\n",
      "Epoch 29/30, Batch 350, Loss: 2.6834\n",
      "Epoch 29/30, Batch 360, Loss: 2.8900\n",
      "Epoch 29/30, Batch 370, Loss: 2.8431\n",
      "Epoch 29/30, Batch 380, Loss: 2.8617\n",
      "Epoch 29/30, Batch 390, Loss: 2.8381\n",
      "Epoch 29/30, Batch 400, Loss: 2.8613\n",
      "Epoch 29/30, Batch 410, Loss: 2.7422\n",
      "Epoch 29/30, Batch 420, Loss: 2.8240\n",
      "Epoch 29/30, Batch 430, Loss: 2.7799\n",
      "Epoch 29/30, Batch 440, Loss: 2.7259\n",
      "Epoch 29/30, Batch 450, Loss: 2.8237\n",
      "Epoch 29/30, Batch 460, Loss: 2.7289\n",
      "Epoch 29/30, Batch 470, Loss: 2.7692\n",
      "Epoch 29/30, Batch 480, Loss: 2.8528\n",
      "Epoch 29/30, Batch 490, Loss: 2.9276\n",
      "Epoch 29/30, Batch 500, Loss: 3.0049\n",
      "Epoch 29/30, Batch 510, Loss: 2.7416\n",
      "Epoch 29/30, Batch 520, Loss: 2.8898\n",
      "Epoch 29/30, Batch 530, Loss: 3.0026\n",
      "Epoch 29/30, Batch 540, Loss: 2.7677\n",
      "Epoch 29/30, Batch 550, Loss: 2.7540\n",
      "Epoch 29/30, Batch 560, Loss: 2.9152\n",
      "Epoch 29/30, Batch 570, Loss: 2.6790\n",
      "Epoch 29/30, Batch 580, Loss: 2.9598\n",
      "Epoch 29/30, Batch 590, Loss: 2.8883\n",
      "Epoch 29/30, Batch 600, Loss: 2.9695\n",
      "Epoch 29/30, Batch 610, Loss: 2.8621\n",
      "Epoch 29/30, Batch 620, Loss: 2.9421\n",
      "Epoch 29/30, Batch 630, Loss: 2.8304\n",
      "Epoch 29/30, Batch 640, Loss: 2.7817\n",
      "Epoch 29/30, Batch 650, Loss: 2.8792\n",
      "Epoch 29/30, Batch 660, Loss: 2.8525\n",
      "Epoch 29/30, Batch 670, Loss: 2.8879\n",
      "Epoch 29/30, Batch 680, Loss: 2.8210\n",
      "Epoch 29/30, Batch 690, Loss: 2.8750\n",
      "Epoch 29/30, Batch 700, Loss: 2.8869\n",
      "Epoch 29/30, Batch 710, Loss: 2.8919\n",
      "Epoch 29/30, Batch 720, Loss: 2.9815\n",
      "Epoch 29/30, Batch 730, Loss: 2.7382\n",
      "Epoch 29/30, Batch 740, Loss: 2.8592\n",
      "Epoch 29/30, Batch 750, Loss: 2.8089\n",
      "Epoch 29/30, Batch 760, Loss: 2.8072\n",
      "Epoch 29/30, Batch 770, Loss: 2.8998\n",
      "Epoch 29/30, Batch 780, Loss: 2.9447\n",
      "Epoch 29/30, Batch 790, Loss: 2.8041\n",
      "Epoch 29/30, Batch 800, Loss: 2.9752\n",
      "Epoch 29/30, Batch 810, Loss: 2.8162\n",
      "Epoch 29/30, Batch 820, Loss: 2.7669\n",
      "Epoch 29/30, Batch 830, Loss: 2.8457\n",
      "Epoch 29/30, Batch 840, Loss: 2.8401\n",
      "Epoch 29/30, Batch 850, Loss: 2.7567\n",
      "Epoch 29/30, Batch 860, Loss: 2.7471\n",
      "Epoch 29/30, Batch 870, Loss: 2.9457\n",
      "Epoch 29/30, Batch 880, Loss: 2.9096\n",
      "Epoch 29/30, Batch 890, Loss: 2.9874\n",
      "Epoch 29/30, Batch 900, Loss: 2.8133\n",
      "Epoch 29/30, Batch 910, Loss: 2.9593\n",
      "Epoch 29/30, Batch 920, Loss: 3.0277\n",
      "Epoch 29/30, Batch 930, Loss: 2.8823\n",
      "Epoch 29/30, Batch 940, Loss: 2.9633\n",
      "Epoch 29/30, Batch 950, Loss: 2.9450\n",
      "Epoch 29/30, Batch 960, Loss: 2.8438\n",
      "Epoch 29/30, Batch 970, Loss: 2.9869\n",
      "Epoch 29/30, Batch 980, Loss: 2.9582\n",
      "Epoch 29/30, Batch 990, Loss: 2.8618\n",
      "Epoch 29/30, Batch 1000, Loss: 2.9857\n",
      "Epoch 29/30, Batch 1010, Loss: 2.9122\n",
      "Epoch 29/30, Batch 1020, Loss: 2.8348\n",
      "Epoch 29/30, Batch 1030, Loss: 2.8993\n",
      "Epoch 29/30, Batch 1040, Loss: 2.9070\n",
      "Epoch 29/30, Batch 1050, Loss: 2.8242\n",
      "Epoch 29/30, Batch 1060, Loss: 2.8128\n",
      "Epoch 29/30, Batch 1070, Loss: 2.9751\n",
      "Epoch 29/30, Batch 1080, Loss: 2.9463\n",
      "Epoch 29/30, Batch 1090, Loss: 2.9578\n",
      "Epoch 29/30, Batch 1100, Loss: 2.9642\n",
      "Epoch 29/30, Batch 1110, Loss: 2.8479\n",
      "Epoch 29/30, Batch 1120, Loss: 2.7120\n",
      "Epoch 29/30, Batch 1130, Loss: 2.9069\n",
      "Epoch 29/30, Batch 1140, Loss: 2.8596\n",
      "Epoch 29/30, Batch 1150, Loss: 2.9622\n",
      "Epoch 29/30, Batch 1160, Loss: 2.9691\n",
      "Epoch 29/30, Batch 1170, Loss: 2.8672\n",
      "Epoch 29/30, Batch 1180, Loss: 2.9098\n",
      "finish  save model of epoch : 28!\n",
      "epoch using time 2266.711\n",
      "Epoch 30/30, Batch 10, Loss: 2.9077\n",
      "Epoch 30/30, Batch 20, Loss: 2.7236\n",
      "Epoch 30/30, Batch 30, Loss: 2.9462\n",
      "Epoch 30/30, Batch 40, Loss: 2.8034\n",
      "Epoch 30/30, Batch 50, Loss: 2.7611\n",
      "Epoch 30/30, Batch 60, Loss: 2.8158\n",
      "Epoch 30/30, Batch 70, Loss: 2.8104\n",
      "Epoch 30/30, Batch 80, Loss: 2.7366\n",
      "Epoch 30/30, Batch 90, Loss: 2.8895\n",
      "Epoch 30/30, Batch 100, Loss: 2.8727\n",
      "Epoch 30/30, Batch 110, Loss: 2.8145\n",
      "Epoch 30/30, Batch 120, Loss: 2.7782\n",
      "Epoch 30/30, Batch 130, Loss: 2.8341\n",
      "Epoch 30/30, Batch 140, Loss: 2.7510\n",
      "Epoch 30/30, Batch 150, Loss: 2.8491\n",
      "Epoch 30/30, Batch 160, Loss: 2.8686\n",
      "Epoch 30/30, Batch 170, Loss: 2.7828\n",
      "Epoch 30/30, Batch 180, Loss: 2.7541\n",
      "Epoch 30/30, Batch 190, Loss: 2.8125\n",
      "Epoch 30/30, Batch 200, Loss: 2.7943\n",
      "Epoch 30/30, Batch 210, Loss: 2.6353\n",
      "Epoch 30/30, Batch 220, Loss: 2.7573\n",
      "Epoch 30/30, Batch 230, Loss: 2.8118\n",
      "Epoch 30/30, Batch 240, Loss: 2.9428\n",
      "Epoch 30/30, Batch 250, Loss: 2.7579\n",
      "Epoch 30/30, Batch 260, Loss: 2.8721\n",
      "Epoch 30/30, Batch 270, Loss: 2.6734\n",
      "Epoch 30/30, Batch 280, Loss: 2.8655\n",
      "Epoch 30/30, Batch 290, Loss: 2.8202\n",
      "Epoch 30/30, Batch 300, Loss: 2.8411\n",
      "Epoch 30/30, Batch 310, Loss: 3.0116\n",
      "Epoch 30/30, Batch 320, Loss: 2.8830\n",
      "Epoch 30/30, Batch 330, Loss: 2.8401\n",
      "Epoch 30/30, Batch 340, Loss: 2.7563\n",
      "Epoch 30/30, Batch 350, Loss: 2.8794\n",
      "Epoch 30/30, Batch 360, Loss: 2.8108\n",
      "Epoch 30/30, Batch 370, Loss: 2.7896\n",
      "Epoch 30/30, Batch 380, Loss: 2.8943\n",
      "Epoch 30/30, Batch 390, Loss: 2.8480\n",
      "Epoch 30/30, Batch 400, Loss: 2.7667\n",
      "Epoch 30/30, Batch 410, Loss: 2.7992\n",
      "Epoch 30/30, Batch 420, Loss: 2.8628\n",
      "Epoch 30/30, Batch 430, Loss: 2.7068\n",
      "Epoch 30/30, Batch 440, Loss: 2.8229\n",
      "Epoch 30/30, Batch 450, Loss: 2.8906\n",
      "Epoch 30/30, Batch 460, Loss: 2.9155\n",
      "Epoch 30/30, Batch 470, Loss: 2.7387\n",
      "Epoch 30/30, Batch 480, Loss: 2.7126\n",
      "Epoch 30/30, Batch 490, Loss: 2.8545\n",
      "Epoch 30/30, Batch 500, Loss: 2.9726\n",
      "Epoch 30/30, Batch 510, Loss: 3.0052\n",
      "Epoch 30/30, Batch 520, Loss: 2.7438\n",
      "Epoch 30/30, Batch 530, Loss: 2.7677\n",
      "Epoch 30/30, Batch 540, Loss: 2.8082\n",
      "Epoch 30/30, Batch 550, Loss: 2.7932\n",
      "Epoch 30/30, Batch 560, Loss: 2.8522\n",
      "Epoch 30/30, Batch 570, Loss: 2.8340\n",
      "Epoch 30/30, Batch 580, Loss: 2.9414\n",
      "Epoch 30/30, Batch 590, Loss: 2.9010\n",
      "Epoch 30/30, Batch 600, Loss: 2.8026\n",
      "Epoch 30/30, Batch 610, Loss: 2.8585\n",
      "Epoch 30/30, Batch 620, Loss: 2.8426\n",
      "Epoch 30/30, Batch 630, Loss: 2.9217\n",
      "Epoch 30/30, Batch 640, Loss: 2.7586\n",
      "Epoch 30/30, Batch 650, Loss: 2.7718\n",
      "Epoch 30/30, Batch 660, Loss: 2.8759\n",
      "Epoch 30/30, Batch 670, Loss: 2.8116\n",
      "Epoch 30/30, Batch 680, Loss: 2.9146\n",
      "Epoch 30/30, Batch 690, Loss: 2.8995\n",
      "Epoch 30/30, Batch 700, Loss: 2.8924\n",
      "Epoch 30/30, Batch 710, Loss: 2.9850\n",
      "Epoch 30/30, Batch 720, Loss: 2.6575\n",
      "Epoch 30/30, Batch 730, Loss: 2.9443\n",
      "Epoch 30/30, Batch 740, Loss: 2.6946\n",
      "Epoch 30/30, Batch 750, Loss: 2.7625\n",
      "Epoch 30/30, Batch 760, Loss: 2.7727\n",
      "Epoch 30/30, Batch 770, Loss: 2.9251\n",
      "Epoch 30/30, Batch 780, Loss: 2.8275\n",
      "Epoch 30/30, Batch 790, Loss: 2.7426\n",
      "Epoch 30/30, Batch 800, Loss: 2.8412\n",
      "Epoch 30/30, Batch 810, Loss: 2.9696\n",
      "Epoch 30/30, Batch 820, Loss: 2.8531\n",
      "Epoch 30/30, Batch 830, Loss: 2.7884\n",
      "Epoch 30/30, Batch 840, Loss: 2.8962\n",
      "Epoch 30/30, Batch 850, Loss: 2.9720\n",
      "Epoch 30/30, Batch 860, Loss: 2.8492\n",
      "Epoch 30/30, Batch 870, Loss: 2.7144\n",
      "Epoch 30/30, Batch 880, Loss: 2.8774\n",
      "Epoch 30/30, Batch 890, Loss: 2.8710\n",
      "Epoch 30/30, Batch 900, Loss: 2.8173\n",
      "Epoch 30/30, Batch 910, Loss: 2.8194\n",
      "Epoch 30/30, Batch 920, Loss: 2.8004\n",
      "Epoch 30/30, Batch 930, Loss: 2.8464\n",
      "Epoch 30/30, Batch 940, Loss: 2.8249\n",
      "Epoch 30/30, Batch 950, Loss: 2.9000\n",
      "Epoch 30/30, Batch 960, Loss: 2.8520\n",
      "Epoch 30/30, Batch 970, Loss: 2.9069\n",
      "Epoch 30/30, Batch 980, Loss: 2.8507\n",
      "Epoch 30/30, Batch 990, Loss: 2.8934\n",
      "Epoch 30/30, Batch 1000, Loss: 2.8103\n",
      "Epoch 30/30, Batch 1010, Loss: 2.8442\n",
      "Epoch 30/30, Batch 1020, Loss: 2.8638\n",
      "Epoch 30/30, Batch 1030, Loss: 2.9265\n",
      "Epoch 30/30, Batch 1040, Loss: 2.9724\n",
      "Epoch 30/30, Batch 1050, Loss: 2.9693\n",
      "Epoch 30/30, Batch 1060, Loss: 2.8622\n",
      "Epoch 30/30, Batch 1070, Loss: 2.8955\n",
      "Epoch 30/30, Batch 1080, Loss: 2.9100\n",
      "Epoch 30/30, Batch 1090, Loss: 2.8580\n",
      "Epoch 30/30, Batch 1100, Loss: 2.9109\n",
      "Epoch 30/30, Batch 1110, Loss: 2.8041\n",
      "Epoch 30/30, Batch 1120, Loss: 2.7496\n",
      "Epoch 30/30, Batch 1130, Loss: 2.8521\n",
      "Epoch 30/30, Batch 1140, Loss: 2.8471\n",
      "Epoch 30/30, Batch 1150, Loss: 2.8733\n",
      "Epoch 30/30, Batch 1160, Loss: 2.8024\n",
      "Epoch 30/30, Batch 1170, Loss: 2.7730\n",
      "Epoch 30/30, Batch 1180, Loss: 2.8145\n",
      "finish  save model of epoch : 29!\n",
      "epoch using time 2324.202\n"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共处理有效诗句数量: 151798\n",
      "春日访王丞相X春风吹雨过，春色满春风。花柳花如锦，春风吹柳花。Y\n",
      "共处理有效诗句数量: 151798\n",
      "X春风吹不动，一笑一时休。Y\n",
      "共处理有效诗句数量: 151798\n",
      "江岸别杜甫X江南江北路，江北见山川。一舸归船去，孤帆过岭云。Y\n",
      "共处理有效诗句数量: 151798\n",
      "X风雨凄凄夜，清风吹暮云。Y\n",
      "共处理有效诗句数量: 151798\n",
      "送白居易X一别三年别，今年一再归。一年归故国，一别一年秋。Y\n",
      "共处理有效诗句数量: 151798\n",
      "月见X月照孤蟾照影斜，月明风月照人空。夜深月色无人管，一夜风声入夜钟。Y\n",
      "共处理有效诗句数量: 151798\n",
      "与李白会诗X一片青山一点尘，一年春色满人家。春风吹尽春风急，不见春风一片春。Y\n",
      "共处理有效诗句数量: 151798\n",
      "X九衢之北，其中之德。以爲之之，以爲以力。Y\n",
      "共处理有效诗句数量: 151798\n",
      "北山雪X山中有佳处，山色自相依。山色无人到，山深不见梅。Y\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(gen_poem(\"春日访王丞相X\"))\n",
    "\n",
    "print(gen_poem(\"X春风\"))\n",
    "\n",
    "print(gen_poem(\"江岸别杜甫X\"))\n",
    "\n",
    "print(gen_poem(\"X风\"))\n",
    "\n",
    "print(gen_poem(\"送白居易X\"))\n",
    "\n",
    "print(gen_poem(\"月见X月\"))\n",
    "\n",
    "print(gen_poem(\"与李白会诗X\"))\n",
    "\n",
    "print(gen_poem(\"X九\"))\n",
    "\n",
    "\n",
    "\n",
    "print(gen_poem(\"北山雪X\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
