{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtQgkiidYEPz",
        "outputId": "65313d62-f3d8-4093-e7d0-e2304c4780b1"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU0nX4mTX_W6",
        "outputId": "42229d1e-6873-4668-aa80-6156b5f7a13d"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/negacio_uab_revised_version.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/content/drive/MyDrive/negacio_uab_revised_version.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m data\n",
            "File \u001b[0;32m~/miniforge3/envs/ML/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/negacio_uab_revised_version.json'"
          ]
        }
      ],
      "source": [
        "import json\n",
        "with open(\"/content/drive/MyDrive/negacio_uab_revised_version.json\",\"r\") as f:\n",
        "    data = json.load(f)\n",
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CgKWUCDX_W8"
      },
      "source": [
        "data -> array. Each cell is one \"historia clinica\" entry\n",
        "\n",
        "data[0] -> dictionary with \"data\" and \"anotations\" and \"predictions\"\n",
        "\n",
        "        -> \"data\" would be the \"x\". Contains id, actual text. And other data.\n",
        "\n",
        "        -> \"annotations\" seems to be empty\n",
        "        \n",
        "        -> \"predictions\" our \"y\", \"label\". Contains the tags for indexes of the text.\n",
        "\n",
        "data[0][\"predictions\"][0] <- for some reason nested inside an unnecessary array\n",
        "\n",
        "data[0][\"predictions\"][0][\"result\"] <- also nested inside unnecessary dictionary with one key\n",
        "\n",
        "\n",
        "data[0][\"predictions\"][0][\"result\"] -> inside here we have a list of tags for different ranges.\n",
        "\n",
        "data[0][\"predictions\"][0][\"result\"][0] -> in each of these elements of the list, we have another dictionary\n",
        "\n",
        "data[0][\"pred2ictions\"][0][\"result\"][0][\"value\"] -> to access a dictionary with the keys \"start\", \"end\", and \"labels\". For start, end index and the tag\n",
        "\n",
        "data[0][\"pred2ictions\"][0][\"result\"][0] -> we have more keys such as \"id\", \"from_name\", \"to_name\" and \"type\". Use?? differentiate if is label or not? what else can we have?\n",
        "\n",
        "data[0][\"pred2ictions\"][0][\"result\"][0][\"value\"][\"labels\"] -> Not sure if we can have more than one tag per range but the tags are in an array in string form ['NEG']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UegiMBwwX_W-",
        "outputId": "e84d3c2f-f5fc-432b-e224-c7a2c228bc63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['NEG']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[2][\"predictions\"][0][\"result\"][0][\"value\"][\"labels\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mA2dZfA0X_W_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCGcPILgX_XA",
        "outputId": "5c556e2d-7fd3-4a12-973f-cc3957d8c01a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting icecream\n",
            "  Downloading icecream-2.1.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Collecting colorama>=0.3.9 (from icecream)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pygments>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from icecream) (2.14.0)\n",
            "Collecting executing>=0.3.1 (from icecream)\n",
            "  Downloading executing-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Collecting asttokens>=2.0.1 (from icecream)\n",
            "  Downloading asttokens-2.2.1-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.0.1->icecream) (1.16.0)\n",
            "Installing collected packages: executing, colorama, asttokens, icecream\n",
            "Successfully installed asttokens-2.2.1 colorama-0.4.6 executing-1.2.0 icecream-2.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install icecream\n",
        "from icecream import ic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "jVOLuH63X_XA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JPtdgrTcX_XA"
      },
      "outputs": [],
      "source": [
        "class characters_text2tagsDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This class takes the given dataset in form of a list of nested dictionaries\n",
        "    and outputs a window of sequence_length characters and tags around an id word.\n",
        "    Given a center word we get from -sequence_lenght/2 to +sequence_lenght/2.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,data,sequence_length): \n",
        "        self.sequence_lenght = sequence_length\n",
        "        self.data = copy.deepcopy(data)\n",
        "        self.tags = list(np.empty(len(data)))\n",
        "\n",
        "        #we'll use this for as we go extract vocab an usable tags\n",
        "        self.vocab = set()\n",
        "        self.uniq_tags = set()\n",
        "\n",
        "        #To start, we should clean up a bit our data, as we noticed that there are a lot of unnecessary empty spaces.\n",
        "        for i,report in enumerate(data):\n",
        "            self.remove_empty_spaces(i,report)\n",
        "        \n",
        "        #We should padd the reports so that no window of sequence_lenght captures more than one document\n",
        "        #save previous reports length to later on keep track.\n",
        "        self.data_l = np.array([len(d[\"data\"][\"text\"]) for d in data])\n",
        "        self.dataset_lenght = np.sum(self.data_l) #how many characters over whole reports\n",
        "        \n",
        "        for i,report in enumerate(data): #iterate over all reports\n",
        "            self.extract_vocab(report)\n",
        "            self.extract_unique_tags(report)\n",
        "            self.pad_report(i,report)\n",
        "        \n",
        "        #get lenghts for padded data\n",
        "        self.pdata_l = np.array([len(d[\"data\"][\"text\"]) for d in data])\n",
        "        self.pdata_cl = np.array([int(np.sum(self.pdata_l[:i])) for i in range(len(self.pdata_l))])\n",
        "        \n",
        "        #Manually add a \"NONE\" tag for untagged characters\n",
        "        self.uniq_tags.add('NONE')\n",
        "\n",
        "        #convert vocab and uniq_tags to list so they have an index we can use\n",
        "        self.vocab = list(self.vocab)\n",
        "        self.uniq_tags = list(self.uniq_tags)\n",
        "        #generate tag data for every caracter in each report\n",
        "        self.generate_tags_data()\n",
        "\n",
        "    def __len__(self):\n",
        "       return self.dataset_lenght\n",
        "\n",
        "    def __getitem__(self, idx): #idx iterates over all characters in the dataset\n",
        "        assert (idx >= 0) & (idx < self.dataset_lenght), \"Dataset index out of range\"\n",
        "        #ic(idx)\n",
        "\n",
        "        #look at what report of all we are looking at and calculate the index taking into account the paddings:\n",
        "        # idx passed would be the index of a character among all reports concatenated without padding.\n",
        "        \n",
        "        prev_reports = np.where(self.pdata_cl < idx)[0]\n",
        "        if(len(prev_reports)) == 0: #if all idx are bigger or equal then we are in the first report\n",
        "            rep_idx = 0 \n",
        "            real_idx = self.sequence_lenght \n",
        "        else:\n",
        "            rep_idx = prev_reports[-1] #last report is the one we are on\n",
        "            real_idx = idx - self.pdata_cl[rep_idx] + self.sequence_lenght\n",
        "        \n",
        "        #ic(rep_idx)\n",
        "        #ic(real_idx)\n",
        "\n",
        "        a_i = real_idx-int(self.sequence_lenght/2)\n",
        "        b_i = real_idx+int(self.sequence_lenght/2)\n",
        "        \n",
        "        #ic(a_i)\n",
        "        #ic(b_i)\n",
        "\n",
        "        txt_window = self.data[rep_idx][\"data\"][\"text\"][a_i:b_i]\n",
        "        tensor_txt_window = torch.tensor([self.vocab.index(ch) for ch in txt_window])\n",
        "        tag_window = torch.tensor(self.tags[rep_idx][a_i:b_i])\n",
        "\n",
        "        return tensor_txt_window, tag_window\n",
        "    \n",
        "    ####\n",
        "    #AUXILIARY FUNCTIONS FOR CLEANER CODE\n",
        "    ####\n",
        "    def extract_vocab(self,report):\n",
        "        for l in list(report[\"data\"][\"text\"]):\n",
        "            self.vocab.add(l) #as vocab is a set it will take care of having unique values\n",
        "\n",
        "    def extract_unique_tags(self,report):\n",
        "        for tagged_range in report[\"predictions\"][0][\"result\"]:\n",
        "            #we checked and this array for all reports only contains one element one label per tagged range\n",
        "            self.uniq_tags.add(tagged_range[\"value\"][\"labels\"][0])\n",
        "    \n",
        "    def pad_report(self,i,report):\n",
        "        padd = \" \"*self.sequence_lenght\n",
        "        self.data[i][\"data\"][\"text\"] = padd + report[\"data\"][\"text\"] + padd\n",
        "\n",
        "    def remove_empty_spaces(self,i,report):\n",
        "        self.data[i][\"data\"][\"text\"] = report[\"data\"][\"text\"].strip()\n",
        "    \n",
        "    def generate_tags_data(self):\n",
        "        for i,report in enumerate(self.data):\n",
        "            text = report[\"data\"][\"text\"]\n",
        "            #start by assigning NONE tag to all characters\n",
        "            self.tags[i] = np.array([self.uniq_tags.index('NONE')]*len(text))\n",
        "            \n",
        "            #iterate among know tagged ranges and assign\n",
        "            for tagged_range in report[\"predictions\"][0][\"result\"]:\n",
        "                start_idx = tagged_range[\"value\"][\"start\"]\n",
        "                end_idx = tagged_range[\"value\"][\"end\"]\n",
        "                tag = tagged_range[\"value\"][\"labels\"][0]\n",
        "\n",
        "                #these idx are without taking into account the initial padding so take that into account:\n",
        "                start_idx += self.sequence_lenght\n",
        "                end_idx += self.sequence_lenght\n",
        "                for j in range(start_idx,end_idx): #end_idx +1, the last one is also tagged?  \n",
        "                    self.tags[i][j] = self.uniq_tags.index(tag)         \n",
        "    ####\n",
        "    ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZwQIvx3pX_XB"
      },
      "outputs": [],
      "source": [
        "sequence_length=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BdvEeczlX_XC"
      },
      "outputs": [],
      "source": [
        "dataset = characters_text2tagsDataset(data,sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRT59HxYX_XC",
        "outputId": "cfa16b64-71e0-45a9-c073-86f969a8de9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([68, 68, 68, 68,  7, 65, 68, 44, 50, 12, 69, 36, 52, 50, 21, 68, 63, 54,\n",
              "         50,  7, 50, 63, 21, 37, 68, 26, 26, 68, 26, 26, 26, 68, 26, 26, 26, 68,\n",
              "          7, 65, 72,  2, 50, 12, 36, 15, 50, 37, 68, 26, 26, 26, 26, 26, 26, 26,\n",
              "         26, 68, 12, 72, 11, 72, 37, 68, 15, 36,  7, 21, 68, 15, 21, 69, 21, 68,\n",
              "         15, 72, 68,  7, 21, 50, 11, 72, 66, 72,  7, 69, 37, 68, 33,  4, 49, 16,\n",
              "         33, 49, 33,  4, 13, 62, 68, 72, 15, 21]),\n",
              " tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2]))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.__getitem__(1444556)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_5i4SCTX_XC",
        "outputId": "6d646477-970b-43db-ad3d-ac4d23e09270"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "txt, tag = dataset.__getitem__(5304)\n",
        "len(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EgKcuDdoX_XD"
      },
      "outputs": [],
      "source": [
        "ic.enable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "raiUECHwX_XD",
        "outputId": "3a26ad6a-9dd1-49f4-aa06-e19357e8d8fe"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'                                                                                                   '"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.data[1][\"data\"][\"text\"][1899:1999]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgsLbtAkX_XD",
        "outputId": "bbd021cb-f6ba-450b-ba87-df1fe1705362"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([68, 72, 12, 69, 21, 24, 54, 72, 20, 68, 21, 64, 72, 24, 52, 50, 54, 68,\n",
              "         50, 68, 21, 66, 24, 68, 24, 36,  7, 68, 63, 36,  7, 69, 52, 36, 54, 68,\n",
              "         15, 72, 54, 68, 15, 36, 54, 36, 52, 49, 68, 24, 36,  7, 68, 69, 52, 36,\n",
              "         64, 50, 12, 66, 72, 68, 15, 50, 12, 69, 21, 54, 49, 68, 15, 36,  7, 21,\n",
              "         15, 21, 68, 54, 21, 68, 24, 36,  7, 21, 68, 72,  5, 36, 54, 56, 63, 50,\n",
              "         36, 68, 63, 54, 50,  7, 50, 63, 21, 68]),\n",
              " tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2]))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.__getitem__(533540)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EavPl6p7X_XD",
        "outputId": "fb3f33d9-e913-40fd-9224-fcd8e069fec7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([72, 52,  7, 21, 49, 68, 15, 72, 24, 50, 15, 36, 68, 21, 68, 54, 21, 68,\n",
              "         24, 56, 72,  7, 21, 68, 72,  5, 36, 54, 56, 63, 50, 36,  7, 68, 63, 54,\n",
              "         50,  7, 50, 63, 21, 20, 68, 12, 72, 68, 15, 72, 63, 50, 15, 72, 68, 21,\n",
              "         54, 69, 21, 68, 44, 36, 12,  2, 50, 69, 21, 54, 21, 52, 50, 21, 49, 68,\n",
              "         36, 52, 50, 72,  7, 69, 21, 63, 50, 36, 68, 15, 50, 21, 31,  7, 36, 12,\n",
              "         69, 50, 63, 21, 68, 21, 12, 66, 21, 68]),\n",
              " tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "         2, 2, 2, 2]))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset.__getitem__(33490)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmBtBKf4X_XD",
        "outputId": "eedfe84d-df4b-426a-9080-c1035b134c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "365\n",
            "tensor([68, 15,  1, 50,  7, 31, 52, 72, 12, 68,  2, 21, 63, 50, 72,  7, 69, 72,\n",
            "        68, 15, 72, 68, 10,  4, 68, 21, 17, 36, 12, 68, 23, 56, 72, 68, 21, 63,\n",
            "        56, 15, 72, 68, 15, 72, 52, 50,  5, 21, 15, 21, 68, 21, 68, 56, 52, 31,\n",
            "        72,  7, 63, 50, 21, 12, 68, 15, 72, 68,  2, 12, 50, 23, 56, 50, 21, 69,\n",
            "        52, 50, 21, 68, 69, 52, 21, 12, 68, 12, 50, 66, 49, 68, 21,  7, 69, 72,\n",
            "        63, 72, 15, 72,  7, 69, 12, 68,  8, 12])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 4])\n",
            "366\n",
            "tensor([15,  1, 50,  7, 31, 52, 72, 12, 68,  2, 21, 63, 50, 72,  7, 69, 72, 68,\n",
            "        15, 72, 68, 10,  4, 68, 21, 17, 36, 12, 68, 23, 56, 72, 68, 21, 63, 56,\n",
            "        15, 72, 68, 15, 72, 52, 50,  5, 21, 15, 21, 68, 21, 68, 56, 52, 31, 72,\n",
            "         7, 63, 50, 21, 12, 68, 15, 72, 68,  2, 12, 50, 23, 56, 50, 21, 69, 52,\n",
            "        50, 21, 68, 69, 52, 21, 12, 68, 12, 50, 66, 49, 68, 21,  7, 69, 72, 63,\n",
            "        72, 15, 72,  7, 69, 12, 68,  8, 12, 50])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 4, 4])\n",
            "367\n",
            "tensor([ 1, 50,  7, 31, 52, 72, 12, 68,  2, 21, 63, 50, 72,  7, 69, 72, 68, 15,\n",
            "        72, 68, 10,  4, 68, 21, 17, 36, 12, 68, 23, 56, 72, 68, 21, 63, 56, 15,\n",
            "        72, 68, 15, 72, 52, 50,  5, 21, 15, 21, 68, 21, 68, 56, 52, 31, 72,  7,\n",
            "        63, 50, 21, 12, 68, 15, 72, 68,  2, 12, 50, 23, 56, 50, 21, 69, 52, 50,\n",
            "        21, 68, 69, 52, 21, 12, 68, 12, 50, 66, 49, 68, 21,  7, 69, 72, 63, 72,\n",
            "        15, 72,  7, 69, 12, 68,  8, 12, 50,  7])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 4, 4, 4])\n",
            "368\n",
            "tensor([50,  7, 31, 52, 72, 12, 68,  2, 21, 63, 50, 72,  7, 69, 72, 68, 15, 72,\n",
            "        68, 10,  4, 68, 21, 17, 36, 12, 68, 23, 56, 72, 68, 21, 63, 56, 15, 72,\n",
            "        68, 15, 72, 52, 50,  5, 21, 15, 21, 68, 21, 68, 56, 52, 31, 72,  7, 63,\n",
            "        50, 21, 12, 68, 15, 72, 68,  2, 12, 50, 23, 56, 50, 21, 69, 52, 50, 21,\n",
            "        68, 69, 52, 21, 12, 68, 12, 50, 66, 49, 68, 21,  7, 69, 72, 63, 72, 15,\n",
            "        72,  7, 69, 12, 68,  8, 12, 50,  7, 68])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        4, 4, 4, 2])\n",
            "369\n",
            "tensor([ 7, 31, 52, 72, 12, 68,  2, 21, 63, 50, 72,  7, 69, 72, 68, 15, 72, 68,\n",
            "        10,  4, 68, 21, 17, 36, 12, 68, 23, 56, 72, 68, 21, 63, 56, 15, 72, 68,\n",
            "        15, 72, 52, 50,  5, 21, 15, 21, 68, 21, 68, 56, 52, 31, 72,  7, 63, 50,\n",
            "        21, 12, 68, 15, 72, 68,  2, 12, 50, 23, 56, 50, 21, 69, 52, 50, 21, 68,\n",
            "        69, 52, 21, 12, 68, 12, 50, 66, 49, 68, 21,  7, 69, 72, 63, 72, 15, 72,\n",
            "         7, 69, 12, 68,  8, 12, 50,  7, 68, 21])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4,\n",
            "        4, 4, 2, 3])\n",
            "370\n",
            "tensor([31, 52, 72, 12, 68,  2, 21, 63, 50, 72,  7, 69, 72, 68, 15, 72, 68, 10,\n",
            "         4, 68, 21, 17, 36, 12, 68, 23, 56, 72, 68, 21, 63, 56, 15, 72, 68, 15,\n",
            "        72, 52, 50,  5, 21, 15, 21, 68, 21, 68, 56, 52, 31, 72,  7, 63, 50, 21,\n",
            "        12, 68, 15, 72, 68,  2, 12, 50, 23, 56, 50, 21, 69, 52, 50, 21, 68, 69,\n",
            "        52, 21, 12, 68, 12, 50, 66, 49, 68, 21,  7, 69, 72, 63, 72, 15, 72,  7,\n",
            "        69, 12, 68,  8, 12, 50,  7, 68, 21, 54])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4,\n",
            "        4, 2, 3, 3])\n",
            "371\n",
            "tensor([52, 72, 12, 68,  2, 21, 63, 50, 72,  7, 69, 72, 68, 15, 72, 68, 10,  4,\n",
            "        68, 21, 17, 36, 12, 68, 23, 56, 72, 68, 21, 63, 56, 15, 72, 68, 15, 72,\n",
            "        52, 50,  5, 21, 15, 21, 68, 21, 68, 56, 52, 31, 72,  7, 63, 50, 21, 12,\n",
            "        68, 15, 72, 68,  2, 12, 50, 23, 56, 50, 21, 69, 52, 50, 21, 68, 69, 52,\n",
            "        21, 12, 68, 12, 50, 66, 49, 68, 21,  7, 69, 72, 63, 72, 15, 72,  7, 69,\n",
            "        12, 68,  8, 12, 50,  7, 68, 21, 54, 72])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4,\n",
            "        2, 3, 3, 3])\n",
            "372\n",
            "tensor([72, 12, 68,  2, 21, 63, 50, 72,  7, 69, 72, 68, 15, 72, 68, 10,  4, 68,\n",
            "        21, 17, 36, 12, 68, 23, 56, 72, 68, 21, 63, 56, 15, 72, 68, 15, 72, 52,\n",
            "        50,  5, 21, 15, 21, 68, 21, 68, 56, 52, 31, 72,  7, 63, 50, 21, 12, 68,\n",
            "        15, 72, 68,  2, 12, 50, 23, 56, 50, 21, 69, 52, 50, 21, 68, 69, 52, 21,\n",
            "        12, 68, 12, 50, 66, 49, 68, 21,  7, 69, 72, 63, 72, 15, 72,  7, 69, 12,\n",
            "        68,  8, 12, 50,  7, 68, 21, 54, 72, 52])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2,\n",
            "        3, 3, 3, 3])\n",
            "373\n",
            "tensor([12, 68,  2, 21, 63, 50, 72,  7, 69, 72, 68, 15, 72, 68, 10,  4, 68, 21,\n",
            "        17, 36, 12, 68, 23, 56, 72, 68, 21, 63, 56, 15, 72, 68, 15, 72, 52, 50,\n",
            "         5, 21, 15, 21, 68, 21, 68, 56, 52, 31, 72,  7, 63, 50, 21, 12, 68, 15,\n",
            "        72, 68,  2, 12, 50, 23, 56, 50, 21, 69, 52, 50, 21, 68, 69, 52, 21, 12,\n",
            "        68, 12, 50, 66, 49, 68, 21,  7, 69, 72, 63, 72, 15, 72,  7, 69, 12, 68,\n",
            "         8, 12, 50,  7, 68, 21, 54, 72, 52, 31])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3,\n",
            "        3, 3, 3, 3])\n",
            "374\n",
            "tensor([68,  2, 21, 63, 50, 72,  7, 69, 72, 68, 15, 72, 68, 10,  4, 68, 21, 17,\n",
            "        36, 12, 68, 23, 56, 72, 68, 21, 63, 56, 15, 72, 68, 15, 72, 52, 50,  5,\n",
            "        21, 15, 21, 68, 21, 68, 56, 52, 31, 72,  7, 63, 50, 21, 12, 68, 15, 72,\n",
            "        68,  2, 12, 50, 23, 56, 50, 21, 69, 52, 50, 21, 68, 69, 52, 21, 12, 68,\n",
            "        12, 50, 66, 49, 68, 21,  7, 69, 72, 63, 72, 15, 72,  7, 69, 12, 68,  8,\n",
            "        12, 50,  7, 68, 21, 54, 72, 52, 31, 50])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3,\n",
            "        3, 3, 3, 3])\n",
            "375\n",
            "tensor([ 2, 21, 63, 50, 72,  7, 69, 72, 68, 15, 72, 68, 10,  4, 68, 21, 17, 36,\n",
            "        12, 68, 23, 56, 72, 68, 21, 63, 56, 15, 72, 68, 15, 72, 52, 50,  5, 21,\n",
            "        15, 21, 68, 21, 68, 56, 52, 31, 72,  7, 63, 50, 21, 12, 68, 15, 72, 68,\n",
            "         2, 12, 50, 23, 56, 50, 21, 69, 52, 50, 21, 68, 69, 52, 21, 12, 68, 12,\n",
            "        50, 66, 49, 68, 21,  7, 69, 72, 63, 72, 15, 72,  7, 69, 12, 68,  8, 12,\n",
            "        50,  7, 68, 21, 54, 72, 52, 31, 50, 21])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3, 3,\n",
            "        3, 3, 3, 3])\n",
            "376\n",
            "tensor([21, 63, 50, 72,  7, 69, 72, 68, 15, 72, 68, 10,  4, 68, 21, 17, 36, 12,\n",
            "        68, 23, 56, 72, 68, 21, 63, 56, 15, 72, 68, 15, 72, 52, 50,  5, 21, 15,\n",
            "        21, 68, 21, 68, 56, 52, 31, 72,  7, 63, 50, 21, 12, 68, 15, 72, 68,  2,\n",
            "        12, 50, 23, 56, 50, 21, 69, 52, 50, 21, 68, 69, 52, 21, 12, 68, 12, 50,\n",
            "        66, 49, 68, 21,  7, 69, 72, 63, 72, 15, 72,  7, 69, 12, 68,  8, 12, 50,\n",
            "         7, 68, 21, 54, 72, 52, 31, 50, 21, 12])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3])\n",
            "377\n",
            "tensor([63, 50, 72,  7, 69, 72, 68, 15, 72, 68, 10,  4, 68, 21, 17, 36, 12, 68,\n",
            "        23, 56, 72, 68, 21, 63, 56, 15, 72, 68, 15, 72, 52, 50,  5, 21, 15, 21,\n",
            "        68, 21, 68, 56, 52, 31, 72,  7, 63, 50, 21, 12, 68, 15, 72, 68,  2, 12,\n",
            "        50, 23, 56, 50, 21, 69, 52, 50, 21, 68, 69, 52, 21, 12, 68, 12, 50, 66,\n",
            "        49, 68, 21,  7, 69, 72, 63, 72, 15, 72,  7, 69, 12, 68,  8, 12, 50,  7,\n",
            "        68, 21, 54, 72, 52, 31, 50, 21, 12, 68])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3])\n",
            "378\n",
            "tensor([50, 72,  7, 69, 72, 68, 15, 72, 68, 10,  4, 68, 21, 17, 36, 12, 68, 23,\n",
            "        56, 72, 68, 21, 63, 56, 15, 72, 68, 15, 72, 52, 50,  5, 21, 15, 21, 68,\n",
            "        21, 68, 56, 52, 31, 72,  7, 63, 50, 21, 12, 68, 15, 72, 68,  2, 12, 50,\n",
            "        23, 56, 50, 21, 69, 52, 50, 21, 68, 69, 52, 21, 12, 68, 12, 50, 66, 49,\n",
            "        68, 21,  7, 69, 72, 63, 72, 15, 72,  7, 69, 12, 68,  8, 12, 50,  7, 68,\n",
            "        21, 54, 72, 52, 31, 50, 21, 12, 68, 66])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3])\n",
            "379\n",
            "tensor([72,  7, 69, 72, 68, 15, 72, 68, 10,  4, 68, 21, 17, 36, 12, 68, 23, 56,\n",
            "        72, 68, 21, 63, 56, 15, 72, 68, 15, 72, 52, 50,  5, 21, 15, 21, 68, 21,\n",
            "        68, 56, 52, 31, 72,  7, 63, 50, 21, 12, 68, 15, 72, 68,  2, 12, 50, 23,\n",
            "        56, 50, 21, 69, 52, 50, 21, 68, 69, 52, 21, 12, 68, 12, 50, 66, 49, 68,\n",
            "        21,  7, 69, 72, 63, 72, 15, 72,  7, 69, 12, 68,  8, 12, 50,  7, 68, 21,\n",
            "        54, 72, 52, 31, 50, 21, 12, 68, 66, 72])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3])\n",
            "380\n",
            "tensor([ 7, 69, 72, 68, 15, 72, 68, 10,  4, 68, 21, 17, 36, 12, 68, 23, 56, 72,\n",
            "        68, 21, 63, 56, 15, 72, 68, 15, 72, 52, 50,  5, 21, 15, 21, 68, 21, 68,\n",
            "        56, 52, 31, 72,  7, 63, 50, 21, 12, 68, 15, 72, 68,  2, 12, 50, 23, 56,\n",
            "        50, 21, 69, 52, 50, 21, 68, 69, 52, 21, 12, 68, 12, 50, 66, 49, 68, 21,\n",
            "         7, 69, 72, 63, 72, 15, 72,  7, 69, 12, 68,  8, 12, 50,  7, 68, 21, 54,\n",
            "        72, 52, 31, 50, 21, 12, 68, 66, 72, 15])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3])\n",
            "381\n",
            "tensor([69, 72, 68, 15, 72, 68, 10,  4, 68, 21, 17, 36, 12, 68, 23, 56, 72, 68,\n",
            "        21, 63, 56, 15, 72, 68, 15, 72, 52, 50,  5, 21, 15, 21, 68, 21, 68, 56,\n",
            "        52, 31, 72,  7, 63, 50, 21, 12, 68, 15, 72, 68,  2, 12, 50, 23, 56, 50,\n",
            "        21, 69, 52, 50, 21, 68, 69, 52, 21, 12, 68, 12, 50, 66, 49, 68, 21,  7,\n",
            "        69, 72, 63, 72, 15, 72,  7, 69, 12, 68,  8, 12, 50,  7, 68, 21, 54, 72,\n",
            "        52, 31, 50, 21, 12, 68, 66, 72, 15, 50])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3])\n",
            "382\n",
            "tensor([72, 68, 15, 72, 68, 10,  4, 68, 21, 17, 36, 12, 68, 23, 56, 72, 68, 21,\n",
            "        63, 56, 15, 72, 68, 15, 72, 52, 50,  5, 21, 15, 21, 68, 21, 68, 56, 52,\n",
            "        31, 72,  7, 63, 50, 21, 12, 68, 15, 72, 68,  2, 12, 50, 23, 56, 50, 21,\n",
            "        69, 52, 50, 21, 68, 69, 52, 21, 12, 68, 12, 50, 66, 49, 68, 21,  7, 69,\n",
            "        72, 63, 72, 15, 72,  7, 69, 12, 68,  8, 12, 50,  7, 68, 21, 54, 72, 52,\n",
            "        31, 50, 21, 12, 68, 66, 72, 15, 50, 21])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3])\n",
            "383\n",
            "tensor([68, 15, 72, 68, 10,  4, 68, 21, 17, 36, 12, 68, 23, 56, 72, 68, 21, 63,\n",
            "        56, 15, 72, 68, 15, 72, 52, 50,  5, 21, 15, 21, 68, 21, 68, 56, 52, 31,\n",
            "        72,  7, 63, 50, 21, 12, 68, 15, 72, 68,  2, 12, 50, 23, 56, 50, 21, 69,\n",
            "        52, 50, 21, 68, 69, 52, 21, 12, 68, 12, 50, 66, 49, 68, 21,  7, 69, 72,\n",
            "        63, 72, 15, 72,  7, 69, 12, 68,  8, 12, 50,  7, 68, 21, 54, 72, 52, 31,\n",
            "        50, 21, 12, 68, 66, 72, 15, 50, 21, 66])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3])\n",
            "384\n",
            "tensor([15, 72, 68, 10,  4, 68, 21, 17, 36, 12, 68, 23, 56, 72, 68, 21, 63, 56,\n",
            "        15, 72, 68, 15, 72, 52, 50,  5, 21, 15, 21, 68, 21, 68, 56, 52, 31, 72,\n",
            "         7, 63, 50, 21, 12, 68, 15, 72, 68,  2, 12, 50, 23, 56, 50, 21, 69, 52,\n",
            "        50, 21, 68, 69, 52, 21, 12, 68, 12, 50, 66, 49, 68, 21,  7, 69, 72, 63,\n",
            "        72, 15, 72,  7, 69, 12, 68,  8, 12, 50,  7, 68, 21, 54, 72, 52, 31, 50,\n",
            "        21, 12, 68, 66, 72, 15, 50, 21, 66, 72])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3])\n",
            "385\n",
            "tensor([72, 68, 10,  4, 68, 21, 17, 36, 12, 68, 23, 56, 72, 68, 21, 63, 56, 15,\n",
            "        72, 68, 15, 72, 52, 50,  5, 21, 15, 21, 68, 21, 68, 56, 52, 31, 72,  7,\n",
            "        63, 50, 21, 12, 68, 15, 72, 68,  2, 12, 50, 23, 56, 50, 21, 69, 52, 50,\n",
            "        21, 68, 69, 52, 21, 12, 68, 12, 50, 66, 49, 68, 21,  7, 69, 72, 63, 72,\n",
            "        15, 72,  7, 69, 12, 68,  8, 12, 50,  7, 68, 21, 54, 72, 52, 31, 50, 21,\n",
            "        12, 68, 66, 72, 15, 50, 21, 66, 72,  7])\n",
            "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "        2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
            "        3, 3, 3, 3])\n"
          ]
        }
      ],
      "source": [
        "cnt = 0\n",
        "cnt2 = 0\n",
        "for i, (txt, tag) in enumerate(dataset):\n",
        "    cnt += 1\n",
        "    if(len(txt) < dataset.sequence_lenght):\n",
        "        print(i)\n",
        "        print(len(txt))\n",
        "        #print(txt)\n",
        "        #print(tag)\n",
        "        break\n",
        "    if(len(np.unique(tag))>1):\n",
        "        print(i)\n",
        "        print(txt)\n",
        "        print(tag)\n",
        "        cnt2 += 1\n",
        "    if(cnt2 > 20):\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1vn14wRX_XE",
        "outputId": "3c1c384c-415f-426d-8f3c-86d6149e5a9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1611331"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "novhhY1yX_XE",
        "outputId": "ff46d114-b098-41d1-c4f3-6f87da576c83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "386"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yA9ZVdnbX_XE"
      },
      "outputs": [],
      "source": [
        "batch_size = 1000\n",
        "train_loader_inaugural = DataLoader(dataset, shuffle=True, batch_size=batch_size, drop_last=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yKkiY6rlX_XE"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uhpuTdJX_XE",
        "outputId": "294e6016-4bc3-4488-da90-752506da5c5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#If this cell fails you need to change the runtime of your colab notebook to GPU\n",
        "# Go to Runtime -> Change Runtime Type and select GPU\n",
        "assert torch.cuda.is_available() or  torch.has_mps, \"GPU is not enabled\"\n",
        "\n",
        "#  use gpu if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
        "print(\"Using device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "S9Pqte8HX_XE"
      },
      "outputs": [],
      "source": [
        "class Model_LSTM(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob=0.):\n",
        "        super(Model_LSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        # YOUR CODE HERE\n",
        "        self.embedding = nn.Embedding(num_embeddings=input_dim,embedding_dim=embedding_dim) #input -> BATCH, SEQ_LENGHT (one-hot can reduce one dimension)  or -> BATCH, SEQ_LENGHT, INPUT DIM\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, dropout=drop_prob, batch_first=True,bidirectional=True) #input -> BATCH, SEQ_LENGHT, EMBEDDING_DIM\n",
        "        self.fc = nn.Linear(hidden_dim*2,output_dim) # input -> BATCH, HIDDEN_DIM\n",
        "        #out -> BATCH, OUTPUT_DIM (if one hot can reduce one dimension and we are only comparing integers, why can't we do BATCH, 1\n",
        "         \n",
        "        \n",
        "    def forward(self, x, h ,c):\n",
        "        #ic(x.shape)\n",
        "        emb = self.embedding(x)\n",
        "        #ic(emb.shape)\n",
        "        out, (h,c) = self.lstm(emb, (h,c))\n",
        "        #ic(out.shape)\n",
        "        out = self.fc(out)\n",
        "        #ic(out.shape)\n",
        "        return out, h ,c\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        \" Initialize the hidden state of the RNN to zeros\"\n",
        "        h = nn.Parameter(torch.zeros(self.n_layers*2, batch_size, self.hidden_dim))\n",
        "        c = nn.Parameter(torch.zeros(self.n_layers*2, batch_size, self.hidden_dim))\n",
        "        return h, c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "auK6R9quX_XE",
        "outputId": "d4b4b279-8595-4fa0-dd01-36199bc63f01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:71: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model_LSTM(\n",
            "  (embedding): Embedding(75, 50)\n",
            "  (lstm): LSTM(50, 200, batch_first=True, dropout=0.2, bidirectional=True)\n",
            "  (fc): Linear(in_features=400, out_features=5, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "input_dim = len(dataset.vocab)\n",
        "embedding_dim = 50\n",
        "hidden_dim = 200\n",
        "output_dim = len(dataset.uniq_tags)\n",
        "n_layers = 1\n",
        "drop_prob = 0.2\n",
        "\n",
        "model = Model_LSTM(input_dim, embedding_dim, hidden_dim, output_dim, n_layers, drop_prob).to(device)\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "dsfOf3WpX_XF"
      },
      "outputs": [],
      "source": [
        "def train_lstm(dataloader, model, batch_size, sequence_length, num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    criterion =  nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/LSTM_character_tagger.pth\")\n",
        "        for batch, (x, y) in enumerate(dataloader):\n",
        "            h, c = model.init_hidden(batch_size) # Start with a new state in each batch            \n",
        "            h = h.to(device)\n",
        "            c = c.to(device)\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            y_pred, h,c= model(x, h, c)\n",
        "            #ic(y.shape)\n",
        "            #ic(y_pred.shape)\n",
        "            loss = criterion(y_pred.transpose(1, 2), y)  #cross entropy loss needs (N,C,seq_lenght)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch%30 == 0:\n",
        "                print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
        "                losses.append(loss.item())\n",
        "  \n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "yot9eR2gX_XF"
      },
      "outputs": [],
      "source": [
        "num_epochs = 5\n",
        "train = False\n",
        "if train:\n",
        "  losses = train_lstm(train_loader_inaugural, model, batch_size, sequence_length, num_epochs)\n",
        "  torch.save(model.state_dict(), \"/content/drive/MyDrive/LSTM_character_tagger.pth\")\n",
        "else:\n",
        "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/LSTM_character_tagger.pth\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "pXlmFF2CX_XF",
        "outputId": "5a5f12ec-9f97-4946-8715-4289aa1e656c"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize the loss evolution during training\n",
        "plt.plot(losses)\n",
        "plt.title('model loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq7pbPTDX_XF",
        "outputId": "9556b30e-0336-4cf1-845e-fe06e0a62d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31msome red text\n"
          ]
        }
      ],
      "source": [
        "from colorama import Fore, Back, Style\n",
        " \n",
        "print(Fore.RED + 'some red text')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5FkwFoRtHsyg"
      },
      "outputs": [],
      "source": [
        "colors = [Fore.BLUE,Fore.GREEN,Fore.WHITE,Fore.CYAN,Fore.RED]\n",
        "reference_tag_txt = [colors[i]+dataset.uniq_tags[i] for i in range(len(dataset.uniq_tags))]\n",
        "reference_tag_txt = \" \".join(reference_tag_txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGJxzFcmX_XF",
        "outputId": "d44f6201-68c6-490c-ec1d-d625063249df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mUSCO \u001b[32mUNC \u001b[37mNONE \u001b[36mNSCO \u001b[31mNEG\n",
            "\u001b[37ma\u001b[37mq\u001b[37mu\u001b[37me\u001b[37ms\u001b[37mt\u001b[37ma\u001b[37m \u001b[37ms\u001b[37me\u001b[37mt\u001b[37mm\u001b[37ma\u001b[37mn\u001b[37ma\u001b[37m \u001b[31mn\u001b[31mo\u001b[31m \u001b[36mp\u001b[36mr\u001b[36me\u001b[36ms\u001b[36me\u001b[36mn\u001b[36mt\u001b[36ma\u001b[36mv\u001b[36ma\u001b[36m \u001b[36mc\u001b[36ma\u001b[36mp\u001b[36m \u001b[37ms\u001b[37mi\u001b[37mn\u001b[37mt\u001b[37mo\u001b[37mm\u001b[36ma\u001b[36m \u001b[37md\u001b[37me\u001b[37m \u001b[37mf\u001b[37me\u001b[37mb\u001b[37mr\u001b[37mi\u001b[36ml\u001b[37m.\n"
          ]
        }
      ],
      "source": [
        "test_text = \"aquesta setmana no presentava cap sintoma de febril.\"\n",
        "txt_tensor = torch.tensor([dataset.vocab.index(ch) for ch in test_text]).to(device)\n",
        "h, c = model.init_hidden(batch_size=1)\n",
        "#unbatch so delete batch dimension\n",
        "h = h.squeeze(1).to(device)\n",
        "c = c.squeeze(1).to(device)\n",
        "\n",
        "out, h, c = model.forward(txt_tensor,h,c)\n",
        "out = out.cpu()\n",
        "print(reference_tag_txt)\n",
        "txt = \"\"\n",
        "for i,pred in enumerate(out):\n",
        "    tag_idx = np.where(pred==pred.max())[0][0]\n",
        "    txt += colors[tag_idx] + test_text[i]\n",
        "\n",
        "print(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuCmcRYLAtAL",
        "outputId": "08ee0705-b35c-4b15-a251-8d4cbda88100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mUSCO \u001b[32mUNC \u001b[37mNONE \u001b[36mNSCO \u001b[31mNEG\n",
            "\u001b[37me\u001b[37ml\u001b[37m \u001b[37mp\u001b[37ma\u001b[37mc\u001b[37mi\u001b[37me\u001b[37mn\u001b[37mt\u001b[37me\u001b[37m \u001b[37mp\u001b[37mr\u001b[37me\u001b[37ms\u001b[37me\u001b[37mn\u001b[37mt\u001b[37ma\u001b[37m \u001b[37ml\u001b[37me\u001b[37mu\u001b[37mc\u001b[37mo\u001b[37mp\u001b[37ml\u001b[37ma\u001b[37ms\u001b[37mi\u001b[37ma\u001b[37m \u001b[37ms\u001b[37me\u001b[37mv\u001b[37me\u001b[37mr\u001b[37ma\u001b[37m \u001b[37me\u001b[37mn\u001b[37m \u001b[37me\u001b[37ml\u001b[37m \u001b[37md\u001b[37mo\u001b[37mr\u001b[37ms\u001b[37mo\u001b[37m \u001b[37md\u001b[37me\u001b[37m \u001b[37ml\u001b[37ma\u001b[37m \u001b[37ml\u001b[37me\u001b[37mn\u001b[37mg\u001b[37mu\u001b[37ma\u001b[37m.\n"
          ]
        }
      ],
      "source": [
        "test_text = \"el paciente presenta leucoplasia severa en el dorso de la lengua.\"\n",
        "txt_tensor = torch.tensor([dataset.vocab.index(ch) for ch in test_text]).to(device)\n",
        "h, c = model.init_hidden(batch_size=1)\n",
        "#unbatch so delete batch dimension\n",
        "h = h.squeeze(1).to(device)\n",
        "c = c.squeeze(1).to(device)\n",
        "\n",
        "out, h, c = model.forward(txt_tensor,h,c)\n",
        "out = out.cpu()\n",
        "\n",
        "txt = \"\"\n",
        "print(reference_tag_txt)\n",
        "for i,pred in enumerate(out):\n",
        "    tag_idx = np.where(pred==pred.max())[0][0]\n",
        "    txt += colors[tag_idx] + test_text[i]\n",
        "\n",
        "print(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdzASZAfAwKZ",
        "outputId": "d7dc4a88-f1d0-4eaa-8baa-e12619a939c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mUSCO \u001b[32mUNC \u001b[37mNONE \u001b[36mNSCO \u001b[31mNEG\n",
            "\u001b[37me\u001b[37ml\u001b[37m \u001b[37mp\u001b[37ma\u001b[37mc\u001b[37mi\u001b[37me\u001b[37mn\u001b[37mt\u001b[37me\u001b[37m \u001b[31mn\u001b[31mo\u001b[31m \u001b[36mp\u001b[36mr\u001b[36me\u001b[36ms\u001b[37me\u001b[36mn\u001b[36mt\u001b[36ma\u001b[36m \u001b[37ml\u001b[37me\u001b[37mu\u001b[36mc\u001b[37mo\u001b[36mp\u001b[36ml\u001b[37ma\u001b[36ms\u001b[37mi\u001b[37ma\u001b[36m \u001b[37ms\u001b[37me\u001b[37mv\u001b[37me\u001b[37mr\u001b[37ma\u001b[37m \u001b[37me\u001b[37mn\u001b[37m \u001b[36me\u001b[36ml\u001b[36m \u001b[36md\u001b[36mo\u001b[36mr\u001b[36ms\u001b[36mo\u001b[36m \u001b[36md\u001b[36me\u001b[36m \u001b[36ml\u001b[36ma\u001b[36m \u001b[36ml\u001b[36me\u001b[36mn\u001b[36mg\u001b[36mu\u001b[36ma\u001b[37m.\n"
          ]
        }
      ],
      "source": [
        "test_text = \"el paciente no presenta leucoplasia severa en el dorso de la lengua.\"\n",
        "txt_tensor = torch.tensor([dataset.vocab.index(ch) for ch in test_text]).to(device)\n",
        "h, c = model.init_hidden(batch_size=1)\n",
        "#unbatch so delete batch dimension\n",
        "h = h.squeeze(1).to(device)\n",
        "c = c.squeeze(1).to(device)\n",
        "\n",
        "out, h, c = model.forward(txt_tensor,h,c)\n",
        "out = out.cpu()\n",
        "\n",
        "txt = \"\"\n",
        "print(reference_tag_txt)\n",
        "for i,pred in enumerate(out):\n",
        "    tag_idx = np.where(pred==pred.max())[0][0]\n",
        "    txt += colors[tag_idx] + test_text[i]\n",
        "\n",
        "print(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuOFlf2OBLat",
        "outputId": "2b5ce899-1534-4d5e-da76-8dfea82692f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mUSCO \u001b[32mUNC \u001b[37mNONE \u001b[36mNSCO \u001b[31mNEG\n",
            "\u001b[37me\u001b[37ml\u001b[37m \u001b[37mp\u001b[37ma\u001b[37mc\u001b[37mi\u001b[37me\u001b[37mn\u001b[37mt\u001b[37m \u001b[37mp\u001b[37mr\u001b[37me\u001b[37ms\u001b[37me\u001b[37mn\u001b[37mt\u001b[37ma\u001b[37m \u001b[37ml\u001b[37me\u001b[37mu\u001b[37mc\u001b[37mo\u001b[37mp\u001b[37ml\u001b[37ma\u001b[37ms\u001b[37mi\u001b[37ma\u001b[37m \u001b[37ms\u001b[37me\u001b[37mv\u001b[37me\u001b[37mr\u001b[37ma\u001b[37m \u001b[37ma\u001b[37ml\u001b[37m \u001b[37md\u001b[37mo\u001b[37mr\u001b[37ms\u001b[37ma\u001b[37ml\u001b[37m \u001b[37md\u001b[37me\u001b[37m \u001b[37ml\u001b[37ma\u001b[37m \u001b[37ml\u001b[37ml\u001b[37me\u001b[37mn\u001b[37mg\u001b[37mu\u001b[37ma\u001b[37m.\n"
          ]
        }
      ],
      "source": [
        "test_text = \"el pacient presenta leucoplasia severa al dorsal de la llengua.\"\n",
        "txt_tensor = torch.tensor([dataset.vocab.index(ch) for ch in test_text]).to(device)\n",
        "h, c = model.init_hidden(batch_size=1)\n",
        "#unbatch so delete batch dimension\n",
        "h = h.squeeze(1).to(device)\n",
        "c = c.squeeze(1).to(device)\n",
        "\n",
        "out, h, c = model.forward(txt_tensor,h,c)\n",
        "out = out.cpu()\n",
        "\n",
        "txt = \"\"\n",
        "print(reference_tag_txt)\n",
        "for i,pred in enumerate(out):\n",
        "    tag_idx = np.where(pred==pred.max())[0][0]\n",
        "    txt += colors[tag_idx] + test_text[i]\n",
        "\n",
        "print(txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UzvkYkwBSXr",
        "outputId": "270b92e9-ef1f-4090-ee00-b35175e9a837"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mUSCO \u001b[32mUNC \u001b[37mNONE \u001b[36mNSCO \u001b[31mNEG\n",
            "\u001b[37me\u001b[37ml\u001b[37m \u001b[37mp\u001b[37ma\u001b[37mc\u001b[37mi\u001b[37me\u001b[37mn\u001b[37mt\u001b[37m \u001b[31mn\u001b[31mo\u001b[31m \u001b[36mp\u001b[37mr\u001b[36me\u001b[36ms\u001b[37me\u001b[36mn\u001b[36mt\u001b[36ma\u001b[36m \u001b[37ml\u001b[37me\u001b[36mu\u001b[36mc\u001b[36mo\u001b[36mp\u001b[36ml\u001b[36ma\u001b[37ms\u001b[37mi\u001b[36ma\u001b[36m \u001b[37ms\u001b[37me\u001b[37mv\u001b[37me\u001b[37mr\u001b[37ma\u001b[37m \u001b[37ma\u001b[36ml\u001b[37m \u001b[37md\u001b[37mo\u001b[37mr\u001b[37ms\u001b[37ma\u001b[37ml\u001b[37m \u001b[37md\u001b[37me\u001b[37m \u001b[37ml\u001b[37ma\u001b[37m \u001b[37ml\u001b[37ml\u001b[37me\u001b[37mn\u001b[37mg\u001b[37mu\u001b[37ma\u001b[37m.\n"
          ]
        }
      ],
      "source": [
        "test_text = \"el pacient no presenta leucoplasia severa al dorsal de la llengua.\"\n",
        "txt_tensor = torch.tensor([dataset.vocab.index(ch) for ch in test_text]).to(device)\n",
        "h, c = model.init_hidden(batch_size=1)\n",
        "#unbatch so delete batch dimension\n",
        "h = h.squeeze(1).to(device)\n",
        "c = c.squeeze(1).to(device)\n",
        "\n",
        "out, h, c = model.forward(txt_tensor,h,c)\n",
        "out = out.cpu()\n",
        "\n",
        "txt = \"\"\n",
        "print(reference_tag_txt)\n",
        "\n",
        "for i,pred in enumerate(out):\n",
        "    tag_idx = np.where(pred==pred.max())[0][0]\n",
        "    txt += colors[tag_idx] + test_text[i]\n",
        "\n",
        "print(txt)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.9 ('ML')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "da600ade1a771c82ddf6d22a5a41f856afbf3528a3611e1c80e3ac6da17c9450"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
